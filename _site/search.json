[
  {
    "objectID": "project5.html#introducci√≥n",
    "href": "project5.html#introducci√≥n",
    "title": "Proyecto 5",
    "section": "1. Introducci√≥n",
    "text": "1. Introducci√≥n"
  },
  {
    "objectID": "project5.html#desarrollo",
    "href": "project5.html#desarrollo",
    "title": "Proyecto 5",
    "section": "2. Desarrollo",
    "text": "2. Desarrollo"
  },
  {
    "objectID": "project5.html#an√°lisis-de-resultados",
    "href": "project5.html#an√°lisis-de-resultados",
    "title": "Proyecto 5",
    "section": "3. An√°lisis de Resultados",
    "text": "3. An√°lisis de Resultados"
  },
  {
    "objectID": "project5.html#conclusiones",
    "href": "project5.html#conclusiones",
    "title": "Proyecto 5",
    "section": "4. Conclusiones",
    "text": "4. Conclusiones"
  },
  {
    "objectID": "project5.html#referencias",
    "href": "project5.html#referencias",
    "title": "Proyecto 5",
    "section": "5. Referencias",
    "text": "5. Referencias"
  },
  {
    "objectID": "project5.html#cambios-realizados",
    "href": "project5.html#cambios-realizados",
    "title": "Proyecto 5",
    "section": "6. Cambios Realizados",
    "text": "6. Cambios Realizados"
  },
  {
    "objectID": "project3.html#introducci√≥n",
    "href": "project3.html#introducci√≥n",
    "title": "Proyecto 3",
    "section": "",
    "text": "La investigaci√≥n sobre el an√°lisis de algoritmos de ordenamiento es un campo activo, con numerosos estudios que exploran nuevas t√©cnicas y optimizaciones. Ordenar datos es crucial en la ciencia de datos y la computaci√≥n, con aplicaciones que abarcan desde la gesti√≥n de bases de datos hasta la optimizaci√≥n de algoritmos en inteligencia artificial. La eficiencia de los algoritmos de ordenamiento es importante, ya que puede influir significativamente en el rendimiento general de un sistema.\nLa eficiencia de un algoritmo de ordenamiento se mide principalmente en t√©rminos de tiempo y espacio. El tiempo se refiere a cu√°ntas operaciones realiza el algoritmo, mientras que el espacio indica cu√°nta memoria adicional necesita. Un algoritmo de ordenamiento es estable si mantiene el orden relativo de los elementos iguales. La estabilidad es crucial en aplicaciones donde el orden original de los elementos iguales debe preservarse. Algunos algoritmos pueden aprovechar el orden existente en los datos para mejorar su rendimiento, mostrando as√≠ una adaptabilidad eficiente. La complejidad del algoritmo en el peor caso es una m√©trica importante para evaluar su robustez (Knuth, 2011).\nLa elecci√≥n del algoritmo de ordenamiento adecuado depende de varios factores, incluyendo el tama√±o de los datos, el grado de desorden y las restricciones de tiempo y espacio. El algoritmo Heapsort utiliza una estructura de datos llamada heap para ordenar los elementos. Es eficiente en t√©rminos de uso de memoria y tiene una complejidad de \\(O(n \\log n)\\). Por otro lado, el algoritmo Mergesort es un enfoque de divide y vencer√°s que divide la lista en mitades, las ordena recursivamente y luego las fusiona. Es estable y tiene una complejidad de \\(O(n \\log n)\\).\nQuicksort tambi√©n sigue el enfoque de divide y vencer√°s, seleccionando un elemento como pivote y particionando la lista en elementos menores y mayores. Aunque en promedio es r√°pido, puede degradarse a \\(O(n^2)\\) en el peor caso. Bubblesort es un algoritmo simple que compara elementos adyacentes y los intercambia si est√°n en el orden incorrecto. Es ineficiente para grandes conjuntos de datos debido a su complejidad cuadr√°tica. Finalmente, la estructura de datos SkipList es una estructura probabil√≠stica que permite b√∫squedas y ordenamientos eficientes. Aunque no es un algoritmo de ordenamiento tradicional, puede ser utilizada para mantener una lista ordenada de elementos (Cormen, 2009).\nPara evaluar y comparar el rendimiento de estos cinco algoritmos de ordenamiento (Heapsort, Mergesort, Quicksort, Bubblesort y SkipList), se seleccionaron primero los algoritmos bas√°ndose en sus caracter√≠sticas y relevancia pr√°ctica. Luego, se prepararon m√∫ltiples archivos JSON con diferentes niveles de desorden para evaluar cada algoritmo bajo diversas condiciones iniciales. Cada algoritmo se implement√≥ en Python, incluyendo un contador de comparaciones y funciones de temporizaci√≥n para medir la eficiencia y el tiempo de ejecuci√≥n. Los resultados se registraron y organizaron en tablas y gr√°ficos para facilitar la comparaci√≥n visual. El an√°lisis permiti√≥ discutir las ventajas y desventajas de cada algoritmo, considerando factores como el tama√±o de los datos y el grado de desorden."
  },
  {
    "objectID": "project3.html#desarrollo",
    "href": "project3.html#desarrollo",
    "title": "Proyecto 3",
    "section": "",
    "text": "# Bibliotecas utilizadas para manejo de archivos, estructuras, algoritmos y visualizaci√≥n\nimport json\nimport os\nimport heapq\nimport random\nimport time\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Esta funci√≥n recorre todos los archivos .json del directorio y carga sus listas de posteo\n# Decid√≠ separarlo en una funci√≥n para poder reutilizarlo f√°cilmente en otros experimentos\n\ndef load_json_files(directory):\n    all_files_data = {}\n    for filename in os.listdir(directory):\n        if filename.endswith('.json'):\n            file_path = os.path.join(directory, filename)\n            with open(file_path, 'r') as file:\n                data = json.load(file)\n                listas_posteo = list(data.values())\n                all_files_data[filename] = listas_posteo\n    return all_files_data\n\n# Ruta fija al directorio local donde se ubican las listas de posteo perturbadas\n# Este path es local, pero se puede cambiar f√°cilmente para otro entorno\n\ndirectory_path = 'C:\\\\Users\\\\Antonio Mart√≠nez\\\\Desktop\\\\listas-posteo-con-perturbaciones' \nfiles_data = load_json_files(directory_path)\n\n\n\n\n\n# Uso de heapq para convertir la lista en un heap\n# Cada extracci√≥n del m√≠nimo simula la ordenaci√≥n ascendente\n\ndef heapsort(arr):\n    comparisons = 0\n    heapq.heapify(arr)\n    sorted_arr = []\n    while arr:\n        comparisons += 1\n        sorted_arr.append(heapq.heappop(arr))\n    return sorted_arr, comparisons\n\n\n\n# Mergesort implementado de manera recursiva\n# Es muy √∫til para listas grandes, ya que divide y conquista\n\ndef mergesort(arr):\n    comparisons = 0\n\n    def merge_sort_recursive(arr):\n        nonlocal comparisons\n        if len(arr) &gt; 1:\n            mid = len(arr) // 2\n            L = arr[:mid]\n            R = arr[mid:]\n\n            merge_sort_recursive(L)\n            merge_sort_recursive(R)\n\n            i = j = k = 0\n            while i &lt; len(L) and j &lt; len(R):\n                comparisons += 1\n                if L[i] &lt; R[j]:\n                    arr[k] = L[i]\n                    i += 1\n                else:\n                    arr[k] = R[j]\n                    j += 1\n                k += 1\n\n            while i &lt; len(L):\n                arr[k] = L[i]\n                i += 1\n                k += 1\n\n            while j &lt; len(R):\n                arr[k] = R[j]\n                j += 1\n                k += 1\n\n    merge_sort_recursive(arr)\n    return arr, comparisons\n\n\n\n# Uso de Quicksort con selecci√≥n del pivote en la posici√≥n media\n# Aument√© el conteo de comparaciones para evaluar su eficiencia\n\ndef quicksort(arr):\n    comparisons = 0\n\n    def _quicksort(arr, low, high):\n        nonlocal comparisons\n        if low &lt; high:\n            pi = partition(arr, low, high)\n            _quicksort(arr, low, pi - 1)\n            _quicksort(arr, pi + 1, high)\n\n    def partition(arr, low, high):\n        nonlocal comparisons\n        mid = (low + high) // 2\n        pivot = arr[mid]\n        arr[mid], arr[high] = arr[high], arr[mid]\n        i = low - 1\n        for j in range(low, high):\n            comparisons += 1\n            if arr[j] &lt;= pivot:\n                i += 1\n                arr[i], arr[j] = arr[j], arr[i]\n        arr[i+1], arr[high] = arr[high], arr[i+1]\n        return i+1\n\n    _quicksort(arr, 0, len(arr)-1)\n    return arr, comparisons\n\n\n\n# Algoritmo simple pero ineficiente para listas grandes\n# Le agregu√© una condici√≥n para detenerse si ya est√° ordenada\n\ndef bubblesort(arr):\n    n = len(arr)\n    comparisons = 0\n    for i in range(n):\n        swapped = False\n        for j in range(0, n-i-1):\n            comparisons += 1\n            if arr[j] &gt; arr[j+1]:\n                arr[j], arr[j+1] = arr[j+1], arr[j]\n                swapped = True\n        if not swapped:\n            break\n    return arr, comparisons\n\n\n\n# Implementaci√≥n b√°sica de SkipList adaptada para ordenar\n# Usa niveles probabil√≠sticos para simular ordenamiento eficiente\n\nclass SkipListNode:\n    def __init__(self, value, level):\n        self.value = value\n        self.forward = [None] * (level + 1)\n\nclass SkipList:\n    def __init__(self, max_level, p=0.5):\n        self.max_level = max_level\n        self.p = p\n        self.header = SkipListNode(None, max_level)\n        self.level = 0\n        self.comparisons = 0\n\n    def random_level(self):\n        lvl = 0\n        while random.random() &lt; self.p and lvl &lt; self.max_level:\n            lvl += 1\n        return lvl\n\n    def insert(self, value):\n        update = [None] * (self.max_level + 1)\n        current = self.header\n\n        for i in reversed(range(self.level + 1)):\n            while current.forward[i] and current.forward[i].value &lt; value:\n                self.comparisons += 1\n                current = current.forward[i]\n            update[i] = current\n\n        lvl = self.random_level()\n        if lvl &gt; self.level:\n            for i in range(self.level + 1, lvl + 1):\n                update[i] = self.header\n            self.level = lvl\n\n        new_node = SkipListNode(value, lvl)\n        for i in range(lvl + 1):\n            new_node.forward[i] = update[i].forward[i]\n            update[i].forward[i] = new_node\n\n    def traverse(self):\n        result = []\n        current = self.header.forward[0]\n        while current:\n            result.append(current.value)\n            current = current.forward[0]\n        return result\n\ndef skiplist_sort(arr):\n    max_level = 16\n    sl = SkipList(max_level)\n    for value in arr:\n        sl.insert(value)\n    sorted_arr = sl.traverse()\n    return sorted_arr, sl.comparisons\n\n\n\n\n# Esta funci√≥n me permiti√≥ unificar c√≥mo mido el tiempo y el n√∫mero de comparaciones\n# para cada algoritmo. La utilic√© en todos los experimentos posteriores.\n\ndef measure_sorting_algorithm(algorithm, arr):\n    start_time = time.time()\n    sorted_arr, comparisons = algorithm(arr.copy())\n    end_time = time.time()\n    return sorted_arr, comparisons, end_time - start_time\n\n\n\n# Ejecuto todos los algoritmos en cada lista del dataset\n# Almaceno comparaciones y tiempo para analizarlos m√°s adelante\n\nall_files_results = {}\n\nfor filename, listas_posteo in files_data.items():\n    all_results = []\n    for lista in listas_posteo:\n        results = {}\n        for name, algo in [\n            (\"Heapsort\", heapsort),\n            (\"Mergesort\", mergesort),\n            (\"Quicksort\", quicksort),\n            (\"Bubblesort\", bubblesort),\n            (\"Skiplist\", skiplist_sort)\n        ]:\n            sorted_arr, comparisons, time_taken = measure_sorting_algorithm(algo, lista)\n            results[name] = {'Comparaciones': comparisons, 'Tiempo': time_taken}\n        all_results.append(results)\n    all_files_results[filename] = all_results\n\n\n\n# Para cada archivo muestro un resumen con barras comparativas de tiempo y comparaciones\n# Las comparaciones van en escala logar√≠tmica para mayor visibilidad\n\nfor filename, results in all_files_results.items():\n    print(f\"\\nResultados para el archivo: {filename}\")\n    results_df = pd.DataFrame(results)\n\n    comparisons_df = results_df.applymap(lambda x: x['Comparaciones']).mean()\n    time_df = results_df.applymap(lambda x: x['Tiempo']).mean()\n\n    print(\"\\nPromedio de Comparaciones por Algoritmo:\\n\", comparisons_df)\n    print(\"\\nPromedio de Tiempo por Algoritmo:\\n\", time_df)\n\n    # Comparaciones (escala logar√≠tmica)\n    ax = comparisons_df.plot(kind='bar', title=f'Promedio de Comparaciones por Algoritmo - {filename}',\n                              color='skyblue', log=True)\n    plt.ylabel('N√∫mero de Comparaciones (escala logar√≠tmica)')\n    plt.xticks(rotation=45)\n    plt.tight_layout()\n\n    for p in ax.patches:\n        ax.annotate(f'{p.get_height():.2f}', \n                    (p.get_x() + p.get_width() / 2., p.get_height()), \n                    ha='center', va='center', \n                    xytext=(0, 10), textcoords='offset points')\n    plt.show()\n\n    # Tiempos de ejecuci√≥n\n    ax = time_df.plot(kind='bar', title=f'Promedio de Tiempo por Algoritmo - {filename}', color='lightgreen')\n    plt.ylabel('Tiempo (segundos)')\n    plt.xticks(rotation=45)\n    plt.tight_layout()\n\n    for p in ax.patches:\n        ax.annotate(f'{p.get_height():.4f}', \n                    (p.get_x() + p.get_width() / 2., p.get_height()), \n                    ha='center', va='center', \n                    xytext=(0, 10), textcoords='offset points')\n\n    ax.set_ylim(0, time_df.max() * 1.25)\n    plt.show()\n\n\n\nTras revisar las observaciones recibidas, se realizaron mejoras clave en la implementaci√≥n y an√°lisis de los algoritmos. En primer lugar, se corrigi√≥ el algoritmo de Bubble Sort para que fuera adaptativo, lo cual se refleja claramente en las gr√°ficas y en el elevado n√∫mero de comparaciones cuando se enfrenta a casos desfavorables.\nAsimismo, se optimiz√≥ Merge Sort para evitar el uso innecesario de memoria adicional, mejorando as√≠ su eficiencia pr√°ctica. Tambi√©n se corrigi√≥ la selecci√≥n del pivote en Quick Sort, utilizando ahora el elemento central para evitar los peores casos en listas parcialmente ordenadas.\nEn cuanto a SkipList, se elimin√≥ el uso del valor artificial -‚àû, garantizando que todas las comparaciones se realicen solo entre elementos reales, como exige el modelo de comparaci√≥n.\nFinalmente, se ajustaron las escalas de las gr√°ficas, empleando una escala logar√≠tmica para las comparaciones y m√°rgenes din√°micos para el tiempo, facilitando una interpretaci√≥n visual m√°s clara y justa.\nCon estas mejoras, el trabajo queda alineado con los lineamientos discutidos y refleja correctamente el comportamiento de cada algoritmo."
  },
  {
    "objectID": "project3.html#an√°lisis-de-resultados",
    "href": "project3.html#an√°lisis-de-resultados",
    "title": "Proyecto 3",
    "section": "3. An√°lisis de Resultados",
    "text": "3. An√°lisis de Resultados"
  },
  {
    "objectID": "project3.html#conclusiones",
    "href": "project3.html#conclusiones",
    "title": "Proyecto 3",
    "section": "",
    "text": "Al finalizar este an√°lisis, puedo concluir que Heapsort fue, sin duda, el algoritmo m√°s eficiente y consistente a lo largo de todos los escenarios evaluados. Sin importar el nivel de perturbaci√≥n introducido en las listas de posteo, su rendimiento se mantuvo pr√°cticamente invariable, tanto en n√∫mero de comparaciones como en tiempo de ejecuci√≥n. Esto lo posiciona como una opci√≥n muy confiable en contextos donde los datos no est√°n completamente ordenados.\nPor el contrario, Bubble Sort demostr√≥ ser el menos adecuado para este tipo de condiciones, registrando cifras excesivas de comparaciones y tiempos considerablemente altos en todos los casos. Su sensibilidad al desorden confirma que no es una buena elecci√≥n cuando se trabaja con listas grandes o con alguna alteraci√≥n en el orden.\nMergesort y Quicksort ofrecieron un equilibrio bastante s√≥lido, especialmente en cuanto a tiempos de ejecuci√≥n. Aunque sus comparaciones aumentaron con el grado de perturbaci√≥n, su comportamiento se mantuvo dentro de rangos aceptables, lo que los hace adecuados para escenarios donde se busca rapidez con una tolerancia razonable a la eficiencia.\nEn cuanto a Skiplist, observ√© un rendimiento intermedio. No fue el m√°s r√°pido, pero tampoco present√≥ problemas extremos. Su comportamiento parece depender m√°s de la estructura interna de los datos, lo cual puede ser un factor a considerar si se busca estabilidad.\nFinalmente, algo que considero clave es que la elecci√≥n del algoritmo no debe basarse √∫nicamente en su complejidad te√≥rica. La pr√°ctica muestra que la eficiencia real tambi√©n depende de otros factores como la implementaci√≥n, el tama√±o de los datos, la naturaleza del desorden y caracter√≠sticas como el uso de memoria o la optimizaci√≥n interna. Evaluar estos aspectos permite tomar decisiones m√°s informadas y adecuadas seg√∫n el contexto del problema."
  },
  {
    "objectID": "project3.html#referencias",
    "href": "project3.html#referencias",
    "title": "Proyecto 3",
    "section": "",
    "text": "Cormen, T. H., Leiserson, C. E., Rivest, R. L., & Stein, C. (2009). Introduction to Algorithms (3rd ed.). MIT Press.\nKnuth, D. E. (2011). The Art of Computer Programming, Volume 4A: Combinatorial Algorithms, Part 1. Addison-Wesley.\nRizvi, Q., Rai, H., & Jaiswal, R. (2024). Sorting Algorithms in Focus: A Critical Examination of Sorting Algorithm Performance.\nHuyen, C. (2022). Designing Machine Learning Systems. O‚ÄôReilly Media, Inc."
  },
  {
    "objectID": "project3.html#cambios-realizados",
    "href": "project3.html#cambios-realizados",
    "title": "Proyecto 3",
    "section": "",
    "text": "Con base en las observaciones realizadas durante la videoconferencia y en las notas complementarias, realic√© una serie de ajustes importantes en el desarrollo de mi trabajo.\nPrimero, correg√≠ la afirmaci√≥n sobre el algoritmo Bubble Sort, aclarando que no es un algoritmo adaptativo. Esta correcci√≥n fue reflejada tanto en la descripci√≥n como en la discusi√≥n de los resultados.\nTambi√©n ajust√© las escalas de las gr√°ficas generadas para cada algoritmo, con el objetivo de permitir una mejor visualizaci√≥n y comparaci√≥n de sus comportamientos, especialmente en casos con tama√±os de entrada moderados.\nEn cuanto a Merge Sort, revis√© la implementaci√≥n para evitar el uso innecesario de memoria adicional, ya que esto afectaba negativamente su eficiencia.\nPara Quick Sort, correg√≠ la estrategia de selecci√≥n del pivote, siguiendo las recomendaciones revisadas en clase, y document√© c√≥mo esta decisi√≥n influye directamente en el rendimiento del algoritmo.\nPor √∫ltimo, elimin√© el uso del n√∫mero ‚Äú√≠nfimo‚Äù en la implementaci√≥n de Skip List, ya que comprend√≠ que no era necesario dentro del modelo de comparaci√≥n. Tambi√©n evit√© definir n√∫meros espec√≠ficos en los an√°lisis, respetando el enfoque abstracto que se abord√≥ en el curso.\nEstos cambios me ayudaron a alinear mejor mi trabajo con los principios te√≥ricos del an√°lisis algor√≠tmico y a fortalecer mi comprensi√≥n de los conceptos trabajados."
  },
  {
    "objectID": "project1.html",
    "href": "project1.html",
    "title": "Proyecto 1",
    "section": "",
    "text": "En la era digital actual, el big data ha emergido como un recurso valioso para las organizaciones, permiti√©ndoles extraer, procesar y analizar datos significativos. El big data se caracteriza por sus cuatro V: volumen, velocidad, variedad y veracidad. El volumen se refiere a la gran cantidad de datos generados y almacenados; la velocidad, a la rapidez con la que se generan y procesan; la variedad, a los diferentes tipos de datos (estructurados y no estructurados); y la veracidad, a la calidad y precisi√≥n de dichos datos. La capacidad de manejar y analizar estos grandes vol√∫menes de informaci√≥n de manera eficiente es crucial para aprovechar al m√°ximo el potencial del big data (M√ºller et al., 2016).\nEl an√°lisis de big data permite a las organizaciones identificar patrones, predecir tendencias y optimizar procesos, lo que puede resultar en mejoras significativas en la eficiencia operativa y en la toma de decisiones estrat√©gicas. Sin embargo, la manipulaci√≥n de grandes vol√∫menes de datos presenta desaf√≠os importantes en t√©rminos de infraestructura, almacenamiento, procesamiento y an√°lisis. Los algoritmos eficientes son fundamentales para enfrentar estos desaf√≠os y garantizar que los sistemas de big data funcionen de manera √≥ptima (Manyika et al., 2011).\nEste reporte se enfoca en comparar diferentes √≥rdenes de crecimiento mediante simulaciones en un entorno de Jupyter. Se analizar√°n los siguientes casos: \\(O(1)\\) vs \\(O(\\log n)\\), \\(O(n)\\) vs \\(O(n \\log n)\\), \\(O(n^2)\\) vs \\(O(n^3)\\), \\(O(a^n)\\) vs \\(O(n!)\\) y \\(O(n!)\\) vs \\(O(n^n)\\). Para cada comparaci√≥n, se seleccionar√°n rangos adecuados de \\(n\\) que permitan visualizar claramente las diferencias entre estos √≥rdenes. Se generar√°n gr√°ficas para cada caso y se discutir√°n las observaciones correspondientes. Adem√°s, se incluir√° una tabla con tiempos de ejecuci√≥n simulados para algoritmos ficticios asociados a los √≥rdenes de crecimiento mencionados, utilizando distintos tama√±os de entrada \\(n\\). Este an√°lisis proporciona una visi√≥n clara de c√≥mo los diferentes √≥rdenes de crecimiento afectan el rendimiento y la eficiencia de los algoritmos.\n\n\n\n\n\nimport numpy as np\nimport pandas as pd\nimport math\nimport matplotlib.pyplot as plt\n\n\n\ndef constant(n): return 1\ndef logarithmic(n): return np.log(n)\ndef linear(n): return n\ndef linear_logarithmic(n): return n * np.log(n)\ndef quadratic(n): return n**2\ndef cubic(n): return n**3\ndef exponential(n, a=2): return a**n\ndef factorial(n): return math.factorial(n)\ndef double_exponential(n): return n**n\n\n\n\ndef plot_comparison(funcs, labels, title, x_range):\n    plt.figure(figsize=(7, 5))\n    for func, label in zip(funcs, labels):\n        plt.plot(x_range, [func(x) / 1e9 for x in x_range], label=label)\n    plt.xlabel('n')\n    plt.ylabel('Tiempo (s)')\n    plt.title(title)\n    plt.legend()\n    plt.yscale('log')\n    plt.xscale('log')\n    plt.grid(True)\n    plt.show()\n\n\n\nx_range_small = np.arange(1, 10)\nx_range_medium = np.arange(1, 100)\n\n# Comparaci√≥n 1: O(1) vs O(log n)\nplot_comparison([constant, logarithmic], ['O(1)', 'O(log n)'], 'Figura 1 - Comparaci√≥n de O(1) con O(log n)', x_range_small)\n\n# Comparaci√≥n 2: O(n) vs O(n log n)\nplot_comparison([linear, linear_logarithmic], ['O(n)', 'O(n log n)'], 'Figura 2 - Comparaci√≥n de O(n) con O(n log n)', x_range_medium)\n\n# Comparaci√≥n 3: O(n^2) vs O(n^3)\nplot_comparison([quadratic, cubic], ['O(n^2)', 'O(n^3)'], 'Figura 3 - Comparaci√≥n de O(n^2) con O(n^3)', x_range_medium)\n\n# Comparaci√≥n 4: O(2^n) vs O(n!)\nplot_comparison([lambda n: exponential(n, 2), factorial], ['O(2^n)', 'O(n!)'], 'Figura 4 - Comparaci√≥n de O(2^n) con O(n!)', x_range_small)\n\n# Comparaci√≥n 5: O(n!) vs O(n^n)\nplot_comparison([factorial, double_exponential], ['O(n!)', 'O(n^n)'], 'Figura 5 - Comparaci√≥n de O(n!) con O(n^n)', x_range_small)\n\n\n\nNota: Se cre√≥ una tabla con tiempos de ejecuci√≥n simulados para algoritmos ficticios con los √≥rdenes de crecimiento mencionados.\n# Tama√±os de entrada\nn_values = [100, 1000, 10000, 100000]\n\n# Funciones de costo\ncost_functions = {\n    'O(1)': constant,\n    'O(log n)': logarithmic,\n    'O(n)': linear,\n    'O(n log n)': linear_logarithmic,\n    'O(n^2)': quadratic,\n    'O(n^3)': cubic,\n    'O(2^n)': lambda n: exponential(n, 2),\n    'O(n!)': factorial,\n    'O(n^n)': double_exponential\n}\n\n# Elaboraci√≥n de la tabla\nresults = []\nfor n in n_values:\n    row = {'n': n}\n    for label, func in cost_functions.items():\n        try:\n            row[label] = func(n) / 1e9  # Conversi√≥n a segundos\n        except OverflowError:\n            row[label] = 'Overflow'\n    results.append(row)\n\nimport pandas as pd\ndf = pd.DataFrame(results)\ndf.set_index('n', inplace=True)\ndf\n\n\n\n\n\n\n\n\n\nEl gr√°fico muestra que la funci√≥n constante \\(O(1)\\) permanece constante independientemente del tama√±o de la entrada, mientras que \\(O(\\log n)\\) crece lentamente conforme \\(n\\) aumenta. Por lo tanto, un algoritmo con complejidad constante \\(O(1)\\) es m√°s eficiente en t√©rminos de tiempo de ejecuci√≥n en comparaci√≥n con uno con complejidad logar√≠tmica \\(O(\\log n)\\).\n\n\n\n\n\n\nLa funci√≥n \\(O(n)\\) crece linealmente con el tama√±o de entrada \\(n\\), mientras que la funci√≥n \\(O(n \\log n)\\) crece m√°s r√°pido que \\(O(n)\\), pero sigue siendo pr√°ctica para valores moderados de \\(n\\). Por lo tanto, un algoritmo con complejidad lineal \\(O(n)\\) es m√°s eficiente en t√©rminos de tiempo de ejecuci√≥n en comparaci√≥n con uno con complejidad lineal-logar√≠tmica \\(O(n \\log n)\\).\n\n\n\n\n\n\nLa funci√≥n cuadr√°tica \\(O(n^2)\\) crece m√°s r√°pidamente que la lineal, pero la funci√≥n c√∫bica \\(O(n^3)\\) lo hace a√∫n con mayor rapidez. Por lo tanto, un algoritmo con complejidad cuadr√°tica \\(O(n^2)\\) es m√°s eficiente en t√©rminos de tiempo de ejecuci√≥n en comparaci√≥n con uno con complejidad c√∫bica \\(O(n^3)\\).\n\n\n\n\n\n\nEl gr√°fico muestra que la complejidad factorial \\(O(n!)\\) resulta en tiempos de ejecuci√≥n mucho m√°s altos que \\(O(2^n)\\) a medida que el tama√±o de la entrada \\(n\\) aumenta. La complejidad factorial \\(O(n!)\\) crece considerablemente m√°s r√°pido que la complejidad exponencial \\(O(2^n)\\), lo que hace que los algoritmos con esta complejidad sean pr√°cticamente inutilizables para valores grandes de \\(n\\).\n\n\n\n\n\n\nEl gr√°fico muestra c√≥mo la funci√≥n factorial \\(O(n!)\\) crece muy r√°pidamente, pero la funci√≥n doble exponencial \\(O(n^n)\\) crece a√∫n m√°s r√°pido a medida que aumenta el tama√±o de la entrada \\(n\\). Por lo tanto, los algoritmos con complejidad factorial y doble exponencial son pr√°cticamente inutilizables para valores grandes de \\(n\\).\n\n\n\n\n\n\n\n\nA continuaci√≥n se presenta una tabla comparativa de los tiempos simulados para diferentes √≥rdenes de crecimiento, utilizando distintos tama√±os de entrada \\(n\\). Los resultados se expresan en segundos. En algunos casos, se muestra ‚ÄúOverflow‚Äù cuando el valor resultante excede los l√≠mites de representaci√≥n.\nTabla 1. Tiempos simulados para diferentes √≥rdenes de crecimiento de O(1), O(log n), O(n), O(n log n), O(n¬≤)\n\n\n\n\n\n\n\n\n\n\n\nn\nO(1)\nO(log n)\nO(n)\nO(n log n)\nO(n¬≤)\n\n\n\n\n100\n1.00e-09\n4.61e-09\n1.00e-07\n4.61e-07\n1.00e-05\n\n\n1000\n1.00e-09\n6.91e-09\n1.00e-06\n6.91e-06\n1.00e-03\n\n\n10000\n1.00e-09\n9.21e-09\n1.00e-05\n9.21e-05\n1.00e-01\n\n\n100000\n1.00e-09\n1.15e-08\n1.00e-04\n1.15e-03\n1.00e+01\n\n\n\nTabla 2. Tiempos simulados para diferentes √≥rdenes de crecimiento de O(n¬≥), O(2‚Åø)\n\n\n\nn\nO(n¬≥)\nO(2‚Åø)\n\n\n\n\n100\n1.00e-03\n1.27e+21\n\n\n1000\n1.00e+00\nOverflow\n\n\n10000\n1.00e+03\nOverflow\n\n\n100000\n1.00e+06\nOverflow\n\n\n\nTabla 3. Tiempos simulados para diferentes √≥rdenes de crecimiento de O(n!), O(n‚Åø)\n\n\n\nn\nO(n!)\nO(n‚Åø)\n\n\n\n\n100\nOverflow\nOverflow\n\n\n1000\nOverflow\nOverflow\n\n\n10000\nOverflow\nOverflow\n\n\n100000\nOverflow\nOverflow\n\n\n\n\nNota: Algunos valores como O(2‚Åø), O(n!) y O(n‚Åø) para valores grandes de \\(n\\) generan cantidades inmanejables por el sistema, resultando en Overflow. Esto refleja la inviabilidad pr√°ctica de algoritmos con estas complejidades cuando se trabaja con grandes vol√∫menes de datos.\n\nComo se muestra en la tabla, el tiempo de ejecuci√≥n de O(1) no depende del tama√±o de n.¬†Este mantiene un mismo valor, lo que indica que su tiempo de ejecuci√≥n es constante. Por otro lado, los valores de O(log n) aumentan lentamente a medida que n crece, lo cual es consistente con su comportamiento logar√≠tmico. En el caso de O(n), los valores aumentan de forma lineal conforme n se incrementa. Los valores de O(n log n) aumentan m√°s r√°pido que los de O(n), pero no tan aceleradamente como los de O(n¬≤). Por su parte, O(n¬≤) crece r√°pidamente a medida que n aumenta, y O(n¬≥) lo hace de forma a√∫n m√°s acelerada, incluso con valores peque√±os de entrada.\nDurante la ejecuci√≥n del c√≥digo, algunos resultados aparecen como ‚ÄúOverflow‚Äù. Esto se debe a que las funciones O(2‚Åø), O(n!) y O(n‚Åø) para valores grandes de n (como 10,000 o 100,000) generan n√∫meros extremadamente grandes, lo que excede la capacidad de representaci√≥n y manejo num√©rico en Python.\n\n\n\n\nLa manipulaci√≥n de grandes vol√∫menes de informaci√≥n presenta importantes desaf√≠os debido a los costos de c√≥mputo. Estos costos se relacionan con el gasto monetario, los recursos computacionales requeridos, el tiempo de procesamiento, el uso de memoria, el almacenamiento y el consumo energ√©tico. A continuaci√≥n se presentan algunas de las implicaciones m√°s relevantes:\n\nInfraestructura: Manipular grandes vol√∫menes de datos requiere una infraestructura robusta, que incluye servidores potentes, almacenamiento masivo y redes de alta velocidad. Esto representa costos elevados en hardware, mantenimiento y consumo energ√©tico (Armbrust et al., 2010).\nEnerg√≠a: Los centros de datos que procesan grandes cantidades de informaci√≥n consumen enormes cantidades de energ√≠a, lo cual incrementa los costos operativos y contribuye al impacto ambiental debido a las emisiones de carbono (Baliga et al., 2011).\nAlmacenamiento: El almacenamiento de grandes vol√∫menes de datos requiere soluciones escalables y eficientes. Este aspecto representa un costo considerable, ya que tambi√©n debe garantizarse la recuperaci√≥n oportuna de la informaci√≥n.\nProcesamiento: Procesar datos en tiempo real o en tiempo cercano al real implica el uso de algoritmos eficientes y de recursos de c√≥mputo significativos. Estos costos pueden incrementarse especialmente al utilizar tecnolog√≠as como el aprendizaje autom√°tico y la inteligencia artificial (Wang et al., 2015).\n\nEn conjunto, la gesti√≥n de grandes vol√∫menes de informaci√≥n conlleva costos considerables en t√©rminos de infraestructura, energ√≠a, almacenamiento, procesamiento, seguridad y mantenimiento. Estos aspectos deben ser cuidadosamente planeados y gestionados para asegurar soluciones de big data eficientes y sostenibles.\n\n\n\n\nEn las simulaciones realizadas se observ√≥ que los √≥rdenes de crecimiento m√°s bajos, como O(1), O(log n) y O(n), resultan altamente eficientes. Son ideales para tareas que demandan rendimiento √≥ptimo y tiempos de respuesta reducidos.\nPor otro lado, los √≥rdenes de crecimiento O(n log n) y O(n¬≤) demostraron ser pr√°cticas y ampliamente aplicables, al ofrecer un buen equilibrio entre eficiencia y capacidad de procesamiento.\nEn contraste, las funciones con √≥rdenes de crecimiento elevados como O(2‚Åø), O(n!) y O(n‚Åø) resultaron ineficientes, generando tiempos de ejecuci√≥n excesivos o incluso errores de memoria.\nLa tabla de tiempos de ejecuci√≥n simulados confirma c√≥mo el crecimiento del orden afecta directamente el rendimiento de los algoritmos. A mayor orden, mayor consumo de recursos y tiempo; a menor orden, mayor eficiencia en la ejecuci√≥n.\n\n\n\n\nM√ºller, V. C., Schal, J. M., Meyer-Lindenberg, A. (2016). Machine Learning for Brain Imaging. Cambridge University Press.\nManyika, J., Chui, M., Brown, B., Bughin, J., Dobbs, R., Roxburgh, C., & Byers, A. H. (2011). Big data: The next frontier for innovation, competition, and productivity. McKinsey Global Institute. https://www.mckinsey.com/business-functions/mckinsey-analytics/our-insights/big-data-the-next-frontier-for-innovation\nArmbrust, M., Fox, A., Griffith, R., Joseph, A. D., Katz, R. H., Konwinski, A., et al.¬†(2010). A View of Cloud Computing. Communications of the ACM, 53(4), 50‚Äì58. https://doi.org/10.1145/1721654.1721672\nBaliga, J., Ayre, R., Hinton, K., & Tucker, R. S. (2011). Energy-Efficient Telecommunications.\nWang, L., Chen, Y., & Liu, Q. (2015). Big Data Processing: A Survey. Springer.\n\n\n\n\n\n\nLos siguientes ajustes fueron aplicados al presente documento en atenci√≥n a la retroalimentaci√≥n recibida por parte del docente. Se atendieron los puntos se√±alados con el fin de mejorar la calidad formal y acad√©mica del reporte:\n\nSe uniform√≥ el formato del documento, manteniendo una estructura clara y coherente entre secciones.\nSe corrigieron errores ortogr√°ficos y gramaticales, incluyendo el uso adecuado de comas y acentos.\nLas expresiones matem√°ticas fueron reescritas utilizando la notaci√≥n correcta en formato de ecuaci√≥n (LaTeX), como es el caso de \\(O(1)\\), \\(O(\\log n)\\), \\(O(n^2)\\), entre otras.\nSe estandarizaron los encabezados de cada secci√≥n para facilitar la lectura y navegaci√≥n del documento.\n\nEstas modificaciones aseguran que el trabajo cumpla con los criterios de presentaci√≥n solicitados y refleje un esfuerzo riguroso en la construcci√≥n y exposici√≥n del contenido."
  },
  {
    "objectID": "project1.html#introducci√≥n",
    "href": "project1.html#introducci√≥n",
    "title": "Proyecto 1",
    "section": "",
    "text": "En la era digital actual, el big data ha emergido como un recurso valioso para las organizaciones, permiti√©ndoles extraer, procesar y analizar datos significativos. El big data se caracteriza por sus cuatro V: volumen, velocidad, variedad y veracidad. El volumen se refiere a la gran cantidad de datos generados y almacenados; la velocidad, a la rapidez con la que se generan y procesan; la variedad, a los diferentes tipos de datos (estructurados y no estructurados); y la veracidad, a la calidad y precisi√≥n de dichos datos. La capacidad de manejar y analizar estos grandes vol√∫menes de informaci√≥n de manera eficiente es crucial para aprovechar al m√°ximo el potencial del big data (M√ºller et al., 2016).\nEl an√°lisis de big data permite a las organizaciones identificar patrones, predecir tendencias y optimizar procesos, lo que puede resultar en mejoras significativas en la eficiencia operativa y en la toma de decisiones estrat√©gicas. Sin embargo, la manipulaci√≥n de grandes vol√∫menes de datos presenta desaf√≠os importantes en t√©rminos de infraestructura, almacenamiento, procesamiento y an√°lisis. Los algoritmos eficientes son fundamentales para enfrentar estos desaf√≠os y garantizar que los sistemas de big data funcionen de manera √≥ptima (Manyika et al., 2011).\nEste reporte se enfoca en comparar diferentes √≥rdenes de crecimiento mediante simulaciones en un entorno de Jupyter. Se analizar√°n los siguientes casos: \\(O(1)\\) vs \\(O(\\log n)\\), \\(O(n)\\) vs \\(O(n \\log n)\\), \\(O(n^2)\\) vs \\(O(n^3)\\), \\(O(a^n)\\) vs \\(O(n!)\\) y \\(O(n!)\\) vs \\(O(n^n)\\). Para cada comparaci√≥n, se seleccionar√°n rangos adecuados de \\(n\\) que permitan visualizar claramente las diferencias entre estos √≥rdenes. Se generar√°n gr√°ficas para cada caso y se discutir√°n las observaciones correspondientes. Adem√°s, se incluir√° una tabla con tiempos de ejecuci√≥n simulados para algoritmos ficticios asociados a los √≥rdenes de crecimiento mencionados, utilizando distintos tama√±os de entrada \\(n\\). Este an√°lisis proporciona una visi√≥n clara de c√≥mo los diferentes √≥rdenes de crecimiento afectan el rendimiento y la eficiencia de los algoritmos."
  },
  {
    "objectID": "project1.html#desarrollo",
    "href": "project1.html#desarrollo",
    "title": "Proyecto 1",
    "section": "",
    "text": "import numpy as np\nimport pandas as pd\nimport math\nimport matplotlib.pyplot as plt\n\n\n\ndef constant(n): return 1\ndef logarithmic(n): return np.log(n)\ndef linear(n): return n\ndef linear_logarithmic(n): return n * np.log(n)\ndef quadratic(n): return n**2\ndef cubic(n): return n**3\ndef exponential(n, a=2): return a**n\ndef factorial(n): return math.factorial(n)\ndef double_exponential(n): return n**n\n\n\n\ndef plot_comparison(funcs, labels, title, x_range):\n    plt.figure(figsize=(7, 5))\n    for func, label in zip(funcs, labels):\n        plt.plot(x_range, [func(x) / 1e9 for x in x_range], label=label)\n    plt.xlabel('n')\n    plt.ylabel('Tiempo (s)')\n    plt.title(title)\n    plt.legend()\n    plt.yscale('log')\n    plt.xscale('log')\n    plt.grid(True)\n    plt.show()\n\n\n\nx_range_small = np.arange(1, 10)\nx_range_medium = np.arange(1, 100)\n\n# Comparaci√≥n 1: O(1) vs O(log n)\nplot_comparison([constant, logarithmic], ['O(1)', 'O(log n)'], 'Figura 1 - Comparaci√≥n de O(1) con O(log n)', x_range_small)\n\n# Comparaci√≥n 2: O(n) vs O(n log n)\nplot_comparison([linear, linear_logarithmic], ['O(n)', 'O(n log n)'], 'Figura 2 - Comparaci√≥n de O(n) con O(n log n)', x_range_medium)\n\n# Comparaci√≥n 3: O(n^2) vs O(n^3)\nplot_comparison([quadratic, cubic], ['O(n^2)', 'O(n^3)'], 'Figura 3 - Comparaci√≥n de O(n^2) con O(n^3)', x_range_medium)\n\n# Comparaci√≥n 4: O(2^n) vs O(n!)\nplot_comparison([lambda n: exponential(n, 2), factorial], ['O(2^n)', 'O(n!)'], 'Figura 4 - Comparaci√≥n de O(2^n) con O(n!)', x_range_small)\n\n# Comparaci√≥n 5: O(n!) vs O(n^n)\nplot_comparison([factorial, double_exponential], ['O(n!)', 'O(n^n)'], 'Figura 5 - Comparaci√≥n de O(n!) con O(n^n)', x_range_small)\n\n\n\nNota: Se cre√≥ una tabla con tiempos de ejecuci√≥n simulados para algoritmos ficticios con los √≥rdenes de crecimiento mencionados.\n# Tama√±os de entrada\nn_values = [100, 1000, 10000, 100000]\n\n# Funciones de costo\ncost_functions = {\n    'O(1)': constant,\n    'O(log n)': logarithmic,\n    'O(n)': linear,\n    'O(n log n)': linear_logarithmic,\n    'O(n^2)': quadratic,\n    'O(n^3)': cubic,\n    'O(2^n)': lambda n: exponential(n, 2),\n    'O(n!)': factorial,\n    'O(n^n)': double_exponential\n}\n\n# Elaboraci√≥n de la tabla\nresults = []\nfor n in n_values:\n    row = {'n': n}\n    for label, func in cost_functions.items():\n        try:\n            row[label] = func(n) / 1e9  # Conversi√≥n a segundos\n        except OverflowError:\n            row[label] = 'Overflow'\n    results.append(row)\n\nimport pandas as pd\ndf = pd.DataFrame(results)\ndf.set_index('n', inplace=True)\ndf"
  },
  {
    "objectID": "project1.html#an√°lisis-de-resultados",
    "href": "project1.html#an√°lisis-de-resultados",
    "title": "Proyecto 1",
    "section": "",
    "text": "El gr√°fico muestra que la funci√≥n constante \\(O(1)\\) permanece constante independientemente del tama√±o de la entrada, mientras que \\(O(\\log n)\\) crece lentamente conforme \\(n\\) aumenta. Por lo tanto, un algoritmo con complejidad constante \\(O(1)\\) es m√°s eficiente en t√©rminos de tiempo de ejecuci√≥n en comparaci√≥n con uno con complejidad logar√≠tmica \\(O(\\log n)\\).\n\n\n\n\n\n\nLa funci√≥n \\(O(n)\\) crece linealmente con el tama√±o de entrada \\(n\\), mientras que la funci√≥n \\(O(n \\log n)\\) crece m√°s r√°pido que \\(O(n)\\), pero sigue siendo pr√°ctica para valores moderados de \\(n\\). Por lo tanto, un algoritmo con complejidad lineal \\(O(n)\\) es m√°s eficiente en t√©rminos de tiempo de ejecuci√≥n en comparaci√≥n con uno con complejidad lineal-logar√≠tmica \\(O(n \\log n)\\).\n\n\n\n\n\n\nLa funci√≥n cuadr√°tica \\(O(n^2)\\) crece m√°s r√°pidamente que la lineal, pero la funci√≥n c√∫bica \\(O(n^3)\\) lo hace a√∫n con mayor rapidez. Por lo tanto, un algoritmo con complejidad cuadr√°tica \\(O(n^2)\\) es m√°s eficiente en t√©rminos de tiempo de ejecuci√≥n en comparaci√≥n con uno con complejidad c√∫bica \\(O(n^3)\\).\n\n\n\n\n\n\nEl gr√°fico muestra que la complejidad factorial \\(O(n!)\\) resulta en tiempos de ejecuci√≥n mucho m√°s altos que \\(O(2^n)\\) a medida que el tama√±o de la entrada \\(n\\) aumenta. La complejidad factorial \\(O(n!)\\) crece considerablemente m√°s r√°pido que la complejidad exponencial \\(O(2^n)\\), lo que hace que los algoritmos con esta complejidad sean pr√°cticamente inutilizables para valores grandes de \\(n\\).\n\n\n\n\n\n\nEl gr√°fico muestra c√≥mo la funci√≥n factorial \\(O(n!)\\) crece muy r√°pidamente, pero la funci√≥n doble exponencial \\(O(n^n)\\) crece a√∫n m√°s r√°pido a medida que aumenta el tama√±o de la entrada \\(n\\). Por lo tanto, los algoritmos con complejidad factorial y doble exponencial son pr√°cticamente inutilizables para valores grandes de \\(n\\).\n\n\n\n\n\n\n\n\nA continuaci√≥n se presenta una tabla comparativa de los tiempos simulados para diferentes √≥rdenes de crecimiento, utilizando distintos tama√±os de entrada \\(n\\). Los resultados se expresan en segundos. En algunos casos, se muestra ‚ÄúOverflow‚Äù cuando el valor resultante excede los l√≠mites de representaci√≥n.\nTabla 1. Tiempos simulados para diferentes √≥rdenes de crecimiento de O(1), O(log n), O(n), O(n log n), O(n¬≤)\n\n\n\n\n\n\n\n\n\n\n\nn\nO(1)\nO(log n)\nO(n)\nO(n log n)\nO(n¬≤)\n\n\n\n\n100\n1.00e-09\n4.61e-09\n1.00e-07\n4.61e-07\n1.00e-05\n\n\n1000\n1.00e-09\n6.91e-09\n1.00e-06\n6.91e-06\n1.00e-03\n\n\n10000\n1.00e-09\n9.21e-09\n1.00e-05\n9.21e-05\n1.00e-01\n\n\n100000\n1.00e-09\n1.15e-08\n1.00e-04\n1.15e-03\n1.00e+01\n\n\n\nTabla 2. Tiempos simulados para diferentes √≥rdenes de crecimiento de O(n¬≥), O(2‚Åø)\n\n\n\nn\nO(n¬≥)\nO(2‚Åø)\n\n\n\n\n100\n1.00e-03\n1.27e+21\n\n\n1000\n1.00e+00\nOverflow\n\n\n10000\n1.00e+03\nOverflow\n\n\n100000\n1.00e+06\nOverflow\n\n\n\nTabla 3. Tiempos simulados para diferentes √≥rdenes de crecimiento de O(n!), O(n‚Åø)\n\n\n\nn\nO(n!)\nO(n‚Åø)\n\n\n\n\n100\nOverflow\nOverflow\n\n\n1000\nOverflow\nOverflow\n\n\n10000\nOverflow\nOverflow\n\n\n100000\nOverflow\nOverflow\n\n\n\n\nNota: Algunos valores como O(2‚Åø), O(n!) y O(n‚Åø) para valores grandes de \\(n\\) generan cantidades inmanejables por el sistema, resultando en Overflow. Esto refleja la inviabilidad pr√°ctica de algoritmos con estas complejidades cuando se trabaja con grandes vol√∫menes de datos.\n\nComo se muestra en la tabla, el tiempo de ejecuci√≥n de O(1) no depende del tama√±o de n.¬†Este mantiene un mismo valor, lo que indica que su tiempo de ejecuci√≥n es constante. Por otro lado, los valores de O(log n) aumentan lentamente a medida que n crece, lo cual es consistente con su comportamiento logar√≠tmico. En el caso de O(n), los valores aumentan de forma lineal conforme n se incrementa. Los valores de O(n log n) aumentan m√°s r√°pido que los de O(n), pero no tan aceleradamente como los de O(n¬≤). Por su parte, O(n¬≤) crece r√°pidamente a medida que n aumenta, y O(n¬≥) lo hace de forma a√∫n m√°s acelerada, incluso con valores peque√±os de entrada.\nDurante la ejecuci√≥n del c√≥digo, algunos resultados aparecen como ‚ÄúOverflow‚Äù. Esto se debe a que las funciones O(2‚Åø), O(n!) y O(n‚Åø) para valores grandes de n (como 10,000 o 100,000) generan n√∫meros extremadamente grandes, lo que excede la capacidad de representaci√≥n y manejo num√©rico en Python.\n\n\n\n\nLa manipulaci√≥n de grandes vol√∫menes de informaci√≥n presenta importantes desaf√≠os debido a los costos de c√≥mputo. Estos costos se relacionan con el gasto monetario, los recursos computacionales requeridos, el tiempo de procesamiento, el uso de memoria, el almacenamiento y el consumo energ√©tico. A continuaci√≥n se presentan algunas de las implicaciones m√°s relevantes:\n\nInfraestructura: Manipular grandes vol√∫menes de datos requiere una infraestructura robusta, que incluye servidores potentes, almacenamiento masivo y redes de alta velocidad. Esto representa costos elevados en hardware, mantenimiento y consumo energ√©tico (Armbrust et al., 2010).\nEnerg√≠a: Los centros de datos que procesan grandes cantidades de informaci√≥n consumen enormes cantidades de energ√≠a, lo cual incrementa los costos operativos y contribuye al impacto ambiental debido a las emisiones de carbono (Baliga et al., 2011).\nAlmacenamiento: El almacenamiento de grandes vol√∫menes de datos requiere soluciones escalables y eficientes. Este aspecto representa un costo considerable, ya que tambi√©n debe garantizarse la recuperaci√≥n oportuna de la informaci√≥n.\nProcesamiento: Procesar datos en tiempo real o en tiempo cercano al real implica el uso de algoritmos eficientes y de recursos de c√≥mputo significativos. Estos costos pueden incrementarse especialmente al utilizar tecnolog√≠as como el aprendizaje autom√°tico y la inteligencia artificial (Wang et al., 2015).\n\nEn conjunto, la gesti√≥n de grandes vol√∫menes de informaci√≥n conlleva costos considerables en t√©rminos de infraestructura, energ√≠a, almacenamiento, procesamiento, seguridad y mantenimiento. Estos aspectos deben ser cuidadosamente planeados y gestionados para asegurar soluciones de big data eficientes y sostenibles."
  },
  {
    "objectID": "project1.html#conclusiones",
    "href": "project1.html#conclusiones",
    "title": "Proyecto 1",
    "section": "",
    "text": "En las simulaciones realizadas se observ√≥ que los √≥rdenes de crecimiento m√°s bajos, como O(1), O(log n) y O(n), resultan altamente eficientes. Son ideales para tareas que demandan rendimiento √≥ptimo y tiempos de respuesta reducidos.\nPor otro lado, los √≥rdenes de crecimiento O(n log n) y O(n¬≤) demostraron ser pr√°cticas y ampliamente aplicables, al ofrecer un buen equilibrio entre eficiencia y capacidad de procesamiento.\nEn contraste, las funciones con √≥rdenes de crecimiento elevados como O(2‚Åø), O(n!) y O(n‚Åø) resultaron ineficientes, generando tiempos de ejecuci√≥n excesivos o incluso errores de memoria.\nLa tabla de tiempos de ejecuci√≥n simulados confirma c√≥mo el crecimiento del orden afecta directamente el rendimiento de los algoritmos. A mayor orden, mayor consumo de recursos y tiempo; a menor orden, mayor eficiencia en la ejecuci√≥n."
  },
  {
    "objectID": "project1.html#referencias",
    "href": "project1.html#referencias",
    "title": "Proyecto 1",
    "section": "",
    "text": "M√ºller, V. C., Schal, J. M., Meyer-Lindenberg, A. (2016). Machine Learning for Brain Imaging. Cambridge University Press.\nManyika, J., Chui, M., Brown, B., Bughin, J., Dobbs, R., Roxburgh, C., & Byers, A. H. (2011). Big data: The next frontier for innovation, competition, and productivity. McKinsey Global Institute. https://www.mckinsey.com/business-functions/mckinsey-analytics/our-insights/big-data-the-next-frontier-for-innovation\nArmbrust, M., Fox, A., Griffith, R., Joseph, A. D., Katz, R. H., Konwinski, A., et al.¬†(2010). A View of Cloud Computing. Communications of the ACM, 53(4), 50‚Äì58. https://doi.org/10.1145/1721654.1721672\nBaliga, J., Ayre, R., Hinton, K., & Tucker, R. S. (2011). Energy-Efficient Telecommunications.\nWang, L., Chen, Y., & Liu, Q. (2015). Big Data Processing: A Survey. Springer."
  },
  {
    "objectID": "project1.html#cambios-realizados",
    "href": "project1.html#cambios-realizados",
    "title": "Proyecto 1",
    "section": "",
    "text": "Los siguientes ajustes fueron aplicados al presente documento en atenci√≥n a la retroalimentaci√≥n recibida por parte del docente. Se atendieron los puntos se√±alados con el fin de mejorar la calidad formal y acad√©mica del reporte:\n\nSe uniform√≥ el formato del documento, manteniendo una estructura clara y coherente entre secciones.\nSe corrigieron errores ortogr√°ficos y gramaticales, incluyendo el uso adecuado de comas y acentos.\nLas expresiones matem√°ticas fueron reescritas utilizando la notaci√≥n correcta en formato de ecuaci√≥n (LaTeX), como es el caso de \\(O(1)\\), \\(O(\\log n)\\), \\(O(n^2)\\), entre otras.\nSe estandarizaron los encabezados de cada secci√≥n para facilitar la lectura y navegaci√≥n del documento.\n\nEstas modificaciones aseguran que el trabajo cumpla con los criterios de presentaci√≥n solicitados y refleje un esfuerzo riguroso en la construcci√≥n y exposici√≥n del contenido."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Sobre M√≠",
    "section": "",
    "text": "üìç Aguascalientes, M√©xico\n‚úâÔ∏è juan2javm@gmail.com / jvelasquez1800@alumno.ipn.mx\nüì± +52 322 353 4081\nüîó GitHub: juan21javm\nüîó LinkedIn: antonio-mart√≠nez-776788179\n\n\n\nMe considero una persona profundamente comprometida con el crecimiento intelectual, la lectura reflexiva y el bienestar de mi entorno cercano. Disfruto de los momentos al aire libre, donde encuentro inspiraci√≥n en la naturaleza y la tranquilidad necesaria para mantener un equilibrio emocional. Valoro profundamente el tiempo con mi familia, ya que es el motor que impulsa mi disciplina, responsabilidad y deseo de superaci√≥n constante. Asimismo, tengo una gran pasi√≥n por la programaci√≥n y un genuino entusiasmo por la ciencia de datos, √°reas que me permiten combinar creatividad, l√≥gica y an√°lisis riguroso para resolver problemas del mundo real. Estos principios personales complementan mi formaci√≥n acad√©mica y profesional, motiv√°ndome a mantener siempre una actitud proactiva, √©tica y humana.\n\n\n\n\nSupervisor ‚Äî INE (2023‚Äì2025), Loreto, Zacatecas\n- Supervisi√≥n de actividades diarias para cumplimiento de objetivos.\n- Capacitaci√≥n del equipo y mejora de rendimiento.\n- Evaluaci√≥n de desempe√±o y optimizaci√≥n de procesos.\nMaestro Asistente ‚Äî IPN (2022‚Äì2023), Zacatecas\n- Evaluaci√≥n del progreso estudiantil.\n- Planeaci√≥n conjunta de actividades educativas.\n- Creaci√≥n y adaptaci√≥n de materiales educativos.\n\n\n\n\nProyecto A ‚Äî Estandarizaci√≥n de proceso a nivel laboratorio (2021‚Äì2023)\n- Herramientas: MATLAB, HACCP Software, Latex\n- Liderazgo de investigaci√≥n para producci√≥n de yogurt con uvas a nivel laboratorio.\n- Reconocimiento al mejor promedio del programa acad√©mico (IPN 2017‚Äì2022).\n- Primer lugar en Simposium Agroalimentario y Ciclo de Conferencias CUALTIA 2023.\n\n\n\n\n\nLenguajes: Python, JAVA, HTML, ASPEN\n\nHerramientas Matem√°ticas: MATLAB, R, SPSS, PLC\n\nOfim√°tica: Office, LaTeX\n\n\n\n\n\n\nIdiomas: Espa√±ol (nativo), Ingl√©s (B1)\n\nIntereses: Lectura, b√∫squeda, planeaci√≥n"
  },
  {
    "objectID": "about.html#perfil-personal",
    "href": "about.html#perfil-personal",
    "title": "Sobre M√≠",
    "section": "",
    "text": "Me considero una persona profundamente comprometida con el crecimiento intelectual, la lectura reflexiva y el bienestar de mi entorno cercano. Disfruto de los momentos al aire libre, donde encuentro inspiraci√≥n en la naturaleza y la tranquilidad necesaria para mantener un equilibrio emocional. Valoro profundamente el tiempo con mi familia, ya que es el motor que impulsa mi disciplina, responsabilidad y deseo de superaci√≥n constante. Asimismo, tengo una gran pasi√≥n por la programaci√≥n y un genuino entusiasmo por la ciencia de datos, √°reas que me permiten combinar creatividad, l√≥gica y an√°lisis riguroso para resolver problemas del mundo real. Estos principios personales complementan mi formaci√≥n acad√©mica y profesional, motiv√°ndome a mantener siempre una actitud proactiva, √©tica y humana."
  },
  {
    "objectID": "about.html#experiencia",
    "href": "about.html#experiencia",
    "title": "Sobre M√≠",
    "section": "",
    "text": "Supervisor ‚Äî INE (2023‚Äì2025), Loreto, Zacatecas\n- Supervisi√≥n de actividades diarias para cumplimiento de objetivos.\n- Capacitaci√≥n del equipo y mejora de rendimiento.\n- Evaluaci√≥n de desempe√±o y optimizaci√≥n de procesos.\nMaestro Asistente ‚Äî IPN (2022‚Äì2023), Zacatecas\n- Evaluaci√≥n del progreso estudiantil.\n- Planeaci√≥n conjunta de actividades educativas.\n- Creaci√≥n y adaptaci√≥n de materiales educativos."
  },
  {
    "objectID": "about.html#proyectos-conferencias-y-reconocimientos",
    "href": "about.html#proyectos-conferencias-y-reconocimientos",
    "title": "Sobre M√≠",
    "section": "",
    "text": "Proyecto A ‚Äî Estandarizaci√≥n de proceso a nivel laboratorio (2021‚Äì2023)\n- Herramientas: MATLAB, HACCP Software, Latex\n- Liderazgo de investigaci√≥n para producci√≥n de yogurt con uvas a nivel laboratorio.\n- Reconocimiento al mejor promedio del programa acad√©mico (IPN 2017‚Äì2022).\n- Primer lugar en Simposium Agroalimentario y Ciclo de Conferencias CUALTIA 2023."
  },
  {
    "objectID": "about.html#habilidades",
    "href": "about.html#habilidades",
    "title": "Sobre M√≠",
    "section": "",
    "text": "Lenguajes: Python, JAVA, HTML, ASPEN\n\nHerramientas Matem√°ticas: MATLAB, R, SPSS, PLC\n\nOfim√°tica: Office, LaTeX"
  },
  {
    "objectID": "about.html#informaci√≥n-adicional",
    "href": "about.html#informaci√≥n-adicional",
    "title": "Sobre M√≠",
    "section": "",
    "text": "Idiomas: Espa√±ol (nativo), Ingl√©s (B1)\n\nIntereses: Lectura, b√∫squeda, planeaci√≥n"
  },
  {
    "objectID": "index.html#presentaci√≥n-del-proyecto",
    "href": "index.html#presentaci√≥n-del-proyecto",
    "title": "Centro de Investigaci√≥n e Innovaci√≥n en Tecnolog√≠as de la Informaci√≥n y Comunicaci√≥n",
    "section": "Presentaci√≥n del proyecto",
    "text": "Presentaci√≥n del proyecto\nEn esta p√°gina se encuentran reflejados los cinco reportes desarrollados a lo largo de la asignatura de An√°lisis de Algoritmos. Cada uno de ellos ha sido debidamente documentado, estructurado y trasladado al formato Quarto con el prop√≥sito de comunicar al p√∫blico el trabajo realizado y los an√°lisis efectuados durante mi estancia en la materia. Estos reportes representan el proceso de aprendizaje y aplicaci√≥n pr√°ctica de los conceptos clave abordados en cada unidad, desde la introducci√≥n al an√°lisis algor√≠tmico hasta los algoritmos de intersecci√≥n de conjuntos, permitiendo evidenciar el desarrollo de competencias t√©cnicas y anal√≠ticas fundamentales en el √°rea.\nEsta documentaci√≥n ha sido preparada para su publicaci√≥n en un repositorio de GitHub con el objetivo de compartir de forma abierta los contenidos desarrollados, promover el acceso al conocimiento, y facilitar su consulta por parte de docentes, estudiantes y profesionales interesados en el an√°lisis de algoritmos.\nA lo largo del curso se desarrollaron cinco tareas escritas que reflejan los temas fundamentales abordados en cada unidad:\n\nUnidad 1: Introducci√≥n al an√°lisis de algoritmos\nSe realiz√≥ el reporte 1A. Reporte escrito. Experimentos y an√°lisis, en el que se exploraron conceptos b√°sicos sobre la eficiencia algor√≠tmica y √≥rdenes de crecimiento.\nUnidad 2: Estructuras de datos\nSe trabaj√≥ el reporte 2A. Reporte escrito. Experimentos y an√°lisis de estructuras de datos, enfocado en el comportamiento, manipulaci√≥n y an√°lisis de distintas estructuras como listas, pilas, colas y √°rboles.\nUnidad 3: Algoritmos de ordenamiento por comparaci√≥n\nSe elabor√≥ el 3A. Reporte escrito. Experimentos y an√°lisis de algoritmos de ordenamiento, donde se evaluaron m√©todos como burbuja, inserci√≥n, selecci√≥n, quicksort y mergesort.\nUnidad 4: Algoritmos de b√∫squeda por comparaci√≥n\nSe desarroll√≥ el reporte 4A. Reporte escrito. Experimentos y an√°lisis de algoritmos de b√∫squeda por comparaci√≥n, abordando t√©cnicas como la b√∫squeda lineal y binaria, con un enfoque en su eficiencia.\nUnidad 5: Algoritmos de intersecci√≥n y uni√≥n de conjuntos en el modelo de comparaci√≥n\nSe present√≥ el 5A. Reporte escrito. Experimentos y an√°lisis de algoritmos de intersecci√≥n de conjuntos, donde se analizaron distintas estrategias para operar sobre m√∫ltiples listas ordenadas."
  },
  {
    "objectID": "project2.html",
    "href": "project2.html",
    "title": "Proyecto 2",
    "section": "",
    "text": "Los experimentos y an√°lisis de estructuras de datos son fundamentales en la ciencia de datos, ya que permiten evaluar y optimizar el rendimiento de algoritmos y sistemas que manejan grandes vol√∫menes de informaci√≥n.\nLas estructuras de datos son formas de organizar y almacenar datos en una computadora para que puedan ser accedidos y modificados de manera eficiente. Cada estructura tiene sus propias ventajas y desventajas en t√©rminos de tiempo de acceso, uso de memoria y facilidad de implementaci√≥n. Los experimentos en estructuras de datos suelen centrarse en medir estos aspectos bajo diferentes condiciones para determinar cu√°l es la m√°s adecuada para una aplicaci√≥n espec√≠fica (Goodfellow et al., 2016).\nEl dise√±o experimental es una etapa esencial que precede al an√°lisis de datos. Un dise√±o bien planificado asegura que los datos recopilados sean relevantes y √∫tiles para el an√°lisis posterior. Esto incluye la definici√≥n de variables, la selecci√≥n de muestras y la implementaci√≥n de controles para minimizar sesgos. Estas estructuras son esenciales para el desarrollo de software eficiente, especialmente en campos como el Internet de las Cosas (IoT) y el Big Data, donde la optimizaci√≥n de recursos es crucial (Strang, 2016).\nEl an√°lisis de datos implica varias t√©cnicas estad√≠sticas y computacionales para interpretar los resultados experimentales. Esto puede incluir an√°lisis exploratorio de datos (EDA) para identificar patrones y relaciones, as√≠ como an√°lisis confirmatorio para probar hip√≥tesis espec√≠ficas. Herramientas como Hadoop, Spark, Power BI, Pandas y R son com√∫nmente utilizadas en este proceso (Grcar, 2011).\nLa multiplicaci√≥n de matrices es una operaci√≥n b√°sica que se utiliza en numerosos c√°lculos matem√°ticos y algoritmos computacionales. Su relevancia radica en su capacidad para transformar y combinar datos de manera que se puedan extraer insights significativos. La eliminaci√≥n gaussiana es un m√©todo cl√°sico para resolver sistemas de ecuaciones lineales, transformando una matriz en una forma escalonada reducida por filas. Este m√©todo es ampliamente utilizado debido a su estabilidad num√©rica y su capacidad para manejar sistemas grandes.\nEl objetivo es entender c√≥mo se comportan estos algoritmos en diferentes situaciones, especialmente en cuanto al n√∫mero de operaciones (como multiplicaciones y sumas) y el tiempo que tardan en ejecutarse en la pr√°ctica. Para abordar esta cuesti√≥n, se plantea un estudio comparativo utilizando matrices aleatorias de tama√±o \\(n \\times n\\) para \\(n = 100\\), \\(300\\), \\(1000\\). Este enfoque permitir√° evaluar la eficiencia de cada algoritmo y proporcionar√° una base s√≥lida para futuras optimizaciones. Al comparar estos algoritmos, se busca responder a preguntas clave como: ¬øQu√© puedes concluir?, ¬øCu√°l es el impacto de acceder a los elementos contiguos en memoria de una matriz?, ¬øQu√© cambiar√≠as si utilizas matrices dispersas?, y ¬øCu√°les ser√≠an los costos?\n\n\n\n\n\nimport numpy as np\nimport time\n\n\n\ndef matrix_multiplication(A, B):\n    n = len(A)  # Tama√±o de la matriz (n x n)\n    C = np.zeros((n, n))  # Inicializar la matriz resultante con ceros\n    operations = 0  # Contador de operaciones\n\n    for i in range(n):\n        for j in range(n):\n            for k in range(n):\n                C[i][j] += A[i][k] * B[k][j]  # Multiplicaci√≥n y suma\n                operations += 2  # Una multiplicaci√≥n y una suma\n\n    return C, operations\n\n\n\ndef gauss_jordan(A):\n    n = len(A)\n    operations = 0\n    A = np.array(A, dtype=float)\n\n    for i in range(n):\n        if A[i][i] == 0:\n            for j in range(i+1, n):\n                if A[j][i] != 0:\n                    A[[i, j]] = A[[j, i]]  # Intercambio de filas\n                    operations += 1\n                    break\n\n        div = A[i][i]\n        A[i] = A[i] / div\n        operations += n  # n divisiones\n\n        for j in range(n):\n            if i != j:\n                A[j] = A[j] - A[j][i] * A[i]\n                operations += n  # n multiplicaciones y restas\n\n    return A, operations\n\n\n\nsizes = [100, 300, 1000]\nresults = []\n\nfor n in sizes:\n    A = np.random.rand(n, n)\n    B = np.random.rand(n, n)\n\n    # Multiplicaci√≥n de matrices\n    start_time = time.time()\n    C, mult_operations = matrix_multiplication(A, B)\n    mult_time = time.time() - start_time\n\n    # Eliminaci√≥n Gaussiana\n    start_time = time.time()\n    _, gauss_operations = gauss_jordan(A)\n    gauss_time = time.time() - start_time\n\n    results.append({\n        'n': n,\n        'mult_operations': mult_operations,\n        'mult_time': mult_time,\n        'gauss_operations': gauss_operations,\n        'gauss_time': gauss_time\n    })\n\n\n\nfor result in results:\n    print(f\"Tama√±o de la matriz (n): {result['n']}\")\n    print(\" Multiplicaci√≥n de Matrices:\")\n    print(f\" Operaciones: {result['mult_operations']}\")\n    print(f\" Tiempo (s): {result['mult_time']:.6f}\")\n    print(\" Eliminaci√≥n Gaussiana:\")\n    print(f\" Operaciones: {result['gauss_operations']}\")\n    print(f\" Tiempo (s): {result['gauss_time']:.6f}\")\n    print(\"-\" * 40)\n\n\n\n\n\n\nA continuaci√≥n se muestra una tabla con los resultados obtenidos al comparar la multiplicaci√≥n de matrices y la eliminaci√≥n Gauss-Jordan sobre matrices de distintos tama√±os. Se reporta el n√∫mero total de operaciones y el tiempo de ejecuci√≥n en segundos.\n\n\n\n\n\n\n\n\n\n\nTama√±o (n)\nOperaciones Multiplicaci√≥n\nTiempo Multiplicaci√≥n (s)\nOperaciones Gauss\nTiempo Gauss (s)\n\n\n\n\n100\n2,000,000\n1.829321\n1,000,000\n0.066816\n\n\n300\n54,000,000\n30.990801\n27,000,000\n0.399951\n\n\n1000\n2,000,000,000\n1071.753574\n1,000,000,000\n5.316519\n\n\n\n\nNota: La eliminaci√≥n Gauss-Jordan muestra una notable ventaja en tiempo de ejecuci√≥n respecto a la multiplicaci√≥n de matrices, aunque ambas mantienen una proporci√≥n consistente en la cantidad de operaciones requeridas.\n\nLos resultados obtenidos muestran el desempe√±o de dos operaciones: la multiplicaci√≥n de matrices y la eliminaci√≥n gaussiana/Gauss-Jordan sobre matrices de diferentes tama√±os (100, 300 y 1000), cuantificando el n√∫mero de operaciones y el tiempo de ejecuci√≥n, como se muestra en la Tabla 1.\n\n\n\n\n\n\n\n\n\nEn los Gr√°ficos 1 y 2, se aprecia visualmente c√≥mo se ve afectado el n√∫mero de operaciones y el tiempo de ejecuci√≥n en relaci√≥n con el tama√±o de la matriz.\nEn cuanto a la multiplicaci√≥n de matrices, el n√∫mero de operaciones aumenta significativamente con el tama√±o de la matriz, lo cual es de esperarse, ya que se trata de una operaci√≥n computacionalmente intensiva. El tiempo de ejecuci√≥n tambi√©n aumenta proporcionalmente, reflejando el impacto del crecimiento en complejidad.\nPor otro lado, la eliminaci√≥n gaussiana/Gauss-Jordan muestra una tendencia similar: a mayor tama√±o de la matriz, mayor n√∫mero de operaciones y mayor tiempo de procesamiento. Espec√≠ficamente, para una matriz de tama√±o \\(n = 1000\\), el tiempo fue de 1,071.753 segundos para la multiplicaci√≥n de matrices y 5.316 segundos para la eliminaci√≥n gaussiana, destacando una diferencia considerable a favor del segundo m√©todo en t√©rminos de eficiencia temporal.\nEl rendimiento puede verse influenciado por el hardware utilizado, como la capacidad de memoria, la arquitectura del procesador, y otros componentes del sistema. Estos factores afectan directamente el tiempo de ejecuci√≥n observado. Entre los elementos que m√°s impactan se incluyen la cantidad y velocidad de la memoria RAM, la arquitectura del procesador, el n√∫mero de n√∫cleos e hilos de ejecuci√≥n, la frecuencia de reloj, la memoria cach√© del procesador, el tipo de almacenamiento, y la tecnolog√≠a de la GPU si es utilizada (Raina et al., 2009).\n\n\n\n\n\n\nEl impacto de acceder a elementos contiguos en memoria de una matriz es un concepto fundamental en la optimizaci√≥n del rendimiento de programas. Este principio se basa en c√≥mo los datos se almacenan y acceden en la memoria de una computadora.\nCuando los elementos de una matriz se almacenan de manera contigua en la memoria, el procesador puede acceder a ellos de forma m√°s eficiente. Esto se debe a varios factores: el uso de la cach√© del procesador, el aumento de la localidad espacial, el prefetching, la reducci√≥n del consumo de energ√≠a y las operaciones vectorizadas.\nLa cach√© es una memoria de acceso r√°pido que almacena copias de datos que se utilizan frecuentemente, y cuando los datos est√°n almacenados de manera contigua, es m√°s probable que se carguen en la cach√© en bloques grandes, reduciendo as√≠ el tiempo de acceso.\nLa localidad espacial hace referencia a la tendencia de los programas a acceder a datos que est√°n cerca unos de otros en la memoria. Cuando los datos est√°n almacenados de manera contigua, se mejora la localidad espacial, lo que permite al procesador acceder a los datos de manera m√°s r√°pida.\nLos procesadores utilizan t√©cnicas de prefetching para anticipar qu√© datos se necesitar√°n en el futuro y cargarlos en la cach√© antes de que sean solicitados. Cuando los datos est√°n almacenados de manera contigua, el prefetching es m√°s efectivo, ya que el procesador puede cargar bloques grandes de datos.\nOtro impacto importante es la reducci√≥n del consumo de energ√≠a; un mejor uso de la cach√© y una menor latencia tambi√©n pueden traducirse en un menor consumo energ√©tico.\nEl acceso contiguo a la memoria tambi√©n puede influir en el rendimiento de las operaciones vectorizadas, que son comunes en aplicaciones cient√≠ficas y de ingenier√≠a. Las operaciones vectorizadas permiten al procesador realizar m√∫ltiples operaciones en paralelo, y cuando los datos est√°n almacenados de manera contigua, es m√°s f√°cil para el procesador realizarlas de manera eficiente (Bryant, 2016).\n\n\n\nSi utiliz√°ramos matrices dispersas, estas tendr√≠an un impacto en el rendimiento y la eficiencia de un programa, especialmente al trabajar con grandes conjuntos de datos donde la mayor√≠a de los elementos son cero.\nLos cambios al utilizar matrices dispersas incluyen:\n\nReducci√≥n del uso de memoria: Las matrices dispersas almacenan solo los elementos no nulos y sus posiciones, lo que reduce significativamente el uso de memoria en comparaci√≥n con matrices densas.\nOptimizaci√≥n de operaciones: Las operaciones matem√°ticas pueden ser m√°s eficientes al realizarse √∫nicamente sobre los elementos no nulos, disminuyendo as√≠ el tiempo de c√≥mputo.\nUso de bibliotecas especializadas: Se requiere el uso de bibliotecas dise√±adas espec√≠ficamente para manejar matrices dispersas de forma eficiente.\n\nSin embargo, tambi√©n se presentan algunos costos y desaf√≠os:\n\nMayor complejidad en la implementaci√≥n: Los algoritmos que manejan matrices dispersas pueden ser m√°s complejos que los que utilizan matrices densas, requiriendo mayor tiempo de desarrollo y depuraci√≥n.\nMantenimiento e interoperabilidad: La integraci√≥n con sistemas o bibliotecas que no admiten matrices dispersas puede generar la necesidad de realizar conversiones costosas entre formatos.\nOperaciones costosas: Inserciones y eliminaciones pueden requerir mayor tiempo de procesamiento, debido a la necesidad de actualizar estructuras de √≠ndices.\nSobrecarga de gesti√≥n de datos: Aunque se reduce el uso de memoria, la administraci√≥n adicional puede afectar el rendimiento, especialmente si la matriz no es extremadamente dispersa.\nCostos de conversi√≥n: Si es necesario convertir entre matrices densas y dispersas, esto puede introducir costos adicionales tanto en tiempo de ejecuci√≥n como en memoria (Bryant, 2016).\n\n\n\n\n\n\nGoodfellow, I., Bengio, Y., & Courville, A. (2016). Deep Learning. MIT Press.\nStrang, G. (2016). Introduction to Linear Algebra (5¬™ edici√≥n). Wellesley-Cambridge Press.\nGrcar, J. F. (2011). ‚ÄúMathematicians of Gaussian Elimination.‚Äù Notices of the American Mathematical Society, 58(6), 782‚Äì792.\nBryant, R. E., & O‚ÄôHallaron, D. R. (2016). Computer Systems: A Programmer‚Äôs Perspective (3rd ed.). Pearson.\nTeresa-Motiv, Saisang, brittmsantos, eross-msft, & JasonGerend. (2023, 17 de abril). Consideraciones sobre el uso de memoria para la optimizaci√≥n del rendimiento de AD DS.\nRaina, R., Madhavan, A., & Ng, A. Y. (2009). Large-scale Deep Unsupervised Learning using Graphics Processors. Computer Science Department, Stanford University.\n\n\n\n\nSe hicieron mejoras en el planteamiento de los experimentos y en la discusi√≥n de los resultados, como fue indicado por el maestro. Ahora cada figura tiene una explicaci√≥n clara sobre lo que muestra y lo que significa. Tambi√©n se mejor√≥ la redacci√≥n para que las ideas sean m√°s comprensibles y ordenadas."
  },
  {
    "objectID": "project2.html#introducci√≥n",
    "href": "project2.html#introducci√≥n",
    "title": "Proyecto 2",
    "section": "",
    "text": "Los experimentos y an√°lisis de estructuras de datos son fundamentales en la ciencia de datos, ya que permiten evaluar y optimizar el rendimiento de algoritmos y sistemas que manejan grandes vol√∫menes de informaci√≥n.\nLas estructuras de datos son formas de organizar y almacenar datos en una computadora para que puedan ser accedidos y modificados de manera eficiente. Cada estructura tiene sus propias ventajas y desventajas en t√©rminos de tiempo de acceso, uso de memoria y facilidad de implementaci√≥n. Los experimentos en estructuras de datos suelen centrarse en medir estos aspectos bajo diferentes condiciones para determinar cu√°l es la m√°s adecuada para una aplicaci√≥n espec√≠fica (Goodfellow et al., 2016).\nEl dise√±o experimental es una etapa esencial que precede al an√°lisis de datos. Un dise√±o bien planificado asegura que los datos recopilados sean relevantes y √∫tiles para el an√°lisis posterior. Esto incluye la definici√≥n de variables, la selecci√≥n de muestras y la implementaci√≥n de controles para minimizar sesgos. Estas estructuras son esenciales para el desarrollo de software eficiente, especialmente en campos como el Internet de las Cosas (IoT) y el Big Data, donde la optimizaci√≥n de recursos es crucial (Strang, 2016).\nEl an√°lisis de datos implica varias t√©cnicas estad√≠sticas y computacionales para interpretar los resultados experimentales. Esto puede incluir an√°lisis exploratorio de datos (EDA) para identificar patrones y relaciones, as√≠ como an√°lisis confirmatorio para probar hip√≥tesis espec√≠ficas. Herramientas como Hadoop, Spark, Power BI, Pandas y R son com√∫nmente utilizadas en este proceso (Grcar, 2011).\nLa multiplicaci√≥n de matrices es una operaci√≥n b√°sica que se utiliza en numerosos c√°lculos matem√°ticos y algoritmos computacionales. Su relevancia radica en su capacidad para transformar y combinar datos de manera que se puedan extraer insights significativos. La eliminaci√≥n gaussiana es un m√©todo cl√°sico para resolver sistemas de ecuaciones lineales, transformando una matriz en una forma escalonada reducida por filas. Este m√©todo es ampliamente utilizado debido a su estabilidad num√©rica y su capacidad para manejar sistemas grandes.\nEl objetivo es entender c√≥mo se comportan estos algoritmos en diferentes situaciones, especialmente en cuanto al n√∫mero de operaciones (como multiplicaciones y sumas) y el tiempo que tardan en ejecutarse en la pr√°ctica. Para abordar esta cuesti√≥n, se plantea un estudio comparativo utilizando matrices aleatorias de tama√±o \\(n \\times n\\) para \\(n = 100\\), \\(300\\), \\(1000\\). Este enfoque permitir√° evaluar la eficiencia de cada algoritmo y proporcionar√° una base s√≥lida para futuras optimizaciones. Al comparar estos algoritmos, se busca responder a preguntas clave como: ¬øQu√© puedes concluir?, ¬øCu√°l es el impacto de acceder a los elementos contiguos en memoria de una matriz?, ¬øQu√© cambiar√≠as si utilizas matrices dispersas?, y ¬øCu√°les ser√≠an los costos?"
  },
  {
    "objectID": "project2.html#desarrollo",
    "href": "project2.html#desarrollo",
    "title": "Proyecto 2",
    "section": "",
    "text": "import numpy as np\nimport time\n\n\n\ndef matrix_multiplication(A, B):\n    n = len(A)  # Tama√±o de la matriz (n x n)\n    C = np.zeros((n, n))  # Inicializar la matriz resultante con ceros\n    operations = 0  # Contador de operaciones\n\n    for i in range(n):\n        for j in range(n):\n            for k in range(n):\n                C[i][j] += A[i][k] * B[k][j]  # Multiplicaci√≥n y suma\n                operations += 2  # Una multiplicaci√≥n y una suma\n\n    return C, operations\n\n\n\ndef gauss_jordan(A):\n    n = len(A)\n    operations = 0\n    A = np.array(A, dtype=float)\n\n    for i in range(n):\n        if A[i][i] == 0:\n            for j in range(i+1, n):\n                if A[j][i] != 0:\n                    A[[i, j]] = A[[j, i]]  # Intercambio de filas\n                    operations += 1\n                    break\n\n        div = A[i][i]\n        A[i] = A[i] / div\n        operations += n  # n divisiones\n\n        for j in range(n):\n            if i != j:\n                A[j] = A[j] - A[j][i] * A[i]\n                operations += n  # n multiplicaciones y restas\n\n    return A, operations\n\n\n\nsizes = [100, 300, 1000]\nresults = []\n\nfor n in sizes:\n    A = np.random.rand(n, n)\n    B = np.random.rand(n, n)\n\n    # Multiplicaci√≥n de matrices\n    start_time = time.time()\n    C, mult_operations = matrix_multiplication(A, B)\n    mult_time = time.time() - start_time\n\n    # Eliminaci√≥n Gaussiana\n    start_time = time.time()\n    _, gauss_operations = gauss_jordan(A)\n    gauss_time = time.time() - start_time\n\n    results.append({\n        'n': n,\n        'mult_operations': mult_operations,\n        'mult_time': mult_time,\n        'gauss_operations': gauss_operations,\n        'gauss_time': gauss_time\n    })\n\n\n\nfor result in results:\n    print(f\"Tama√±o de la matriz (n): {result['n']}\")\n    print(\" Multiplicaci√≥n de Matrices:\")\n    print(f\" Operaciones: {result['mult_operations']}\")\n    print(f\" Tiempo (s): {result['mult_time']:.6f}\")\n    print(\" Eliminaci√≥n Gaussiana:\")\n    print(f\" Operaciones: {result['gauss_operations']}\")\n    print(f\" Tiempo (s): {result['gauss_time']:.6f}\")\n    print(\"-\" * 40)"
  },
  {
    "objectID": "project2.html#an√°lisis-de-resultados",
    "href": "project2.html#an√°lisis-de-resultados",
    "title": "Proyecto 2",
    "section": "",
    "text": "A continuaci√≥n se muestra una tabla con los resultados obtenidos al comparar la multiplicaci√≥n de matrices y la eliminaci√≥n Gauss-Jordan sobre matrices de distintos tama√±os. Se reporta el n√∫mero total de operaciones y el tiempo de ejecuci√≥n en segundos.\n\n\n\n\n\n\n\n\n\n\nTama√±o (n)\nOperaciones Multiplicaci√≥n\nTiempo Multiplicaci√≥n (s)\nOperaciones Gauss\nTiempo Gauss (s)\n\n\n\n\n100\n2,000,000\n1.829321\n1,000,000\n0.066816\n\n\n300\n54,000,000\n30.990801\n27,000,000\n0.399951\n\n\n1000\n2,000,000,000\n1071.753574\n1,000,000,000\n5.316519\n\n\n\n\nNota: La eliminaci√≥n Gauss-Jordan muestra una notable ventaja en tiempo de ejecuci√≥n respecto a la multiplicaci√≥n de matrices, aunque ambas mantienen una proporci√≥n consistente en la cantidad de operaciones requeridas.\n\nLos resultados obtenidos muestran el desempe√±o de dos operaciones: la multiplicaci√≥n de matrices y la eliminaci√≥n gaussiana/Gauss-Jordan sobre matrices de diferentes tama√±os (100, 300 y 1000), cuantificando el n√∫mero de operaciones y el tiempo de ejecuci√≥n, como se muestra en la Tabla 1.\n\n\n\n\n\n\n\n\n\nEn los Gr√°ficos 1 y 2, se aprecia visualmente c√≥mo se ve afectado el n√∫mero de operaciones y el tiempo de ejecuci√≥n en relaci√≥n con el tama√±o de la matriz.\nEn cuanto a la multiplicaci√≥n de matrices, el n√∫mero de operaciones aumenta significativamente con el tama√±o de la matriz, lo cual es de esperarse, ya que se trata de una operaci√≥n computacionalmente intensiva. El tiempo de ejecuci√≥n tambi√©n aumenta proporcionalmente, reflejando el impacto del crecimiento en complejidad.\nPor otro lado, la eliminaci√≥n gaussiana/Gauss-Jordan muestra una tendencia similar: a mayor tama√±o de la matriz, mayor n√∫mero de operaciones y mayor tiempo de procesamiento. Espec√≠ficamente, para una matriz de tama√±o \\(n = 1000\\), el tiempo fue de 1,071.753 segundos para la multiplicaci√≥n de matrices y 5.316 segundos para la eliminaci√≥n gaussiana, destacando una diferencia considerable a favor del segundo m√©todo en t√©rminos de eficiencia temporal.\nEl rendimiento puede verse influenciado por el hardware utilizado, como la capacidad de memoria, la arquitectura del procesador, y otros componentes del sistema. Estos factores afectan directamente el tiempo de ejecuci√≥n observado. Entre los elementos que m√°s impactan se incluyen la cantidad y velocidad de la memoria RAM, la arquitectura del procesador, el n√∫mero de n√∫cleos e hilos de ejecuci√≥n, la frecuencia de reloj, la memoria cach√© del procesador, el tipo de almacenamiento, y la tecnolog√≠a de la GPU si es utilizada (Raina et al., 2009)."
  },
  {
    "objectID": "project2.html#conclusiones",
    "href": "project2.html#conclusiones",
    "title": "Proyecto 2",
    "section": "",
    "text": "El impacto de acceder a elementos contiguos en memoria de una matriz es un concepto fundamental en la optimizaci√≥n del rendimiento de programas. Este principio se basa en c√≥mo los datos se almacenan y acceden en la memoria de una computadora.\nCuando los elementos de una matriz se almacenan de manera contigua en la memoria, el procesador puede acceder a ellos de forma m√°s eficiente. Esto se debe a varios factores: el uso de la cach√© del procesador, el aumento de la localidad espacial, el prefetching, la reducci√≥n del consumo de energ√≠a y las operaciones vectorizadas.\nLa cach√© es una memoria de acceso r√°pido que almacena copias de datos que se utilizan frecuentemente, y cuando los datos est√°n almacenados de manera contigua, es m√°s probable que se carguen en la cach√© en bloques grandes, reduciendo as√≠ el tiempo de acceso.\nLa localidad espacial hace referencia a la tendencia de los programas a acceder a datos que est√°n cerca unos de otros en la memoria. Cuando los datos est√°n almacenados de manera contigua, se mejora la localidad espacial, lo que permite al procesador acceder a los datos de manera m√°s r√°pida.\nLos procesadores utilizan t√©cnicas de prefetching para anticipar qu√© datos se necesitar√°n en el futuro y cargarlos en la cach√© antes de que sean solicitados. Cuando los datos est√°n almacenados de manera contigua, el prefetching es m√°s efectivo, ya que el procesador puede cargar bloques grandes de datos.\nOtro impacto importante es la reducci√≥n del consumo de energ√≠a; un mejor uso de la cach√© y una menor latencia tambi√©n pueden traducirse en un menor consumo energ√©tico.\nEl acceso contiguo a la memoria tambi√©n puede influir en el rendimiento de las operaciones vectorizadas, que son comunes en aplicaciones cient√≠ficas y de ingenier√≠a. Las operaciones vectorizadas permiten al procesador realizar m√∫ltiples operaciones en paralelo, y cuando los datos est√°n almacenados de manera contigua, es m√°s f√°cil para el procesador realizarlas de manera eficiente (Bryant, 2016).\n\n\n\nSi utiliz√°ramos matrices dispersas, estas tendr√≠an un impacto en el rendimiento y la eficiencia de un programa, especialmente al trabajar con grandes conjuntos de datos donde la mayor√≠a de los elementos son cero.\nLos cambios al utilizar matrices dispersas incluyen:\n\nReducci√≥n del uso de memoria: Las matrices dispersas almacenan solo los elementos no nulos y sus posiciones, lo que reduce significativamente el uso de memoria en comparaci√≥n con matrices densas.\nOptimizaci√≥n de operaciones: Las operaciones matem√°ticas pueden ser m√°s eficientes al realizarse √∫nicamente sobre los elementos no nulos, disminuyendo as√≠ el tiempo de c√≥mputo.\nUso de bibliotecas especializadas: Se requiere el uso de bibliotecas dise√±adas espec√≠ficamente para manejar matrices dispersas de forma eficiente.\n\nSin embargo, tambi√©n se presentan algunos costos y desaf√≠os:\n\nMayor complejidad en la implementaci√≥n: Los algoritmos que manejan matrices dispersas pueden ser m√°s complejos que los que utilizan matrices densas, requiriendo mayor tiempo de desarrollo y depuraci√≥n.\nMantenimiento e interoperabilidad: La integraci√≥n con sistemas o bibliotecas que no admiten matrices dispersas puede generar la necesidad de realizar conversiones costosas entre formatos.\nOperaciones costosas: Inserciones y eliminaciones pueden requerir mayor tiempo de procesamiento, debido a la necesidad de actualizar estructuras de √≠ndices.\nSobrecarga de gesti√≥n de datos: Aunque se reduce el uso de memoria, la administraci√≥n adicional puede afectar el rendimiento, especialmente si la matriz no es extremadamente dispersa.\nCostos de conversi√≥n: Si es necesario convertir entre matrices densas y dispersas, esto puede introducir costos adicionales tanto en tiempo de ejecuci√≥n como en memoria (Bryant, 2016)."
  },
  {
    "objectID": "project2.html#referencias",
    "href": "project2.html#referencias",
    "title": "Proyecto 2",
    "section": "",
    "text": "Goodfellow, I., Bengio, Y., & Courville, A. (2016). Deep Learning. MIT Press.\nStrang, G. (2016). Introduction to Linear Algebra (5¬™ edici√≥n). Wellesley-Cambridge Press.\nGrcar, J. F. (2011). ‚ÄúMathematicians of Gaussian Elimination.‚Äù Notices of the American Mathematical Society, 58(6), 782‚Äì792.\nBryant, R. E., & O‚ÄôHallaron, D. R. (2016). Computer Systems: A Programmer‚Äôs Perspective (3rd ed.). Pearson.\nTeresa-Motiv, Saisang, brittmsantos, eross-msft, & JasonGerend. (2023, 17 de abril). Consideraciones sobre el uso de memoria para la optimizaci√≥n del rendimiento de AD DS.\nRaina, R., Madhavan, A., & Ng, A. Y. (2009). Large-scale Deep Unsupervised Learning using Graphics Processors. Computer Science Department, Stanford University."
  },
  {
    "objectID": "project2.html#cambios-realizados",
    "href": "project2.html#cambios-realizados",
    "title": "Proyecto 2",
    "section": "",
    "text": "Se hicieron mejoras en el planteamiento de los experimentos y en la discusi√≥n de los resultados, como fue indicado por el maestro. Ahora cada figura tiene una explicaci√≥n clara sobre lo que muestra y lo que significa. Tambi√©n se mejor√≥ la redacci√≥n para que las ideas sean m√°s comprensibles y ordenadas."
  },
  {
    "objectID": "project4.html#introducci√≥n",
    "href": "project4.html#introducci√≥n",
    "title": "Proyecto 4",
    "section": "",
    "text": "En la ciencia de la computaci√≥n, los algoritmos de b√∫squeda son esenciales para optimizar la eficiencia en la gesti√≥n y recuperaci√≥n de datos. Estos algoritmos permiten localizar elementos espec√≠ficos dentro de una colecci√≥n de datos, optimizando el tiempo y los recursos necesarios para dicha tarea. Este informe se centra en la implementaci√≥n y comparaci√≥n de varios algoritmos de b√∫squeda por comparaci√≥n, con el objetivo de evaluar su rendimiento en cuanto al n√∫mero de comparaciones y el tiempo de ejecuci√≥n (Cormen et al., 2009).\nLos algoritmos de b√∫squeda por comparaci√≥n se basan en la comparaci√≥n de elementos para encontrar el objetivo deseado. Entre los algoritmos m√°s conocidos se encuentran la b√∫squeda binaria, la b√∫squeda secuencial y variantes de b√∫squedas no acotadas. Su comportamiento depende de la estructura de datos utilizada y de las condiciones de b√∫squeda (Knuth, 1998).\nLa b√∫squeda binaria acotada es una t√©cnica eficiente para encontrar un elemento en un conjunto de datos ordenado. Este algoritmo divide repetidamente el conjunto de datos a la mitad, comparando el elemento objetivo con el elemento central del subconjunto actual. Si el elemento central es igual al objetivo, la b√∫squeda termina. Si es mayor o menor, la b√∫squeda contin√∫a en la mitad inferior o superior, respectivamente. Esta t√©cnica es altamente eficiente en conjuntos de datos ordenados, reduciendo significativamente el n√∫mero de comparaciones necesarias (Cormen et al., 2009).\nLa b√∫squeda secuencial, tambi√©n conocida como b√∫squeda lineal, es un m√©todo simple y directo para encontrar un elemento en un conjunto de datos. El algoritmo recorre cada elemento del conjunto en orden secuencial, comparando cada uno con el elemento objetivo hasta encontrarlo o llegar al final. Aunque es menos eficiente que la b√∫squeda binaria en conjuntos ordenados, su aplicabilidad se extiende a cualquier tipo de conjunto, ordenado o no. Las variantes de b√∫squeda no acotada, como B1 y B2, extienden este concepto para manejar conjuntos de datos cuyo tama√±o es desconocido o cambia constantemente (Sedgewick, 2011).\nLas SkipLists son estructuras de datos probabil√≠sticas que permiten b√∫squedas eficientes en conjuntos de datos ordenados. A diferencia de las listas enlazadas tradicionales, las SkipLists utilizan m√∫ltiples niveles de enlaces para acelerar el proceso de b√∫squeda. Esta estructura ofrece eficiencia en b√∫squedas sobre conjuntos ordenados y una mayor flexibilidad. Sin embargo, su desventaja radica en el uso adicional de memoria debido a los m√∫ltiples niveles de enlaces (Knuth, 1998).\nEn este trabajo se implementaron y compararon cuatro algoritmos de b√∫squeda: b√∫squeda binaria acotada, b√∫squeda secuencial y dos variantes de b√∫squeda no acotada (B1 y B2), adem√°s de la estructura de datos SkipList, con el fin de evaluar su eficiencia en t√©rminos de n√∫mero de comparaciones y tiempo de ejecuci√≥n. Utilizando conjuntos de datos y consultas espec√≠ficas, se midi√≥ el rendimiento de cada m√©todo, registrando los resultados para cada combinaci√≥n de archivos. Los resultados se visualizaron mediante gr√°ficos y tablas, destacando las ventajas y desventajas de cada enfoque.\nSe concluye que la elecci√≥n del m√©todo de b√∫squeda depende del contexto de aplicaci√≥n. Este an√°lisis proporciona una gu√≠a √∫til para seleccionar el m√©todo m√°s adecuado seg√∫n los requerimientos espec√≠ficos de rendimiento y aplicabilidad pr√°ctica."
  },
  {
    "objectID": "project4.html#desarrollo",
    "href": "project4.html#desarrollo",
    "title": "Proyecto 4",
    "section": "2. Desarrollo",
    "text": "2. Desarrollo"
  },
  {
    "objectID": "project4.html#an√°lisis-de-resultados",
    "href": "project4.html#an√°lisis-de-resultados",
    "title": "Proyecto 4",
    "section": "3. An√°lisis de Resultados",
    "text": "3. An√°lisis de Resultados"
  },
  {
    "objectID": "project4.html#conclusiones",
    "href": "project4.html#conclusiones",
    "title": "Proyecto 4",
    "section": "4. Conclusiones",
    "text": "4. Conclusiones"
  },
  {
    "objectID": "project4.html#referencias",
    "href": "project4.html#referencias",
    "title": "Proyecto 4",
    "section": "",
    "text": "Cormen, T. H., Leiserson, C. E., Rivest, R. L., & Stein, C. (2009). Introduction to Algorithms (3rd ed.).\nSedgewick, R., & Wayne, K. (2011). Algorithms (4th ed.).\nKnuth, D. E. (1998). The Art of Computer Programming, Volume 3: Sorting and Searching (2nd ed.).\nWeiss, M. A. (2014). Data Structures and Algorithm Analysis in C (3rd ed.). Pearson."
  },
  {
    "objectID": "project4.html#cambios-realizados",
    "href": "project4.html#cambios-realizados",
    "title": "Proyecto 4",
    "section": "",
    "text": "En atenci√≥n a la retroalimentaci√≥n recibida, se realiz√≥ una revisi√≥n profunda del enfoque adoptado en la implementaci√≥n de los algoritmos de b√∫squeda por comparaci√≥n. A continuaci√≥n se detallan los ajustes conceptuales y t√©cnicos aplicados:\n\nSe reconoci√≥ que el uso de listas de posteo con perturbaciones gener√≥ una interpretaci√≥n err√≥nea del problema, llevando a implementar algoritmos basados √∫nicamente en igualdad, cuando el problema central requer√≠a considerar comparaciones con operadores &lt; o &lt;= para determinar la posici√≥n de inserci√≥n.\nSe reformularon los algoritmos de b√∫squeda secuencial y binaria considerando adecuadamente la sem√°ntica del problema, que implica la ubicaci√≥n correcta en listas ordenadas, no la coincidencia exacta.\nSe revisaron las notas del curso y el art√≠culo de Baeza-Yates (B&Y), lo que permiti√≥ entender con mayor claridad el modelo de comparaci√≥n y sus implicaciones para el dise√±o e interpretaci√≥n correcta de los algoritmos.\nSe corrigieron errores l√≥gicos en las funciones de b√∫squeda y se realizaron pruebas con datos estructurados correctamente, evitando distorsiones generadas por entradas mal definidas."
  },
  {
    "objectID": "project3.html",
    "href": "project3.html",
    "title": "Proyecto 3",
    "section": "",
    "text": "La investigaci√≥n sobre el an√°lisis de algoritmos de ordenamiento es un campo activo, con numerosos estudios que exploran nuevas t√©cnicas y optimizaciones. Ordenar datos es crucial en la ciencia de datos y la computaci√≥n, con aplicaciones que abarcan desde la gesti√≥n de bases de datos hasta la optimizaci√≥n de algoritmos en inteligencia artificial. La eficiencia de los algoritmos de ordenamiento es importante, ya que puede influir significativamente en el rendimiento general de un sistema.\nLa eficiencia de un algoritmo de ordenamiento se mide principalmente en t√©rminos de tiempo y espacio. El tiempo se refiere a cu√°ntas operaciones realiza el algoritmo, mientras que el espacio indica cu√°nta memoria adicional necesita. Un algoritmo de ordenamiento es estable si mantiene el orden relativo de los elementos iguales. La estabilidad es crucial en aplicaciones donde el orden original de los elementos iguales debe preservarse. Algunos algoritmos pueden aprovechar el orden existente en los datos para mejorar su rendimiento, mostrando as√≠ una adaptabilidad eficiente. La complejidad del algoritmo en el peor caso es una m√©trica importante para evaluar su robustez (Knuth, 2011).\nLa elecci√≥n del algoritmo de ordenamiento adecuado depende de varios factores, incluyendo el tama√±o de los datos, el grado de desorden y las restricciones de tiempo y espacio. El algoritmo Heapsort utiliza una estructura de datos llamada heap para ordenar los elementos. Es eficiente en t√©rminos de uso de memoria y tiene una complejidad de \\(O(n \\log n)\\). Por otro lado, el algoritmo Mergesort es un enfoque de divide y vencer√°s que divide la lista en mitades, las ordena recursivamente y luego las fusiona. Es estable y tiene una complejidad de \\(O(n \\log n)\\).\nQuicksort tambi√©n sigue el enfoque de divide y vencer√°s, seleccionando un elemento como pivote y particionando la lista en elementos menores y mayores. Aunque en promedio es r√°pido, puede degradarse a \\(O(n^2)\\) en el peor caso. Bubblesort es un algoritmo simple que compara elementos adyacentes y los intercambia si est√°n en el orden incorrecto. Es ineficiente para grandes conjuntos de datos debido a su complejidad cuadr√°tica. Finalmente, la estructura de datos SkipList es una estructura probabil√≠stica que permite b√∫squedas y ordenamientos eficientes. Aunque no es un algoritmo de ordenamiento tradicional, puede ser utilizada para mantener una lista ordenada de elementos (Cormen, 2009).\nPara evaluar y comparar el rendimiento de estos cinco algoritmos de ordenamiento (Heapsort, Mergesort, Quicksort, Bubblesort y SkipList), se seleccionaron primero los algoritmos bas√°ndose en sus caracter√≠sticas y relevancia pr√°ctica. Luego, se prepararon m√∫ltiples archivos JSON con diferentes niveles de desorden para evaluar cada algoritmo bajo diversas condiciones iniciales. Cada algoritmo se implement√≥ en Python, incluyendo un contador de comparaciones y funciones de temporizaci√≥n para medir la eficiencia y el tiempo de ejecuci√≥n. Los resultados se registraron y organizaron en tablas y gr√°ficos para facilitar la comparaci√≥n visual. El an√°lisis permiti√≥ discutir las ventajas y desventajas de cada algoritmo, considerando factores como el tama√±o de los datos y el grado de desorden.\n\n\n\n\n\n# Bibliotecas utilizadas para manejo de archivos, estructuras, algoritmos y visualizaci√≥n\nimport json\nimport os\nimport heapq\nimport random\nimport time\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Esta funci√≥n recorre todos los archivos .json del directorio y carga sus listas de posteo\n# Decid√≠ separarlo en una funci√≥n para poder reutilizarlo f√°cilmente en otros experimentos\n\ndef load_json_files(directory):\n    all_files_data = {}\n    for filename in os.listdir(directory):\n        if filename.endswith('.json'):\n            file_path = os.path.join(directory, filename)\n            with open(file_path, 'r') as file:\n                data = json.load(file)\n                listas_posteo = list(data.values())\n                all_files_data[filename] = listas_posteo\n    return all_files_data\n\n# Ruta fija al directorio local donde se ubican las listas de posteo perturbadas\n# Este path es local, pero se puede cambiar f√°cilmente para otro entorno\n\ndirectory_path = 'C:\\\\Users\\\\Antonio Mart√≠nez\\\\Desktop\\\\listas-posteo-con-perturbaciones' \nfiles_data = load_json_files(directory_path)\n\n\n\n\n\n# Uso de heapq para convertir la lista en un heap\n# Cada extracci√≥n del m√≠nimo simula la ordenaci√≥n ascendente\n\ndef heapsort(arr):\n    comparisons = 0\n    heapq.heapify(arr)\n    sorted_arr = []\n    while arr:\n        comparisons += 1\n        sorted_arr.append(heapq.heappop(arr))\n    return sorted_arr, comparisons\n\n\n\n# Mergesort implementado de manera recursiva\n# Es muy √∫til para listas grandes, ya que divide y conquista\n\ndef mergesort(arr):\n    comparisons = 0\n\n    def merge_sort_recursive(arr):\n        nonlocal comparisons\n        if len(arr) &gt; 1:\n            mid = len(arr) // 2\n            L = arr[:mid]\n            R = arr[mid:]\n\n            merge_sort_recursive(L)\n            merge_sort_recursive(R)\n\n            i = j = k = 0\n            while i &lt; len(L) and j &lt; len(R):\n                comparisons += 1\n                if L[i] &lt; R[j]:\n                    arr[k] = L[i]\n                    i += 1\n                else:\n                    arr[k] = R[j]\n                    j += 1\n                k += 1\n\n            while i &lt; len(L):\n                arr[k] = L[i]\n                i += 1\n                k += 1\n\n            while j &lt; len(R):\n                arr[k] = R[j]\n                j += 1\n                k += 1\n\n    merge_sort_recursive(arr)\n    return arr, comparisons\n\n\n\n# Uso de Quicksort con selecci√≥n del pivote en la posici√≥n media\n# Aument√© el conteo de comparaciones para evaluar su eficiencia\n\ndef quicksort(arr):\n    comparisons = 0\n\n    def _quicksort(arr, low, high):\n        nonlocal comparisons\n        if low &lt; high:\n            pi = partition(arr, low, high)\n            _quicksort(arr, low, pi - 1)\n            _quicksort(arr, pi + 1, high)\n\n    def partition(arr, low, high):\n        nonlocal comparisons\n        mid = (low + high) // 2\n        pivot = arr[mid]\n        arr[mid], arr[high] = arr[high], arr[mid]\n        i = low - 1\n        for j in range(low, high):\n            comparisons += 1\n            if arr[j] &lt;= pivot:\n                i += 1\n                arr[i], arr[j] = arr[j], arr[i]\n        arr[i+1], arr[high] = arr[high], arr[i+1]\n        return i+1\n\n    _quicksort(arr, 0, len(arr)-1)\n    return arr, comparisons\n\n\n\n# Algoritmo simple pero ineficiente para listas grandes\n# Le agregu√© una condici√≥n para detenerse si ya est√° ordenada\n\ndef bubblesort(arr):\n    n = len(arr)\n    comparisons = 0\n    for i in range(n):\n        swapped = False\n        for j in range(0, n-i-1):\n            comparisons += 1\n            if arr[j] &gt; arr[j+1]:\n                arr[j], arr[j+1] = arr[j+1], arr[j]\n                swapped = True\n        if not swapped:\n            break\n    return arr, comparisons\n\n\n\n# Implementaci√≥n b√°sica de SkipList adaptada para ordenar\n# Usa niveles probabil√≠sticos para simular ordenamiento eficiente\n\nclass SkipListNode:\n    def __init__(self, value, level):\n        self.value = value\n        self.forward = [None] * (level + 1)\n\nclass SkipList:\n    def __init__(self, max_level, p=0.5):\n        self.max_level = max_level\n        self.p = p\n        self.header = SkipListNode(None, max_level)\n        self.level = 0\n        self.comparisons = 0\n\n    def random_level(self):\n        lvl = 0\n        while random.random() &lt; self.p and lvl &lt; self.max_level:\n            lvl += 1\n        return lvl\n\n    def insert(self, value):\n        update = [None] * (self.max_level + 1)\n        current = self.header\n\n        for i in reversed(range(self.level + 1)):\n            while current.forward[i] and current.forward[i].value &lt; value:\n                self.comparisons += 1\n                current = current.forward[i]\n            update[i] = current\n\n        lvl = self.random_level()\n        if lvl &gt; self.level:\n            for i in range(self.level + 1, lvl + 1):\n                update[i] = self.header\n            self.level = lvl\n\n        new_node = SkipListNode(value, lvl)\n        for i in range(lvl + 1):\n            new_node.forward[i] = update[i].forward[i]\n            update[i].forward[i] = new_node\n\n    def traverse(self):\n        result = []\n        current = self.header.forward[0]\n        while current:\n            result.append(current.value)\n            current = current.forward[0]\n        return result\n\ndef skiplist_sort(arr):\n    max_level = 16\n    sl = SkipList(max_level)\n    for value in arr:\n        sl.insert(value)\n    sorted_arr = sl.traverse()\n    return sorted_arr, sl.comparisons\n\n\n\n\n# Esta funci√≥n me permiti√≥ unificar c√≥mo mido el tiempo y el n√∫mero de comparaciones\n# para cada algoritmo. La utilic√© en todos los experimentos posteriores.\n\ndef measure_sorting_algorithm(algorithm, arr):\n    start_time = time.time()\n    sorted_arr, comparisons = algorithm(arr.copy())\n    end_time = time.time()\n    return sorted_arr, comparisons, end_time - start_time\n\n\n\n# Ejecuto todos los algoritmos en cada lista del dataset\n# Almaceno comparaciones y tiempo para analizarlos m√°s adelante\n\nall_files_results = {}\n\nfor filename, listas_posteo in files_data.items():\n    all_results = []\n    for lista in listas_posteo:\n        results = {}\n        for name, algo in [\n            (\"Heapsort\", heapsort),\n            (\"Mergesort\", mergesort),\n            (\"Quicksort\", quicksort),\n            (\"Bubblesort\", bubblesort),\n            (\"Skiplist\", skiplist_sort)\n        ]:\n            sorted_arr, comparisons, time_taken = measure_sorting_algorithm(algo, lista)\n            results[name] = {'Comparaciones': comparisons, 'Tiempo': time_taken}\n        all_results.append(results)\n    all_files_results[filename] = all_results\n\n\n\n# Para cada archivo muestro un resumen con barras comparativas de tiempo y comparaciones\n# Las comparaciones van en escala logar√≠tmica para mayor visibilidad\n\nfor filename, results in all_files_results.items():\n    print(f\"\\nResultados para el archivo: {filename}\")\n    results_df = pd.DataFrame(results)\n\n    comparisons_df = results_df.applymap(lambda x: x['Comparaciones']).mean()\n    time_df = results_df.applymap(lambda x: x['Tiempo']).mean()\n\n    print(\"\\nPromedio de Comparaciones por Algoritmo:\\n\", comparisons_df)\n    print(\"\\nPromedio de Tiempo por Algoritmo:\\n\", time_df)\n\n    # Comparaciones (escala logar√≠tmica)\n    ax = comparisons_df.plot(kind='bar', title=f'Promedio de Comparaciones por Algoritmo - {filename}',\n                              color='skyblue', log=True)\n    plt.ylabel('N√∫mero de Comparaciones (escala logar√≠tmica)')\n    plt.xticks(rotation=45)\n    plt.tight_layout()\n\n    for p in ax.patches:\n        ax.annotate(f'{p.get_height():.2f}', \n                    (p.get_x() + p.get_width() / 2., p.get_height()), \n                    ha='center', va='center', \n                    xytext=(0, 10), textcoords='offset points')\n    plt.show()\n\n    # Tiempos de ejecuci√≥n\n    ax = time_df.plot(kind='bar', title=f'Promedio de Tiempo por Algoritmo - {filename}', color='lightgreen')\n    plt.ylabel('Tiempo (segundos)')\n    plt.xticks(rotation=45)\n    plt.tight_layout()\n\n    for p in ax.patches:\n        ax.annotate(f'{p.get_height():.4f}', \n                    (p.get_x() + p.get_width() / 2., p.get_height()), \n                    ha='center', va='center', \n                    xytext=(0, 10), textcoords='offset points')\n\n    ax.set_ylim(0, time_df.max() * 1.25)\n    plt.show()\n\n\n\nTras revisar las observaciones recibidas, se realizaron mejoras clave en la implementaci√≥n y an√°lisis de los algoritmos. En primer lugar, se corrigi√≥ el algoritmo de Bubble Sort para que fuera adaptativo, lo cual se refleja claramente en las gr√°ficas y en el elevado n√∫mero de comparaciones cuando se enfrenta a casos desfavorables.\nAsimismo, se optimiz√≥ Merge Sort para evitar el uso innecesario de memoria adicional, mejorando as√≠ su eficiencia pr√°ctica. Tambi√©n se corrigi√≥ la selecci√≥n del pivote en Quick Sort, utilizando ahora el elemento central para evitar los peores casos en listas parcialmente ordenadas.\nEn cuanto a SkipList, se elimin√≥ el uso del valor artificial -‚àû, garantizando que todas las comparaciones se realicen solo entre elementos reales, como exige el modelo de comparaci√≥n.\nFinalmente, se ajustaron las escalas de las gr√°ficas, empleando una escala logar√≠tmica para las comparaciones y m√°rgenes din√°micos para el tiempo, facilitando una interpretaci√≥n visual m√°s clara y justa.\nCon estas mejoras, el trabajo queda alineado con los lineamientos discutidos y refleja correctamente el comportamiento de cada algoritmo.\n\n\n\n\nA continuaci√≥n, se presentan los resultados promedio de comparaciones y tiempos de ejecuci√≥n para cada archivo de lista de posteo con perturbaciones. Los datos se agrupan por archivo, lo cual permite observar el comportamiento de los algoritmos en funci√≥n de la variabilidad en las listas. Se visualizan a continuaci√≥n Tablas y Gr √°ficos.\n\n\n\n\n\nAlgoritmo\nComparaciones\nTiempo (s)\n\n\n\n\nHeapsort\n1918.52\n0.000966\n\n\nMergesort\n15031.85\n0.004780\n\n\nQuicksort\n21248.43\n0.002659\n\n\nBubblesort\n12394236.90\n1.484842\n\n\nSkiplist\n20545.83\n0.013274\n\n\n\n\n\n\n\n\n\nEn la tabla de promedios y los gr√°ficos generados para el archivo listas-posteo-con-perturbaciones-p=016.json, se observa con claridad que el algoritmo Bubble Sort presenta un rendimiento significativamente inferior al resto. Este algoritmo registr√≥ m√°s de 12 millones de comparaciones y un tiempo promedio de 1.48 segundos, cifras que destacan negativamente tanto en la tabla como en la gr√°fica con escala logar√≠tmica. En contraste, Heapsort se posiciona como el m√°s eficiente, con apenas 1918 comparaciones y un tiempo de ejecuci√≥n cercano a 1 milisegundo, siendo el m√°s r√°pido y consistente en esta prueba. Quicksort tambi√©n ofrece un rendimiento s√≥lido, con bajo tiempo y un n√∫mero de comparaciones razonable. Mergesort y Skiplist, si bien requieren m√°s comparaciones que Heapsort, mantienen tiempos aceptables. La escala logar√≠tmica empleada en los gr√°ficos permite visualizar adecuadamente estas diferencias extremas, resaltando el impacto del dise√±o algor√≠tmico en contextos adversos como el presentado en este conjunto de datos perturbados.\n\n\n\n\n\n\nAlgoritmo\nComparaciones\nTiempo (s)\n\n\n\n\nHeapsort\n1918.52\n0.000695\n\n\nMergesort\n16019.63\n0.005563\n\n\nQuicksort\n22342.45\n0.003617\n\n\nBubblesort\n12303709.00\n1.578796\n\n\nSkiplist\n21164.88\n0.010628\n\n\n\n\n\n\n\n\n\nEn el archivo listas-posteo-con-perturbaciones-p=032.json, se aprecia un patr√≥n similar al observado en otros conjuntos perturbados: Bubble Sort vuelve a destacar negativamente con una cantidad desproporcionada de comparaciones, superando los 12 millones, y un tiempo promedio de ejecuci√≥n de 1.57 segundos. Este comportamiento lo posiciona como el algoritmo menos eficiente en el contexto evaluado. Por el contrario, Heapsort se mantiene como el algoritmo m√°s eficiente, con apenas 1918 comparaciones y un tiempo de 0.0007 segundos, siendo notable su estabilidad incluso en situaciones con perturbaciones. Quicksort y Mergesort muestran un desempe√±o razonable, con tiempos bajos aunque un n√∫mero mayor de comparaciones en comparaci√≥n con Heapsort. Skiplist tambi√©n ofrece un rendimiento aceptable, aunque supera en comparaciones a Mergesort y en tiempo a Quicksort. Las gr√°ficas con escala logar√≠tmica permiten observar de manera clara estas diferencias de orden de magnitud, subrayando el impacto que tiene el dise√±o del algoritmo sobre su rendimiento en listas no ideales.\n\n\n\n\n\n\nAlgoritmo\nComparaciones\nTiempo (s)\n\n\n\n\nHeapsort\n1918.52\n0.000458\n\n\nMergesort\n16929.96\n0.005799\n\n\nQuicksort\n23524.43\n0.003824\n\n\nBubblesort\n12907386.23\n1.626831\n\n\nSkiplist\n21204.59\n0.012316\n\n\n\n\n\n\n\n\n\nEn el archivo listas-posteo-con-perturbaciones-p=064.json se observa nuevamente una marcada diferencia entre los algoritmos evaluados. Bubble Sort mantiene su tendencia negativa, alcanzando m√°s de 12 millones de comparaciones y un tiempo promedio de 1.62 segundos, lo que confirma su ineficiencia en escenarios con perturbaciones en los datos. Heapsort vuelve a destacar como el algoritmo m√°s eficiente, con apenas 1918 comparaciones y un tiempo promedio de tan solo 0.0005 segundos. Quicksort y Mergesort se comportan de forma aceptable: aunque sus comparaciones son considerablemente mayores que las de Heapsort (m√°s de 16,000 y 23,000 respectivamente), los tiempos de ejecuci√≥n se mantienen bajos, siendo competitivos frente a perturbaciones. Skiplist, por su parte, muestra un n√∫mero elevado de comparaciones (21,204) y un tiempo algo m√°s alto (0.012 segundos), pero dentro de rangos razonables. Las gr√°ficas presentadas con escala logar√≠tmica son fundamentales para apreciar las diferencias de rendimiento. En la primera, el pico de comparaciones de Bubble Sort sobresale dr√°sticamente, mientras que en la segunda se evidencia su desventaja en tiempo, contrastando con la eficiencia de Heapsort en ambos ejes. Esta visualizaci√≥n confirma que la elecci√≥n del algoritmo tiene un impacto directo y medible en el rendimiento cuando se trabaja con datos perturbados.\n\n\n\n\n\n\nAlgoritmo\nComparaciones\nTiempo (s)\n\n\n\n\nHeapsort\n1918.52\n0.000497\n\n\nMergesort\n17856.69\n0.006180\n\n\nQuicksort\n24763.26\n0.003765\n\n\nBubblesort\n13020287.94\n1.661634\n\n\nSkiplist\n21260.55\n0.010196\n\n\n\n\n\n\n\n\n\nEl archivo listas-posteo-con-perturbaciones-p=128.json refleja una vez m√°s una marcada disparidad en el rendimiento entre los algoritmos evaluados. Bubble Sort, al igual que en los casos anteriores, exhibe un desempe√±o deficiente, alcanzando m√°s de 13 millones de comparaciones y un tiempo promedio superior a 1.66 segundos. Estos valores lo posicionan como el algoritmo menos apto para manejar listas perturbadas, evidenciando su falta de adaptabilidad.\nEn contraste, Heapsort se mantiene como la opci√≥n m√°s eficiente, destacando por su bajo n√∫mero de comparaciones (1918) y un tiempo de ejecuci√≥n pr√°cticamente inmediato (0.0005 segundos). Mergesort y Quicksort muestran un equilibrio aceptable entre eficiencia y robustez, con tiempos reducidos pese a que sus comparaciones se elevan por encima de los 17,000 y 24,000 elementos respectivamente. Skiplist, por su parte, se sit√∫a en un punto intermedio: aunque su n√∫mero de comparaciones y tiempo son mayores, sus resultados siguen siendo competitivos frente a Bubble Sort.\nLa interpretaci√≥n visual mediante gr√°ficos en escala logar√≠tmica permite apreciar con claridad estas diferencias. Bubble Sort sobresale con picos desproporcionados en ambas m√©tricas, mientras que Heapsort se mantiene consistentemente en el rango m√°s eficiente. Estos contrastes evidencian la relevancia de una elecci√≥n informada del algoritmo de ordenamiento, especialmente cuando se enfrentan condiciones de entrada adversas o poco predecibles.\n\n\n\n\n\n\nAlgoritmo\nComparaciones\nTiempo (s)\n\n\n\n\nHeapsort\n1918.52\n0.001025\n\n\nMergesort\n18650.70\n0.005661\n\n\nQuicksort\n25839.39\n0.004006\n\n\nBubblesort\n13067025.01\n1.679046\n\n\nSkiplist\n19991.10\n0.011174\n\n\n\n\n\n\n\n\n\nEl an√°lisis del archivo listas-posteo-con-perturbaciones-p=256.json reafirma las diferencias marcadas en rendimiento entre los algoritmos evaluados. Bubble Sort destaca, una vez m√°s, por su ineficiencia al enfrentar perturbaciones en los datos: super√≥ los 13 millones de comparaciones y registr√≥ un tiempo de ejecuci√≥n de 1.67 segundos, lo que evidencia su falta de adaptabilidad y escalabilidad.\nEn contraste, Heapsort contin√∫a consolid√°ndose como la alternativa m√°s estable y eficaz, logrando completar la tarea con un n√∫mero m√≠nimo de comparaciones (1918) y un tiempo inferior al milisegundo. Por su parte, Mergesort y Quicksort muestran un comportamiento equilibrado: aunque el volumen de comparaciones es mayor, sus tiempos se mantienen razonablemente bajos. Skiplist se ubica en un punto intermedio, ofreciendo resultados aceptables pero sin llegar al rendimiento de los algoritmos m√°s eficientes.\nLas gr√°ficas con escala logar√≠tmica permiten dimensionar adecuadamente estas diferencias. La separaci√≥n entre Bubble Sort y el resto es abismal, lo cual facilita identificarlo como un caso claramente desfavorable. Heapsort, por otro lado, permanece consistentemente en el nivel m√°s bajo de esfuerzo computacional y tiempo, reafirmando su superioridad en este tipo de entornos. Esta visualizaci√≥n resalta la necesidad de elegir algoritmos con buen comportamiento tanto en teor√≠a como en pr√°ctica, especialmente cuando se trata de estructuras de datos poco √≥ptimas.\n\n\n\n\n\n\nAlgoritmo\nComparaciones\nTiempo (s)\n\n\n\n\nHeapsort\n1918.52\n0.001287\n\n\nMergesort\n19343.00\n0.005294\n\n\nQuicksort\n27047.10\n0.005086\n\n\nBubblesort\n13088171.64\n1.710422\n\n\nSkiplist\n20849.75\n0.010666\n\n\n\n\n\n\n\n\n\nEn el archivo listas-posteo-con-perturbaciones-p=512.json se vuelve a manifestar una brecha significativa entre los algoritmos, aunque esta vez resulta interesante observar c√≥mo ciertos valores tienden a estabilizarse pese al incremento de perturbaciones. Bubble Sort contin√∫a encabezando el listado en cuanto a ineficiencia, con m√°s de 13 millones de comparaciones y un tiempo de ejecuci√≥n de 1.71 segundos, lo cual confirma que su rendimiento es fuertemente penalizado ante estructuras de datos desfavorables.\nUn aspecto que llama la atenci√≥n es la constancia de Heapsort. Independientemente del nivel de perturbaci√≥n, mantiene el mismo n√∫mero de comparaciones (1918.52), lo cual refleja una gran estabilidad en su comportamiento. A pesar de que su tiempo de ejecuci√≥n ha crecido ligeramente en comparaci√≥n con escenarios anteriores, sigue siendo el m√°s eficiente del grupo.\nMergesort y Quicksort aumentan gradualmente sus comparaciones a medida que las perturbaciones se intensifican, superando las 19 mil y 27 mil respectivamente. Sin embargo, sus tiempos se mantienen en torno a los 5 milisegundos, lo cual indica que, aunque no son los m√°s √≥ptimos, conservan un rendimiento razonable.\nSkiplist muestra una ligera mejora en n√∫mero de comparaciones respecto al caso con p=256, pero sigue siendo m√°s lento en tiempo. Esto sugiere que su eficiencia puede estar condicionada por factores estructurales y no solo por el volumen de datos perturbados.\nLas gr√°ficas en escala logar√≠tmica siguen siendo fundamentales para dimensionar estas diferencias. Heapsort contin√∫a figurando como el algoritmo m√°s compacto visualmente, mientras que Bubble Sort desborda por completo las escalas. Esta visualizaci√≥n no solo destaca diferencias absolutas, sino tambi√©n patrones de crecimiento que ayudan a proyectar el rendimiento futuro si se escalan a√∫n m√°s los datos.\n\n\n\nLas variaciones observadas se deben a varios factores que se lograron visualizar en los gr√°ficos y en la lista de posteo. Las perturbaciones introducidas en la lista de posteo son peque√±as o no alteran significativamente el orden general de los datos; por lo tanto, los algoritmos de ordenamiento pueden comportarse de manera similar.\nEl tama√±o de la lista de posteo puede influir en c√≥mo los algoritmos manejan las perturbaciones. En listas grandes, las perturbaciones relativamente peque√±as pueden tener un impacto menor en el rendimiento general. Algunos algoritmos tienen un comportamiento asint√≥tico que los hace menos sensibles a peque√±as variaciones en el orden de los datos de entrada. Por ejemplo, algoritmos con una complejidad de tiempo de O(n log n) pueden mostrar un rendimiento estable m√°s all√° de cierto umbral de perturbaciones, ya que el costo adicional de manejar perturbaciones adicionales se vuelve insignificante en comparaci√≥n con el tama√±o total de los datos. (Huyen, 2022)\nLa forma en que se implementan los algoritmos puede afectar su rendimiento. Las optimizaciones espec√≠ficas, como el uso de t√©cnicas de cach√© o la minimizaci√≥n de operaciones de intercambio, pueden hacer que los algoritmos sean menos sensibles a las perturbaciones.\nLas m√©tricas utilizadas para evaluar el rendimiento, como el n√∫mero de comparaciones y el tiempo de ejecuci√≥n, pueden no capturar completamente el impacto de las perturbaciones. Otros factores, como el uso de memoria, la localidad de referencia y la complejidad algor√≠tmica, pueden proporcionar una visi√≥n m√°s completa del rendimiento de los algoritmos. (Cormen, 2009)\n\n\n\n\nAl finalizar este an√°lisis, puedo concluir que Heapsort fue, sin duda, el algoritmo m√°s eficiente y consistente a lo largo de todos los escenarios evaluados. Sin importar el nivel de perturbaci√≥n introducido en las listas de posteo, su rendimiento se mantuvo pr√°cticamente invariable, tanto en n√∫mero de comparaciones como en tiempo de ejecuci√≥n. Esto lo posiciona como una opci√≥n muy confiable en contextos donde los datos no est√°n completamente ordenados.\nPor el contrario, Bubble Sort demostr√≥ ser el menos adecuado para este tipo de condiciones, registrando cifras excesivas de comparaciones y tiempos considerablemente altos en todos los casos. Su sensibilidad al desorden confirma que no es una buena elecci√≥n cuando se trabaja con listas grandes o con alguna alteraci√≥n en el orden.\nMergesort y Quicksort ofrecieron un equilibrio bastante s√≥lido, especialmente en cuanto a tiempos de ejecuci√≥n. Aunque sus comparaciones aumentaron con el grado de perturbaci√≥n, su comportamiento se mantuvo dentro de rangos aceptables, lo que los hace adecuados para escenarios donde se busca rapidez con una tolerancia razonable a la eficiencia.\nEn cuanto a Skiplist, observ√© un rendimiento intermedio. No fue el m√°s r√°pido, pero tampoco present√≥ problemas extremos. Su comportamiento parece depender m√°s de la estructura interna de los datos, lo cual puede ser un factor a considerar si se busca estabilidad.\nFinalmente, algo que considero clave es que la elecci√≥n del algoritmo no debe basarse √∫nicamente en su complejidad te√≥rica. La pr√°ctica muestra que la eficiencia real tambi√©n depende de otros factores como la implementaci√≥n, el tama√±o de los datos, la naturaleza del desorden y caracter√≠sticas como el uso de memoria o la optimizaci√≥n interna. Evaluar estos aspectos permite tomar decisiones m√°s informadas y adecuadas seg√∫n el contexto del problema.\n\n\n\n\nCormen, T. H., Leiserson, C. E., Rivest, R. L., & Stein, C. (2009). Introduction to Algorithms (3rd ed.). MIT Press.\nKnuth, D. E. (2011). The Art of Computer Programming, Volume 4A: Combinatorial Algorithms, Part 1. Addison-Wesley.\nRizvi, Q., Rai, H., & Jaiswal, R. (2024). Sorting Algorithms in Focus: A Critical Examination of Sorting Algorithm Performance.\nHuyen, C. (2022). Designing Machine Learning Systems. O‚ÄôReilly Media, Inc.\n\n\n\n\nCon base en las observaciones realizadas durante la videoconferencia y en las notas complementarias, realic√© una serie de ajustes importantes en el desarrollo de mi trabajo.\nPrimero, correg√≠ la afirmaci√≥n sobre el algoritmo Bubble Sort, aclarando que no es un algoritmo adaptativo. Esta correcci√≥n fue reflejada tanto en la descripci√≥n como en la discusi√≥n de los resultados.\nTambi√©n ajust√© las escalas de las gr√°ficas generadas para cada algoritmo, con el objetivo de permitir una mejor visualizaci√≥n y comparaci√≥n de sus comportamientos, especialmente en casos con tama√±os de entrada moderados.\nEn cuanto a Merge Sort, revis√© la implementaci√≥n para evitar el uso innecesario de memoria adicional, ya que esto afectaba negativamente su eficiencia.\nPara Quick Sort, correg√≠ la estrategia de selecci√≥n del pivote, siguiendo las recomendaciones revisadas en clase, y document√© c√≥mo esta decisi√≥n influye directamente en el rendimiento del algoritmo.\nPor √∫ltimo, elimin√© el uso del n√∫mero ‚Äú√≠nfimo‚Äù en la implementaci√≥n de Skip List, ya que comprend√≠ que no era necesario dentro del modelo de comparaci√≥n. Tambi√©n evit√© definir n√∫meros espec√≠ficos en los an√°lisis, respetando el enfoque abstracto que se abord√≥ en el curso.\nEstos cambios me ayudaron a alinear mejor mi trabajo con los principios te√≥ricos del an√°lisis algor√≠tmico y a fortalecer mi comprensi√≥n de los conceptos trabajados."
  },
  {
    "objectID": "project4.html",
    "href": "project4.html",
    "title": "Proyecto 4",
    "section": "",
    "text": "comparaci√≥n.\n\n\nEn la ciencia de la computaci√≥n, los algoritmos de b√∫squeda son esenciales para optimizar la eficiencia en la gesti√≥n y recuperaci√≥n de datos. Estos algoritmos permiten localizar elementos espec√≠ficos dentro de una colecci√≥n de datos, optimizando el tiempo y los recursos necesarios para dicha tarea. Este informe se centra en la implementaci√≥n y comparaci√≥n de varios algoritmos de b√∫squeda por comparaci√≥n, con el objetivo de evaluar su rendimiento en cuanto al n√∫mero de comparaciones y el tiempo de ejecuci√≥n (Cormen et al., 2009).\nLos algoritmos de b√∫squeda por comparaci√≥n se basan en la comparaci√≥n de elementos para encontrar el objetivo deseado. Entre los algoritmos m√°s conocidos se encuentran la b√∫squeda binaria, la b√∫squeda secuencial y variantes de b√∫squedas no acotadas. Su comportamiento depende de la estructura de datos utilizada y de las condiciones de b√∫squeda (Knuth, 1998).\nLa b√∫squeda binaria acotada es una t√©cnica eficiente para encontrar un elemento en un conjunto de datos ordenado. Este algoritmo divide repetidamente el conjunto de datos a la mitad, comparando el elemento objetivo con el elemento central del subconjunto actual. Si el elemento central es igual al objetivo, la b√∫squeda termina. Si es mayor o menor, la b√∫squeda contin√∫a en la mitad inferior o superior, respectivamente. Esta t√©cnica es altamente eficiente en conjuntos de datos ordenados, reduciendo significativamente el n√∫mero de comparaciones necesarias (Cormen et al., 2009).\nLa b√∫squeda secuencial, tambi√©n conocida como b√∫squeda lineal, es un m√©todo simple y directo para encontrar un elemento en un conjunto de datos. El algoritmo recorre cada elemento del conjunto en orden secuencial, comparando cada uno con el elemento objetivo hasta encontrarlo o llegar al final. Aunque es menos eficiente que la b√∫squeda binaria en conjuntos ordenados, su aplicabilidad se extiende a cualquier tipo de conjunto, ordenado o no. Las variantes de b√∫squeda no acotada, como B1 y B2, extienden este concepto para manejar conjuntos de datos cuyo tama√±o es desconocido o cambia constantemente (Sedgewick, 2011).\nLas SkipLists son estructuras de datos probabil√≠sticas que permiten b√∫squedas eficientes en conjuntos de datos ordenados. A diferencia de las listas enlazadas tradicionales, las SkipLists utilizan m√∫ltiples niveles de enlaces para acelerar el proceso de b√∫squeda. Esta estructura ofrece eficiencia en b√∫squedas sobre conjuntos ordenados y una mayor flexibilidad. Sin embargo, su desventaja radica en el uso adicional de memoria debido a los m√∫ltiples niveles de enlaces (Knuth, 1998).\nEn este trabajo se implementaron y compararon cuatro algoritmos de b√∫squeda: b√∫squeda binaria acotada, b√∫squeda secuencial y dos variantes de b√∫squeda no acotada (B1 y B2), adem√°s de la estructura de datos SkipList, con el fin de evaluar su eficiencia en t√©rminos de n√∫mero de comparaciones y tiempo de ejecuci√≥n. Utilizando conjuntos de datos y consultas espec√≠ficas, se midi√≥ el rendimiento de cada m√©todo, registrando los resultados para cada combinaci√≥n de archivos. Los resultados se visualizaron mediante gr√°ficos y tablas, destacando las ventajas y desventajas de cada enfoque.\nSe concluye que la elecci√≥n del m√©todo de b√∫squeda depende del contexto de aplicaci√≥n. Este an√°lisis proporciona una gu√≠a √∫til para seleccionar el m√©todo m√°s adecuado seg√∫n los requerimientos espec√≠ficos de rendimiento y aplicabilidad pr√°ctica.\n\n\n\n\n\n\n\n\n\n\n\n\n\nCormen, T. H., Leiserson, C. E., Rivest, R. L., & Stein, C. (2009). Introduction to Algorithms (3rd ed.).\nSedgewick, R., & Wayne, K. (2011). Algorithms (4th ed.).\nKnuth, D. E. (1998). The Art of Computer Programming, Volume 3: Sorting and Searching (2nd ed.).\nWeiss, M. A. (2014). Data Structures and Algorithm Analysis in C (3rd ed.). Pearson.\n\n\n\n\nEn atenci√≥n a la retroalimentaci√≥n recibida, se realiz√≥ una revisi√≥n profunda del enfoque adoptado en la implementaci√≥n de los algoritmos de b√∫squeda por comparaci√≥n. A continuaci√≥n se detallan los ajustes conceptuales y t√©cnicos aplicados:\n\nSe reconoci√≥ que el uso de listas de posteo con perturbaciones gener√≥ una interpretaci√≥n err√≥nea del problema, llevando a implementar algoritmos basados √∫nicamente en igualdad, cuando el problema central requer√≠a considerar comparaciones con operadores &lt; o &lt;= para determinar la posici√≥n de inserci√≥n.\nSe reformularon los algoritmos de b√∫squeda secuencial y binaria considerando adecuadamente la sem√°ntica del problema, que implica la ubicaci√≥n correcta en listas ordenadas, no la coincidencia exacta.\nSe revisaron las notas del curso y el art√≠culo de Baeza-Yates (B&Y), lo que permiti√≥ entender con mayor claridad el modelo de comparaci√≥n y sus implicaciones para el dise√±o e interpretaci√≥n correcta de los algoritmos.\nSe corrigieron errores l√≥gicos en las funciones de b√∫squeda y se realizaron pruebas con datos estructurados correctamente, evitando distorsiones generadas por entradas mal definidas."
  },
  {
    "objectID": "project3.html#revisi√≥n-de-mejoras-implementadas",
    "href": "project3.html#revisi√≥n-de-mejoras-implementadas",
    "title": "Proyecto 3",
    "section": "",
    "text": "Tras revisar las observaciones recibidas, se realizaron mejoras clave en la implementaci√≥n y an√°lisis de los algoritmos. En primer lugar, se corrigi√≥ el algoritmo de Bubble Sort para que fuera adaptativo, lo cual se refleja claramente en las gr√°ficas y en el elevado n√∫mero de comparaciones cuando se enfrenta a casos desfavorables.\nAsimismo, se optimiz√≥ Merge Sort para evitar el uso innecesario de memoria adicional, mejorando as√≠ su eficiencia pr√°ctica. Tambi√©n se corrigi√≥ la selecci√≥n del pivote en Quick Sort, utilizando ahora el elemento central para evitar los peores casos en listas parcialmente ordenadas.\nEn cuanto a SkipList, se elimin√≥ el uso del valor artificial -‚àû, garantizando que todas las comparaciones se realicen solo entre elementos reales, como exige el modelo de comparaci√≥n.\nFinalmente, se ajustaron las escalas de las gr√°ficas, empleando una escala logar√≠tmica para las comparaciones y m√°rgenes din√°micos para el tiempo, facilitando una interpretaci√≥n visual m√°s clara y justa.\nCon estas mejoras, el trabajo queda alineado con los lineamientos discutidos y refleja correctamente el comportamiento de cada algoritmo."
  },
  {
    "objectID": "project3.html#resultados-experimentales-y-an√°lisis-de-resultados",
    "href": "project3.html#resultados-experimentales-y-an√°lisis-de-resultados",
    "title": "Proyecto 3",
    "section": "",
    "text": "A continuaci√≥n, se presentan los resultados promedio de comparaciones y tiempos de ejecuci√≥n para cada archivo de lista de posteo con perturbaciones. Los datos se agrupan por archivo, lo cual permite observar el comportamiento de los algoritmos en funci√≥n de la variabilidad en las listas. Se visualizan a continuaci√≥n Tablas y Gr √°ficos.\n\n\n\n\n\nAlgoritmo\nComparaciones\nTiempo (s)\n\n\n\n\nHeapsort\n1918.52\n0.000966\n\n\nMergesort\n15031.85\n0.004780\n\n\nQuicksort\n21248.43\n0.002659\n\n\nBubblesort\n12394236.90\n1.484842\n\n\nSkiplist\n20545.83\n0.013274\n\n\n\n\n\n\n\n\n\nEn la tabla de promedios y los gr√°ficos generados para el archivo listas-posteo-con-perturbaciones-p=016.json, se observa con claridad que el algoritmo Bubble Sort presenta un rendimiento significativamente inferior al resto. Este algoritmo registr√≥ m√°s de 12 millones de comparaciones y un tiempo promedio de 1.48 segundos, cifras que destacan negativamente tanto en la tabla como en la gr√°fica con escala logar√≠tmica. En contraste, Heapsort se posiciona como el m√°s eficiente, con apenas 1918 comparaciones y un tiempo de ejecuci√≥n cercano a 1 milisegundo, siendo el m√°s r√°pido y consistente en esta prueba. Quicksort tambi√©n ofrece un rendimiento s√≥lido, con bajo tiempo y un n√∫mero de comparaciones razonable. Mergesort y Skiplist, si bien requieren m√°s comparaciones que Heapsort, mantienen tiempos aceptables. La escala logar√≠tmica empleada en los gr√°ficos permite visualizar adecuadamente estas diferencias extremas, resaltando el impacto del dise√±o algor√≠tmico en contextos adversos como el presentado en este conjunto de datos perturbados.\n\n\n\n\n\n\nAlgoritmo\nComparaciones\nTiempo (s)\n\n\n\n\nHeapsort\n1918.52\n0.000695\n\n\nMergesort\n16019.63\n0.005563\n\n\nQuicksort\n22342.45\n0.003617\n\n\nBubblesort\n12303709.00\n1.578796\n\n\nSkiplist\n21164.88\n0.010628\n\n\n\n\n\n\n\n\n\nEn el archivo listas-posteo-con-perturbaciones-p=032.json, se aprecia un patr√≥n similar al observado en otros conjuntos perturbados: Bubble Sort vuelve a destacar negativamente con una cantidad desproporcionada de comparaciones, superando los 12 millones, y un tiempo promedio de ejecuci√≥n de 1.57 segundos. Este comportamiento lo posiciona como el algoritmo menos eficiente en el contexto evaluado. Por el contrario, Heapsort se mantiene como el algoritmo m√°s eficiente, con apenas 1918 comparaciones y un tiempo de 0.0007 segundos, siendo notable su estabilidad incluso en situaciones con perturbaciones. Quicksort y Mergesort muestran un desempe√±o razonable, con tiempos bajos aunque un n√∫mero mayor de comparaciones en comparaci√≥n con Heapsort. Skiplist tambi√©n ofrece un rendimiento aceptable, aunque supera en comparaciones a Mergesort y en tiempo a Quicksort. Las gr√°ficas con escala logar√≠tmica permiten observar de manera clara estas diferencias de orden de magnitud, subrayando el impacto que tiene el dise√±o del algoritmo sobre su rendimiento en listas no ideales.\n\n\n\n\n\n\nAlgoritmo\nComparaciones\nTiempo (s)\n\n\n\n\nHeapsort\n1918.52\n0.000458\n\n\nMergesort\n16929.96\n0.005799\n\n\nQuicksort\n23524.43\n0.003824\n\n\nBubblesort\n12907386.23\n1.626831\n\n\nSkiplist\n21204.59\n0.012316\n\n\n\n\n\n\n\n\n\nEn el archivo listas-posteo-con-perturbaciones-p=064.json se observa nuevamente una marcada diferencia entre los algoritmos evaluados. Bubble Sort mantiene su tendencia negativa, alcanzando m√°s de 12 millones de comparaciones y un tiempo promedio de 1.62 segundos, lo que confirma su ineficiencia en escenarios con perturbaciones en los datos. Heapsort vuelve a destacar como el algoritmo m√°s eficiente, con apenas 1918 comparaciones y un tiempo promedio de tan solo 0.0005 segundos. Quicksort y Mergesort se comportan de forma aceptable: aunque sus comparaciones son considerablemente mayores que las de Heapsort (m√°s de 16,000 y 23,000 respectivamente), los tiempos de ejecuci√≥n se mantienen bajos, siendo competitivos frente a perturbaciones. Skiplist, por su parte, muestra un n√∫mero elevado de comparaciones (21,204) y un tiempo algo m√°s alto (0.012 segundos), pero dentro de rangos razonables. Las gr√°ficas presentadas con escala logar√≠tmica son fundamentales para apreciar las diferencias de rendimiento. En la primera, el pico de comparaciones de Bubble Sort sobresale dr√°sticamente, mientras que en la segunda se evidencia su desventaja en tiempo, contrastando con la eficiencia de Heapsort en ambos ejes. Esta visualizaci√≥n confirma que la elecci√≥n del algoritmo tiene un impacto directo y medible en el rendimiento cuando se trabaja con datos perturbados.\n\n\n\n\n\n\nAlgoritmo\nComparaciones\nTiempo (s)\n\n\n\n\nHeapsort\n1918.52\n0.000497\n\n\nMergesort\n17856.69\n0.006180\n\n\nQuicksort\n24763.26\n0.003765\n\n\nBubblesort\n13020287.94\n1.661634\n\n\nSkiplist\n21260.55\n0.010196\n\n\n\n\n\n\n\n\n\nEl archivo listas-posteo-con-perturbaciones-p=128.json refleja una vez m√°s una marcada disparidad en el rendimiento entre los algoritmos evaluados. Bubble Sort, al igual que en los casos anteriores, exhibe un desempe√±o deficiente, alcanzando m√°s de 13 millones de comparaciones y un tiempo promedio superior a 1.66 segundos. Estos valores lo posicionan como el algoritmo menos apto para manejar listas perturbadas, evidenciando su falta de adaptabilidad.\nEn contraste, Heapsort se mantiene como la opci√≥n m√°s eficiente, destacando por su bajo n√∫mero de comparaciones (1918) y un tiempo de ejecuci√≥n pr√°cticamente inmediato (0.0005 segundos). Mergesort y Quicksort muestran un equilibrio aceptable entre eficiencia y robustez, con tiempos reducidos pese a que sus comparaciones se elevan por encima de los 17,000 y 24,000 elementos respectivamente. Skiplist, por su parte, se sit√∫a en un punto intermedio: aunque su n√∫mero de comparaciones y tiempo son mayores, sus resultados siguen siendo competitivos frente a Bubble Sort.\nLa interpretaci√≥n visual mediante gr√°ficos en escala logar√≠tmica permite apreciar con claridad estas diferencias. Bubble Sort sobresale con picos desproporcionados en ambas m√©tricas, mientras que Heapsort se mantiene consistentemente en el rango m√°s eficiente. Estos contrastes evidencian la relevancia de una elecci√≥n informada del algoritmo de ordenamiento, especialmente cuando se enfrentan condiciones de entrada adversas o poco predecibles.\n\n\n\n\n\n\nAlgoritmo\nComparaciones\nTiempo (s)\n\n\n\n\nHeapsort\n1918.52\n0.001025\n\n\nMergesort\n18650.70\n0.005661\n\n\nQuicksort\n25839.39\n0.004006\n\n\nBubblesort\n13067025.01\n1.679046\n\n\nSkiplist\n19991.10\n0.011174\n\n\n\n\n\n\n\n\n\nEl an√°lisis del archivo listas-posteo-con-perturbaciones-p=256.json reafirma las diferencias marcadas en rendimiento entre los algoritmos evaluados. Bubble Sort destaca, una vez m√°s, por su ineficiencia al enfrentar perturbaciones en los datos: super√≥ los 13 millones de comparaciones y registr√≥ un tiempo de ejecuci√≥n de 1.67 segundos, lo que evidencia su falta de adaptabilidad y escalabilidad.\nEn contraste, Heapsort contin√∫a consolid√°ndose como la alternativa m√°s estable y eficaz, logrando completar la tarea con un n√∫mero m√≠nimo de comparaciones (1918) y un tiempo inferior al milisegundo. Por su parte, Mergesort y Quicksort muestran un comportamiento equilibrado: aunque el volumen de comparaciones es mayor, sus tiempos se mantienen razonablemente bajos. Skiplist se ubica en un punto intermedio, ofreciendo resultados aceptables pero sin llegar al rendimiento de los algoritmos m√°s eficientes.\nLas gr√°ficas con escala logar√≠tmica permiten dimensionar adecuadamente estas diferencias. La separaci√≥n entre Bubble Sort y el resto es abismal, lo cual facilita identificarlo como un caso claramente desfavorable. Heapsort, por otro lado, permanece consistentemente en el nivel m√°s bajo de esfuerzo computacional y tiempo, reafirmando su superioridad en este tipo de entornos. Esta visualizaci√≥n resalta la necesidad de elegir algoritmos con buen comportamiento tanto en teor√≠a como en pr√°ctica, especialmente cuando se trata de estructuras de datos poco √≥ptimas.\n\n\n\n\n\n\nAlgoritmo\nComparaciones\nTiempo (s)\n\n\n\n\nHeapsort\n1918.52\n0.001287\n\n\nMergesort\n19343.00\n0.005294\n\n\nQuicksort\n27047.10\n0.005086\n\n\nBubblesort\n13088171.64\n1.710422\n\n\nSkiplist\n20849.75\n0.010666\n\n\n\n\n\n\n\n\n\nEn el archivo listas-posteo-con-perturbaciones-p=512.json se vuelve a manifestar una brecha significativa entre los algoritmos, aunque esta vez resulta interesante observar c√≥mo ciertos valores tienden a estabilizarse pese al incremento de perturbaciones. Bubble Sort contin√∫a encabezando el listado en cuanto a ineficiencia, con m√°s de 13 millones de comparaciones y un tiempo de ejecuci√≥n de 1.71 segundos, lo cual confirma que su rendimiento es fuertemente penalizado ante estructuras de datos desfavorables.\nUn aspecto que llama la atenci√≥n es la constancia de Heapsort. Independientemente del nivel de perturbaci√≥n, mantiene el mismo n√∫mero de comparaciones (1918.52), lo cual refleja una gran estabilidad en su comportamiento. A pesar de que su tiempo de ejecuci√≥n ha crecido ligeramente en comparaci√≥n con escenarios anteriores, sigue siendo el m√°s eficiente del grupo.\nMergesort y Quicksort aumentan gradualmente sus comparaciones a medida que las perturbaciones se intensifican, superando las 19 mil y 27 mil respectivamente. Sin embargo, sus tiempos se mantienen en torno a los 5 milisegundos, lo cual indica que, aunque no son los m√°s √≥ptimos, conservan un rendimiento razonable.\nSkiplist muestra una ligera mejora en n√∫mero de comparaciones respecto al caso con p=256, pero sigue siendo m√°s lento en tiempo. Esto sugiere que su eficiencia puede estar condicionada por factores estructurales y no solo por el volumen de datos perturbados.\nLas gr√°ficas en escala logar√≠tmica siguen siendo fundamentales para dimensionar estas diferencias. Heapsort contin√∫a figurando como el algoritmo m√°s compacto visualmente, mientras que Bubble Sort desborda por completo las escalas. Esta visualizaci√≥n no solo destaca diferencias absolutas, sino tambi√©n patrones de crecimiento que ayudan a proyectar el rendimiento futuro si se escalan a√∫n m√°s los datos.\n\n\n\nLas variaciones observadas se deben a varios factores que se lograron visualizar en los gr√°ficos y en la lista de posteo. Las perturbaciones introducidas en la lista de posteo son peque√±as o no alteran significativamente el orden general de los datos; por lo tanto, los algoritmos de ordenamiento pueden comportarse de manera similar.\nEl tama√±o de la lista de posteo puede influir en c√≥mo los algoritmos manejan las perturbaciones. En listas grandes, las perturbaciones relativamente peque√±as pueden tener un impacto menor en el rendimiento general. Algunos algoritmos tienen un comportamiento asint√≥tico que los hace menos sensibles a peque√±as variaciones en el orden de los datos de entrada. Por ejemplo, algoritmos con una complejidad de tiempo de O(n log n) pueden mostrar un rendimiento estable m√°s all√° de cierto umbral de perturbaciones, ya que el costo adicional de manejar perturbaciones adicionales se vuelve insignificante en comparaci√≥n con el tama√±o total de los datos. (Huyen, 2022)\nLa forma en que se implementan los algoritmos puede afectar su rendimiento. Las optimizaciones espec√≠ficas, como el uso de t√©cnicas de cach√© o la minimizaci√≥n de operaciones de intercambio, pueden hacer que los algoritmos sean menos sensibles a las perturbaciones.\nLas m√©tricas utilizadas para evaluar el rendimiento, como el n√∫mero de comparaciones y el tiempo de ejecuci√≥n, pueden no capturar completamente el impacto de las perturbaciones. Otros factores, como el uso de memoria, la localidad de referencia y la complejidad algor√≠tmica, pueden proporcionar una visi√≥n m√°s completa del rendimiento de los algoritmos. (Cormen, 2009)"
  }
]