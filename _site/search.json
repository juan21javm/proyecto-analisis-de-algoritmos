[
  {
    "objectID": "project5.html#introducción",
    "href": "project5.html#introducción",
    "title": "Proyecto 5",
    "section": "1. Introducción",
    "text": "1. Introducción"
  },
  {
    "objectID": "project5.html#desarrollo",
    "href": "project5.html#desarrollo",
    "title": "Proyecto 5",
    "section": "2. Desarrollo",
    "text": "2. Desarrollo"
  },
  {
    "objectID": "project5.html#análisis-de-resultados",
    "href": "project5.html#análisis-de-resultados",
    "title": "Proyecto 5",
    "section": "3. Análisis de Resultados",
    "text": "3. Análisis de Resultados"
  },
  {
    "objectID": "project5.html#conclusiones",
    "href": "project5.html#conclusiones",
    "title": "Proyecto 5",
    "section": "4. Conclusiones",
    "text": "4. Conclusiones"
  },
  {
    "objectID": "project5.html#referencias",
    "href": "project5.html#referencias",
    "title": "Proyecto 5",
    "section": "5. Referencias",
    "text": "5. Referencias"
  },
  {
    "objectID": "project5.html#cambios-realizados",
    "href": "project5.html#cambios-realizados",
    "title": "Proyecto 5",
    "section": "6. Cambios Realizados",
    "text": "6. Cambios Realizados"
  },
  {
    "objectID": "project3.html#introducción",
    "href": "project3.html#introducción",
    "title": "Proyecto 3",
    "section": "",
    "text": "La investigación sobre el análisis de algoritmos de ordenamiento es un campo activo, con numerosos estudios que exploran nuevas técnicas y optimizaciones. Ordenar datos es crucial en la ciencia de datos y la computación, con aplicaciones que abarcan desde la gestión de bases de datos hasta la optimización de algoritmos en inteligencia artificial. La eficiencia de los algoritmos de ordenamiento es importante, ya que puede influir significativamente en el rendimiento general de un sistema.\nLa eficiencia de un algoritmo de ordenamiento se mide principalmente en términos de tiempo y espacio. El tiempo se refiere a cuántas operaciones realiza el algoritmo, mientras que el espacio indica cuánta memoria adicional necesita. Un algoritmo de ordenamiento es estable si mantiene el orden relativo de los elementos iguales. La estabilidad es crucial en aplicaciones donde el orden original de los elementos iguales debe preservarse. Algunos algoritmos pueden aprovechar el orden existente en los datos para mejorar su rendimiento, mostrando así una adaptabilidad eficiente. La complejidad del algoritmo en el peor caso es una métrica importante para evaluar su robustez (Knuth, 2011).\nLa elección del algoritmo de ordenamiento adecuado depende de varios factores, incluyendo el tamaño de los datos, el grado de desorden y las restricciones de tiempo y espacio. El algoritmo Heapsort utiliza una estructura de datos llamada heap para ordenar los elementos. Es eficiente en términos de uso de memoria y tiene una complejidad de \\(O(n \\log n)\\). Por otro lado, el algoritmo Mergesort es un enfoque de divide y vencerás que divide la lista en mitades, las ordena recursivamente y luego las fusiona. Es estable y tiene una complejidad de \\(O(n \\log n)\\).\nQuicksort también sigue el enfoque de divide y vencerás, seleccionando un elemento como pivote y particionando la lista en elementos menores y mayores. Aunque en promedio es rápido, puede degradarse a \\(O(n^2)\\) en el peor caso. Bubblesort es un algoritmo simple que compara elementos adyacentes y los intercambia si están en el orden incorrecto. Es ineficiente para grandes conjuntos de datos debido a su complejidad cuadrática. Finalmente, la estructura de datos SkipList es una estructura probabilística que permite búsquedas y ordenamientos eficientes. Aunque no es un algoritmo de ordenamiento tradicional, puede ser utilizada para mantener una lista ordenada de elementos (Cormen, 2009).\nPara evaluar y comparar el rendimiento de estos cinco algoritmos de ordenamiento (Heapsort, Mergesort, Quicksort, Bubblesort y SkipList), se seleccionaron primero los algoritmos basándose en sus características y relevancia práctica. Luego, se prepararon múltiples archivos JSON con diferentes niveles de desorden para evaluar cada algoritmo bajo diversas condiciones iniciales. Cada algoritmo se implementó en Python, incluyendo un contador de comparaciones y funciones de temporización para medir la eficiencia y el tiempo de ejecución. Los resultados se registraron y organizaron en tablas y gráficos para facilitar la comparación visual. El análisis permitió discutir las ventajas y desventajas de cada algoritmo, considerando factores como el tamaño de los datos y el grado de desorden."
  },
  {
    "objectID": "project3.html#desarrollo",
    "href": "project3.html#desarrollo",
    "title": "Proyecto 3",
    "section": "",
    "text": "# Bibliotecas utilizadas para manejo de archivos, estructuras, algoritmos y visualización\nimport json\nimport os\nimport heapq\nimport random\nimport time\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Esta función recorre todos los archivos .json del directorio y carga sus listas de posteo\n# Decidí separarlo en una función para poder reutilizarlo fácilmente en otros experimentos\n\ndef load_json_files(directory):\n    all_files_data = {}\n    for filename in os.listdir(directory):\n        if filename.endswith('.json'):\n            file_path = os.path.join(directory, filename)\n            with open(file_path, 'r') as file:\n                data = json.load(file)\n                listas_posteo = list(data.values())\n                all_files_data[filename] = listas_posteo\n    return all_files_data\n\n# Ruta fija al directorio local donde se ubican las listas de posteo perturbadas\n# Este path es local, pero se puede cambiar fácilmente para otro entorno\n\ndirectory_path = 'C:\\\\Users\\\\Antonio Martínez\\\\Desktop\\\\listas-posteo-con-perturbaciones' \nfiles_data = load_json_files(directory_path)\n\n\n\n\n\n# Uso de heapq para convertir la lista en un heap\n# Cada extracción del mínimo simula la ordenación ascendente\n\ndef heapsort(arr):\n    comparisons = 0\n    heapq.heapify(arr)\n    sorted_arr = []\n    while arr:\n        comparisons += 1\n        sorted_arr.append(heapq.heappop(arr))\n    return sorted_arr, comparisons\n\n\n\n# Mergesort implementado de manera recursiva\n# Es muy útil para listas grandes, ya que divide y conquista\n\ndef mergesort(arr):\n    comparisons = 0\n\n    def merge_sort_recursive(arr):\n        nonlocal comparisons\n        if len(arr) &gt; 1:\n            mid = len(arr) // 2\n            L = arr[:mid]\n            R = arr[mid:]\n\n            merge_sort_recursive(L)\n            merge_sort_recursive(R)\n\n            i = j = k = 0\n            while i &lt; len(L) and j &lt; len(R):\n                comparisons += 1\n                if L[i] &lt; R[j]:\n                    arr[k] = L[i]\n                    i += 1\n                else:\n                    arr[k] = R[j]\n                    j += 1\n                k += 1\n\n            while i &lt; len(L):\n                arr[k] = L[i]\n                i += 1\n                k += 1\n\n            while j &lt; len(R):\n                arr[k] = R[j]\n                j += 1\n                k += 1\n\n    merge_sort_recursive(arr)\n    return arr, comparisons\n\n\n\n# Uso de Quicksort con selección del pivote en la posición media\n# Aumenté el conteo de comparaciones para evaluar su eficiencia\n\ndef quicksort(arr):\n    comparisons = 0\n\n    def _quicksort(arr, low, high):\n        nonlocal comparisons\n        if low &lt; high:\n            pi = partition(arr, low, high)\n            _quicksort(arr, low, pi - 1)\n            _quicksort(arr, pi + 1, high)\n\n    def partition(arr, low, high):\n        nonlocal comparisons\n        mid = (low + high) // 2\n        pivot = arr[mid]\n        arr[mid], arr[high] = arr[high], arr[mid]\n        i = low - 1\n        for j in range(low, high):\n            comparisons += 1\n            if arr[j] &lt;= pivot:\n                i += 1\n                arr[i], arr[j] = arr[j], arr[i]\n        arr[i+1], arr[high] = arr[high], arr[i+1]\n        return i+1\n\n    _quicksort(arr, 0, len(arr)-1)\n    return arr, comparisons\n\n\n\n# Algoritmo simple pero ineficiente para listas grandes\n# Le agregué una condición para detenerse si ya está ordenada\n\ndef bubblesort(arr):\n    n = len(arr)\n    comparisons = 0\n    for i in range(n):\n        swapped = False\n        for j in range(0, n-i-1):\n            comparisons += 1\n            if arr[j] &gt; arr[j+1]:\n                arr[j], arr[j+1] = arr[j+1], arr[j]\n                swapped = True\n        if not swapped:\n            break\n    return arr, comparisons\n\n\n\n# Implementación básica de SkipList adaptada para ordenar\n# Usa niveles probabilísticos para simular ordenamiento eficiente\n\nclass SkipListNode:\n    def __init__(self, value, level):\n        self.value = value\n        self.forward = [None] * (level + 1)\n\nclass SkipList:\n    def __init__(self, max_level, p=0.5):\n        self.max_level = max_level\n        self.p = p\n        self.header = SkipListNode(None, max_level)\n        self.level = 0\n        self.comparisons = 0\n\n    def random_level(self):\n        lvl = 0\n        while random.random() &lt; self.p and lvl &lt; self.max_level:\n            lvl += 1\n        return lvl\n\n    def insert(self, value):\n        update = [None] * (self.max_level + 1)\n        current = self.header\n\n        for i in reversed(range(self.level + 1)):\n            while current.forward[i] and current.forward[i].value &lt; value:\n                self.comparisons += 1\n                current = current.forward[i]\n            update[i] = current\n\n        lvl = self.random_level()\n        if lvl &gt; self.level:\n            for i in range(self.level + 1, lvl + 1):\n                update[i] = self.header\n            self.level = lvl\n\n        new_node = SkipListNode(value, lvl)\n        for i in range(lvl + 1):\n            new_node.forward[i] = update[i].forward[i]\n            update[i].forward[i] = new_node\n\n    def traverse(self):\n        result = []\n        current = self.header.forward[0]\n        while current:\n            result.append(current.value)\n            current = current.forward[0]\n        return result\n\ndef skiplist_sort(arr):\n    max_level = 16\n    sl = SkipList(max_level)\n    for value in arr:\n        sl.insert(value)\n    sorted_arr = sl.traverse()\n    return sorted_arr, sl.comparisons\n\n\n\n\n# Esta función me permitió unificar cómo mido el tiempo y el número de comparaciones\n# para cada algoritmo. La utilicé en todos los experimentos posteriores.\n\ndef measure_sorting_algorithm(algorithm, arr):\n    start_time = time.time()\n    sorted_arr, comparisons = algorithm(arr.copy())\n    end_time = time.time()\n    return sorted_arr, comparisons, end_time - start_time\n\n\n\n# Ejecuto todos los algoritmos en cada lista del dataset\n# Almaceno comparaciones y tiempo para analizarlos más adelante\n\nall_files_results = {}\n\nfor filename, listas_posteo in files_data.items():\n    all_results = []\n    for lista in listas_posteo:\n        results = {}\n        for name, algo in [\n            (\"Heapsort\", heapsort),\n            (\"Mergesort\", mergesort),\n            (\"Quicksort\", quicksort),\n            (\"Bubblesort\", bubblesort),\n            (\"Skiplist\", skiplist_sort)\n        ]:\n            sorted_arr, comparisons, time_taken = measure_sorting_algorithm(algo, lista)\n            results[name] = {'Comparaciones': comparisons, 'Tiempo': time_taken}\n        all_results.append(results)\n    all_files_results[filename] = all_results\n\n\n\n# Para cada archivo muestro un resumen con barras comparativas de tiempo y comparaciones\n# Las comparaciones van en escala logarítmica para mayor visibilidad\n\nfor filename, results in all_files_results.items():\n    print(f\"\\nResultados para el archivo: {filename}\")\n    results_df = pd.DataFrame(results)\n\n    comparisons_df = results_df.applymap(lambda x: x['Comparaciones']).mean()\n    time_df = results_df.applymap(lambda x: x['Tiempo']).mean()\n\n    print(\"\\nPromedio de Comparaciones por Algoritmo:\\n\", comparisons_df)\n    print(\"\\nPromedio de Tiempo por Algoritmo:\\n\", time_df)\n\n    # Comparaciones (escala logarítmica)\n    ax = comparisons_df.plot(kind='bar', title=f'Promedio de Comparaciones por Algoritmo - {filename}',\n                              color='skyblue', log=True)\n    plt.ylabel('Número de Comparaciones (escala logarítmica)')\n    plt.xticks(rotation=45)\n    plt.tight_layout()\n\n    for p in ax.patches:\n        ax.annotate(f'{p.get_height():.2f}', \n                    (p.get_x() + p.get_width() / 2., p.get_height()), \n                    ha='center', va='center', \n                    xytext=(0, 10), textcoords='offset points')\n    plt.show()\n\n    # Tiempos de ejecución\n    ax = time_df.plot(kind='bar', title=f'Promedio de Tiempo por Algoritmo - {filename}', color='lightgreen')\n    plt.ylabel('Tiempo (segundos)')\n    plt.xticks(rotation=45)\n    plt.tight_layout()\n\n    for p in ax.patches:\n        ax.annotate(f'{p.get_height():.4f}', \n                    (p.get_x() + p.get_width() / 2., p.get_height()), \n                    ha='center', va='center', \n                    xytext=(0, 10), textcoords='offset points')\n\n    ax.set_ylim(0, time_df.max() * 1.25)\n    plt.show()\n\n\n\nTras revisar las observaciones recibidas, se realizaron mejoras clave en la implementación y análisis de los algoritmos. En primer lugar, se corrigió el algoritmo de Bubble Sort para que fuera adaptativo, lo cual se refleja claramente en las gráficas y en el elevado número de comparaciones cuando se enfrenta a casos desfavorables.\nAsimismo, se optimizó Merge Sort para evitar el uso innecesario de memoria adicional, mejorando así su eficiencia práctica. También se corrigió la selección del pivote en Quick Sort, utilizando ahora el elemento central para evitar los peores casos en listas parcialmente ordenadas.\nEn cuanto a SkipList, se eliminó el uso del valor artificial -∞, garantizando que todas las comparaciones se realicen solo entre elementos reales, como exige el modelo de comparación.\nFinalmente, se ajustaron las escalas de las gráficas, empleando una escala logarítmica para las comparaciones y márgenes dinámicos para el tiempo, facilitando una interpretación visual más clara y justa.\nCon estas mejoras, el trabajo queda alineado con los lineamientos discutidos y refleja correctamente el comportamiento de cada algoritmo."
  },
  {
    "objectID": "project3.html#análisis-de-resultados",
    "href": "project3.html#análisis-de-resultados",
    "title": "Proyecto 3",
    "section": "3. Análisis de Resultados",
    "text": "3. Análisis de Resultados"
  },
  {
    "objectID": "project3.html#conclusiones",
    "href": "project3.html#conclusiones",
    "title": "Proyecto 3",
    "section": "",
    "text": "Al finalizar este análisis, puedo concluir que Heapsort fue, sin duda, el algoritmo más eficiente y consistente a lo largo de todos los escenarios evaluados. Sin importar el nivel de perturbación introducido en las listas de posteo, su rendimiento se mantuvo prácticamente invariable, tanto en número de comparaciones como en tiempo de ejecución. Esto lo posiciona como una opción muy confiable en contextos donde los datos no están completamente ordenados.\nPor el contrario, Bubble Sort demostró ser el menos adecuado para este tipo de condiciones, registrando cifras excesivas de comparaciones y tiempos considerablemente altos en todos los casos. Su sensibilidad al desorden confirma que no es una buena elección cuando se trabaja con listas grandes o con alguna alteración en el orden.\nMergesort y Quicksort ofrecieron un equilibrio bastante sólido, especialmente en cuanto a tiempos de ejecución. Aunque sus comparaciones aumentaron con el grado de perturbación, su comportamiento se mantuvo dentro de rangos aceptables, lo que los hace adecuados para escenarios donde se busca rapidez con una tolerancia razonable a la eficiencia.\nEn cuanto a Skiplist, observé un rendimiento intermedio. No fue el más rápido, pero tampoco presentó problemas extremos. Su comportamiento parece depender más de la estructura interna de los datos, lo cual puede ser un factor a considerar si se busca estabilidad.\nFinalmente, algo que considero clave es que la elección del algoritmo no debe basarse únicamente en su complejidad teórica. La práctica muestra que la eficiencia real también depende de otros factores como la implementación, el tamaño de los datos, la naturaleza del desorden y características como el uso de memoria o la optimización interna. Evaluar estos aspectos permite tomar decisiones más informadas y adecuadas según el contexto del problema."
  },
  {
    "objectID": "project3.html#referencias",
    "href": "project3.html#referencias",
    "title": "Proyecto 3",
    "section": "",
    "text": "Cormen, T. H., Leiserson, C. E., Rivest, R. L., & Stein, C. (2009). Introduction to Algorithms (3rd ed.). MIT Press.\nKnuth, D. E. (2011). The Art of Computer Programming, Volume 4A: Combinatorial Algorithms, Part 1. Addison-Wesley.\nRizvi, Q., Rai, H., & Jaiswal, R. (2024). Sorting Algorithms in Focus: A Critical Examination of Sorting Algorithm Performance.\nHuyen, C. (2022). Designing Machine Learning Systems. O’Reilly Media, Inc."
  },
  {
    "objectID": "project3.html#cambios-realizados",
    "href": "project3.html#cambios-realizados",
    "title": "Proyecto 3",
    "section": "",
    "text": "Con base en las observaciones realizadas durante la videoconferencia y en las notas complementarias, realicé una serie de ajustes importantes en el desarrollo de mi trabajo.\nPrimero, corregí la afirmación sobre el algoritmo Bubble Sort, aclarando que no es un algoritmo adaptativo. Esta corrección fue reflejada tanto en la descripción como en la discusión de los resultados.\nTambién ajusté las escalas de las gráficas generadas para cada algoritmo, con el objetivo de permitir una mejor visualización y comparación de sus comportamientos, especialmente en casos con tamaños de entrada moderados.\nEn cuanto a Merge Sort, revisé la implementación para evitar el uso innecesario de memoria adicional, ya que esto afectaba negativamente su eficiencia.\nPara Quick Sort, corregí la estrategia de selección del pivote, siguiendo las recomendaciones revisadas en clase, y documenté cómo esta decisión influye directamente en el rendimiento del algoritmo.\nPor último, eliminé el uso del número “ínfimo” en la implementación de Skip List, ya que comprendí que no era necesario dentro del modelo de comparación. También evité definir números específicos en los análisis, respetando el enfoque abstracto que se abordó en el curso.\nEstos cambios me ayudaron a alinear mejor mi trabajo con los principios teóricos del análisis algorítmico y a fortalecer mi comprensión de los conceptos trabajados."
  },
  {
    "objectID": "project1.html",
    "href": "project1.html",
    "title": "Proyecto 1",
    "section": "",
    "text": "En la era digital actual, el big data ha emergido como un recurso valioso para las organizaciones, permitiéndoles extraer, procesar y analizar datos significativos. El big data se caracteriza por sus cuatro V: volumen, velocidad, variedad y veracidad. El volumen se refiere a la gran cantidad de datos generados y almacenados; la velocidad, a la rapidez con la que se generan y procesan; la variedad, a los diferentes tipos de datos (estructurados y no estructurados); y la veracidad, a la calidad y precisión de dichos datos. La capacidad de manejar y analizar estos grandes volúmenes de información de manera eficiente es crucial para aprovechar al máximo el potencial del big data (Müller et al., 2016).\nEl análisis de big data permite a las organizaciones identificar patrones, predecir tendencias y optimizar procesos, lo que puede resultar en mejoras significativas en la eficiencia operativa y en la toma de decisiones estratégicas. Sin embargo, la manipulación de grandes volúmenes de datos presenta desafíos importantes en términos de infraestructura, almacenamiento, procesamiento y análisis. Los algoritmos eficientes son fundamentales para enfrentar estos desafíos y garantizar que los sistemas de big data funcionen de manera óptima (Manyika et al., 2011).\nEste reporte se enfoca en comparar diferentes órdenes de crecimiento mediante simulaciones en un entorno de Jupyter. Se analizarán los siguientes casos: \\(O(1)\\) vs \\(O(\\log n)\\), \\(O(n)\\) vs \\(O(n \\log n)\\), \\(O(n^2)\\) vs \\(O(n^3)\\), \\(O(a^n)\\) vs \\(O(n!)\\) y \\(O(n!)\\) vs \\(O(n^n)\\). Para cada comparación, se seleccionarán rangos adecuados de \\(n\\) que permitan visualizar claramente las diferencias entre estos órdenes. Se generarán gráficas para cada caso y se discutirán las observaciones correspondientes. Además, se incluirá una tabla con tiempos de ejecución simulados para algoritmos ficticios asociados a los órdenes de crecimiento mencionados, utilizando distintos tamaños de entrada \\(n\\). Este análisis proporciona una visión clara de cómo los diferentes órdenes de crecimiento afectan el rendimiento y la eficiencia de los algoritmos.\n\n\n\n\n\nimport numpy as np\nimport pandas as pd\nimport math\nimport matplotlib.pyplot as plt\n\n\n\ndef constant(n): return 1\ndef logarithmic(n): return np.log(n)\ndef linear(n): return n\ndef linear_logarithmic(n): return n * np.log(n)\ndef quadratic(n): return n**2\ndef cubic(n): return n**3\ndef exponential(n, a=2): return a**n\ndef factorial(n): return math.factorial(n)\ndef double_exponential(n): return n**n\n\n\n\ndef plot_comparison(funcs, labels, title, x_range):\n    plt.figure(figsize=(7, 5))\n    for func, label in zip(funcs, labels):\n        plt.plot(x_range, [func(x) / 1e9 for x in x_range], label=label)\n    plt.xlabel('n')\n    plt.ylabel('Tiempo (s)')\n    plt.title(title)\n    plt.legend()\n    plt.yscale('log')\n    plt.xscale('log')\n    plt.grid(True)\n    plt.show()\n\n\n\nx_range_small = np.arange(1, 10)\nx_range_medium = np.arange(1, 100)\n\n# Comparación 1: O(1) vs O(log n)\nplot_comparison([constant, logarithmic], ['O(1)', 'O(log n)'], 'Figura 1 - Comparación de O(1) con O(log n)', x_range_small)\n\n# Comparación 2: O(n) vs O(n log n)\nplot_comparison([linear, linear_logarithmic], ['O(n)', 'O(n log n)'], 'Figura 2 - Comparación de O(n) con O(n log n)', x_range_medium)\n\n# Comparación 3: O(n^2) vs O(n^3)\nplot_comparison([quadratic, cubic], ['O(n^2)', 'O(n^3)'], 'Figura 3 - Comparación de O(n^2) con O(n^3)', x_range_medium)\n\n# Comparación 4: O(2^n) vs O(n!)\nplot_comparison([lambda n: exponential(n, 2), factorial], ['O(2^n)', 'O(n!)'], 'Figura 4 - Comparación de O(2^n) con O(n!)', x_range_small)\n\n# Comparación 5: O(n!) vs O(n^n)\nplot_comparison([factorial, double_exponential], ['O(n!)', 'O(n^n)'], 'Figura 5 - Comparación de O(n!) con O(n^n)', x_range_small)\n\n\n\nNota: Se creó una tabla con tiempos de ejecución simulados para algoritmos ficticios con los órdenes de crecimiento mencionados.\n# Tamaños de entrada\nn_values = [100, 1000, 10000, 100000]\n\n# Funciones de costo\ncost_functions = {\n    'O(1)': constant,\n    'O(log n)': logarithmic,\n    'O(n)': linear,\n    'O(n log n)': linear_logarithmic,\n    'O(n^2)': quadratic,\n    'O(n^3)': cubic,\n    'O(2^n)': lambda n: exponential(n, 2),\n    'O(n!)': factorial,\n    'O(n^n)': double_exponential\n}\n\n# Elaboración de la tabla\nresults = []\nfor n in n_values:\n    row = {'n': n}\n    for label, func in cost_functions.items():\n        try:\n            row[label] = func(n) / 1e9  # Conversión a segundos\n        except OverflowError:\n            row[label] = 'Overflow'\n    results.append(row)\n\nimport pandas as pd\ndf = pd.DataFrame(results)\ndf.set_index('n', inplace=True)\ndf\n\n\n\n\n\n\n\n\n\nEl gráfico muestra que la función constante \\(O(1)\\) permanece constante independientemente del tamaño de la entrada, mientras que \\(O(\\log n)\\) crece lentamente conforme \\(n\\) aumenta. Por lo tanto, un algoritmo con complejidad constante \\(O(1)\\) es más eficiente en términos de tiempo de ejecución en comparación con uno con complejidad logarítmica \\(O(\\log n)\\).\n\n\n\n\n\n\nLa función \\(O(n)\\) crece linealmente con el tamaño de entrada \\(n\\), mientras que la función \\(O(n \\log n)\\) crece más rápido que \\(O(n)\\), pero sigue siendo práctica para valores moderados de \\(n\\). Por lo tanto, un algoritmo con complejidad lineal \\(O(n)\\) es más eficiente en términos de tiempo de ejecución en comparación con uno con complejidad lineal-logarítmica \\(O(n \\log n)\\).\n\n\n\n\n\n\nLa función cuadrática \\(O(n^2)\\) crece más rápidamente que la lineal, pero la función cúbica \\(O(n^3)\\) lo hace aún con mayor rapidez. Por lo tanto, un algoritmo con complejidad cuadrática \\(O(n^2)\\) es más eficiente en términos de tiempo de ejecución en comparación con uno con complejidad cúbica \\(O(n^3)\\).\n\n\n\n\n\n\nEl gráfico muestra que la complejidad factorial \\(O(n!)\\) resulta en tiempos de ejecución mucho más altos que \\(O(2^n)\\) a medida que el tamaño de la entrada \\(n\\) aumenta. La complejidad factorial \\(O(n!)\\) crece considerablemente más rápido que la complejidad exponencial \\(O(2^n)\\), lo que hace que los algoritmos con esta complejidad sean prácticamente inutilizables para valores grandes de \\(n\\).\n\n\n\n\n\n\nEl gráfico muestra cómo la función factorial \\(O(n!)\\) crece muy rápidamente, pero la función doble exponencial \\(O(n^n)\\) crece aún más rápido a medida que aumenta el tamaño de la entrada \\(n\\). Por lo tanto, los algoritmos con complejidad factorial y doble exponencial son prácticamente inutilizables para valores grandes de \\(n\\).\n\n\n\n\n\n\n\n\nA continuación se presenta una tabla comparativa de los tiempos simulados para diferentes órdenes de crecimiento, utilizando distintos tamaños de entrada \\(n\\). Los resultados se expresan en segundos. En algunos casos, se muestra “Overflow” cuando el valor resultante excede los límites de representación.\nTabla 1. Tiempos simulados para diferentes órdenes de crecimiento de O(1), O(log n), O(n), O(n log n), O(n²)\n\n\n\n\n\n\n\n\n\n\n\nn\nO(1)\nO(log n)\nO(n)\nO(n log n)\nO(n²)\n\n\n\n\n100\n1.00e-09\n4.61e-09\n1.00e-07\n4.61e-07\n1.00e-05\n\n\n1000\n1.00e-09\n6.91e-09\n1.00e-06\n6.91e-06\n1.00e-03\n\n\n10000\n1.00e-09\n9.21e-09\n1.00e-05\n9.21e-05\n1.00e-01\n\n\n100000\n1.00e-09\n1.15e-08\n1.00e-04\n1.15e-03\n1.00e+01\n\n\n\nTabla 2. Tiempos simulados para diferentes órdenes de crecimiento de O(n³), O(2ⁿ)\n\n\n\nn\nO(n³)\nO(2ⁿ)\n\n\n\n\n100\n1.00e-03\n1.27e+21\n\n\n1000\n1.00e+00\nOverflow\n\n\n10000\n1.00e+03\nOverflow\n\n\n100000\n1.00e+06\nOverflow\n\n\n\nTabla 3. Tiempos simulados para diferentes órdenes de crecimiento de O(n!), O(nⁿ)\n\n\n\nn\nO(n!)\nO(nⁿ)\n\n\n\n\n100\nOverflow\nOverflow\n\n\n1000\nOverflow\nOverflow\n\n\n10000\nOverflow\nOverflow\n\n\n100000\nOverflow\nOverflow\n\n\n\n\nNota: Algunos valores como O(2ⁿ), O(n!) y O(nⁿ) para valores grandes de \\(n\\) generan cantidades inmanejables por el sistema, resultando en Overflow. Esto refleja la inviabilidad práctica de algoritmos con estas complejidades cuando se trabaja con grandes volúmenes de datos.\n\nComo se muestra en la tabla, el tiempo de ejecución de O(1) no depende del tamaño de n. Este mantiene un mismo valor, lo que indica que su tiempo de ejecución es constante. Por otro lado, los valores de O(log n) aumentan lentamente a medida que n crece, lo cual es consistente con su comportamiento logarítmico. En el caso de O(n), los valores aumentan de forma lineal conforme n se incrementa. Los valores de O(n log n) aumentan más rápido que los de O(n), pero no tan aceleradamente como los de O(n²). Por su parte, O(n²) crece rápidamente a medida que n aumenta, y O(n³) lo hace de forma aún más acelerada, incluso con valores pequeños de entrada.\nDurante la ejecución del código, algunos resultados aparecen como “Overflow”. Esto se debe a que las funciones O(2ⁿ), O(n!) y O(nⁿ) para valores grandes de n (como 10,000 o 100,000) generan números extremadamente grandes, lo que excede la capacidad de representación y manejo numérico en Python.\n\n\n\n\nLa manipulación de grandes volúmenes de información presenta importantes desafíos debido a los costos de cómputo. Estos costos se relacionan con el gasto monetario, los recursos computacionales requeridos, el tiempo de procesamiento, el uso de memoria, el almacenamiento y el consumo energético. A continuación se presentan algunas de las implicaciones más relevantes:\n\nInfraestructura: Manipular grandes volúmenes de datos requiere una infraestructura robusta, que incluye servidores potentes, almacenamiento masivo y redes de alta velocidad. Esto representa costos elevados en hardware, mantenimiento y consumo energético (Armbrust et al., 2010).\nEnergía: Los centros de datos que procesan grandes cantidades de información consumen enormes cantidades de energía, lo cual incrementa los costos operativos y contribuye al impacto ambiental debido a las emisiones de carbono (Baliga et al., 2011).\nAlmacenamiento: El almacenamiento de grandes volúmenes de datos requiere soluciones escalables y eficientes. Este aspecto representa un costo considerable, ya que también debe garantizarse la recuperación oportuna de la información.\nProcesamiento: Procesar datos en tiempo real o en tiempo cercano al real implica el uso de algoritmos eficientes y de recursos de cómputo significativos. Estos costos pueden incrementarse especialmente al utilizar tecnologías como el aprendizaje automático y la inteligencia artificial (Wang et al., 2015).\n\nEn conjunto, la gestión de grandes volúmenes de información conlleva costos considerables en términos de infraestructura, energía, almacenamiento, procesamiento, seguridad y mantenimiento. Estos aspectos deben ser cuidadosamente planeados y gestionados para asegurar soluciones de big data eficientes y sostenibles.\n\n\n\n\nEn las simulaciones realizadas se observó que los órdenes de crecimiento más bajos, como O(1), O(log n) y O(n), resultan altamente eficientes. Son ideales para tareas que demandan rendimiento óptimo y tiempos de respuesta reducidos.\nPor otro lado, los órdenes de crecimiento O(n log n) y O(n²) demostraron ser prácticas y ampliamente aplicables, al ofrecer un buen equilibrio entre eficiencia y capacidad de procesamiento.\nEn contraste, las funciones con órdenes de crecimiento elevados como O(2ⁿ), O(n!) y O(nⁿ) resultaron ineficientes, generando tiempos de ejecución excesivos o incluso errores de memoria.\nLa tabla de tiempos de ejecución simulados confirma cómo el crecimiento del orden afecta directamente el rendimiento de los algoritmos. A mayor orden, mayor consumo de recursos y tiempo; a menor orden, mayor eficiencia en la ejecución.\n\n\n\n\nMüller, V. C., Schal, J. M., Meyer-Lindenberg, A. (2016). Machine Learning for Brain Imaging. Cambridge University Press.\nManyika, J., Chui, M., Brown, B., Bughin, J., Dobbs, R., Roxburgh, C., & Byers, A. H. (2011). Big data: The next frontier for innovation, competition, and productivity. McKinsey Global Institute. https://www.mckinsey.com/business-functions/mckinsey-analytics/our-insights/big-data-the-next-frontier-for-innovation\nArmbrust, M., Fox, A., Griffith, R., Joseph, A. D., Katz, R. H., Konwinski, A., et al. (2010). A View of Cloud Computing. Communications of the ACM, 53(4), 50–58. https://doi.org/10.1145/1721654.1721672\nBaliga, J., Ayre, R., Hinton, K., & Tucker, R. S. (2011). Energy-Efficient Telecommunications.\nWang, L., Chen, Y., & Liu, Q. (2015). Big Data Processing: A Survey. Springer.\n\n\n\n\n\n\nLos siguientes ajustes fueron aplicados al presente documento en atención a la retroalimentación recibida por parte del docente. Se atendieron los puntos señalados con el fin de mejorar la calidad formal y académica del reporte:\n\nSe uniformó el formato del documento, manteniendo una estructura clara y coherente entre secciones.\nSe corrigieron errores ortográficos y gramaticales, incluyendo el uso adecuado de comas y acentos.\nLas expresiones matemáticas fueron reescritas utilizando la notación correcta en formato de ecuación (LaTeX), como es el caso de \\(O(1)\\), \\(O(\\log n)\\), \\(O(n^2)\\), entre otras.\nSe estandarizaron los encabezados de cada sección para facilitar la lectura y navegación del documento.\n\nEstas modificaciones aseguran que el trabajo cumpla con los criterios de presentación solicitados y refleje un esfuerzo riguroso en la construcción y exposición del contenido."
  },
  {
    "objectID": "project1.html#introducción",
    "href": "project1.html#introducción",
    "title": "Proyecto 1",
    "section": "",
    "text": "En la era digital actual, el big data ha emergido como un recurso valioso para las organizaciones, permitiéndoles extraer, procesar y analizar datos significativos. El big data se caracteriza por sus cuatro V: volumen, velocidad, variedad y veracidad. El volumen se refiere a la gran cantidad de datos generados y almacenados; la velocidad, a la rapidez con la que se generan y procesan; la variedad, a los diferentes tipos de datos (estructurados y no estructurados); y la veracidad, a la calidad y precisión de dichos datos. La capacidad de manejar y analizar estos grandes volúmenes de información de manera eficiente es crucial para aprovechar al máximo el potencial del big data (Müller et al., 2016).\nEl análisis de big data permite a las organizaciones identificar patrones, predecir tendencias y optimizar procesos, lo que puede resultar en mejoras significativas en la eficiencia operativa y en la toma de decisiones estratégicas. Sin embargo, la manipulación de grandes volúmenes de datos presenta desafíos importantes en términos de infraestructura, almacenamiento, procesamiento y análisis. Los algoritmos eficientes son fundamentales para enfrentar estos desafíos y garantizar que los sistemas de big data funcionen de manera óptima (Manyika et al., 2011).\nEste reporte se enfoca en comparar diferentes órdenes de crecimiento mediante simulaciones en un entorno de Jupyter. Se analizarán los siguientes casos: \\(O(1)\\) vs \\(O(\\log n)\\), \\(O(n)\\) vs \\(O(n \\log n)\\), \\(O(n^2)\\) vs \\(O(n^3)\\), \\(O(a^n)\\) vs \\(O(n!)\\) y \\(O(n!)\\) vs \\(O(n^n)\\). Para cada comparación, se seleccionarán rangos adecuados de \\(n\\) que permitan visualizar claramente las diferencias entre estos órdenes. Se generarán gráficas para cada caso y se discutirán las observaciones correspondientes. Además, se incluirá una tabla con tiempos de ejecución simulados para algoritmos ficticios asociados a los órdenes de crecimiento mencionados, utilizando distintos tamaños de entrada \\(n\\). Este análisis proporciona una visión clara de cómo los diferentes órdenes de crecimiento afectan el rendimiento y la eficiencia de los algoritmos."
  },
  {
    "objectID": "project1.html#desarrollo",
    "href": "project1.html#desarrollo",
    "title": "Proyecto 1",
    "section": "",
    "text": "import numpy as np\nimport pandas as pd\nimport math\nimport matplotlib.pyplot as plt\n\n\n\ndef constant(n): return 1\ndef logarithmic(n): return np.log(n)\ndef linear(n): return n\ndef linear_logarithmic(n): return n * np.log(n)\ndef quadratic(n): return n**2\ndef cubic(n): return n**3\ndef exponential(n, a=2): return a**n\ndef factorial(n): return math.factorial(n)\ndef double_exponential(n): return n**n\n\n\n\ndef plot_comparison(funcs, labels, title, x_range):\n    plt.figure(figsize=(7, 5))\n    for func, label in zip(funcs, labels):\n        plt.plot(x_range, [func(x) / 1e9 for x in x_range], label=label)\n    plt.xlabel('n')\n    plt.ylabel('Tiempo (s)')\n    plt.title(title)\n    plt.legend()\n    plt.yscale('log')\n    plt.xscale('log')\n    plt.grid(True)\n    plt.show()\n\n\n\nx_range_small = np.arange(1, 10)\nx_range_medium = np.arange(1, 100)\n\n# Comparación 1: O(1) vs O(log n)\nplot_comparison([constant, logarithmic], ['O(1)', 'O(log n)'], 'Figura 1 - Comparación de O(1) con O(log n)', x_range_small)\n\n# Comparación 2: O(n) vs O(n log n)\nplot_comparison([linear, linear_logarithmic], ['O(n)', 'O(n log n)'], 'Figura 2 - Comparación de O(n) con O(n log n)', x_range_medium)\n\n# Comparación 3: O(n^2) vs O(n^3)\nplot_comparison([quadratic, cubic], ['O(n^2)', 'O(n^3)'], 'Figura 3 - Comparación de O(n^2) con O(n^3)', x_range_medium)\n\n# Comparación 4: O(2^n) vs O(n!)\nplot_comparison([lambda n: exponential(n, 2), factorial], ['O(2^n)', 'O(n!)'], 'Figura 4 - Comparación de O(2^n) con O(n!)', x_range_small)\n\n# Comparación 5: O(n!) vs O(n^n)\nplot_comparison([factorial, double_exponential], ['O(n!)', 'O(n^n)'], 'Figura 5 - Comparación de O(n!) con O(n^n)', x_range_small)\n\n\n\nNota: Se creó una tabla con tiempos de ejecución simulados para algoritmos ficticios con los órdenes de crecimiento mencionados.\n# Tamaños de entrada\nn_values = [100, 1000, 10000, 100000]\n\n# Funciones de costo\ncost_functions = {\n    'O(1)': constant,\n    'O(log n)': logarithmic,\n    'O(n)': linear,\n    'O(n log n)': linear_logarithmic,\n    'O(n^2)': quadratic,\n    'O(n^3)': cubic,\n    'O(2^n)': lambda n: exponential(n, 2),\n    'O(n!)': factorial,\n    'O(n^n)': double_exponential\n}\n\n# Elaboración de la tabla\nresults = []\nfor n in n_values:\n    row = {'n': n}\n    for label, func in cost_functions.items():\n        try:\n            row[label] = func(n) / 1e9  # Conversión a segundos\n        except OverflowError:\n            row[label] = 'Overflow'\n    results.append(row)\n\nimport pandas as pd\ndf = pd.DataFrame(results)\ndf.set_index('n', inplace=True)\ndf"
  },
  {
    "objectID": "project1.html#análisis-de-resultados",
    "href": "project1.html#análisis-de-resultados",
    "title": "Proyecto 1",
    "section": "",
    "text": "El gráfico muestra que la función constante \\(O(1)\\) permanece constante independientemente del tamaño de la entrada, mientras que \\(O(\\log n)\\) crece lentamente conforme \\(n\\) aumenta. Por lo tanto, un algoritmo con complejidad constante \\(O(1)\\) es más eficiente en términos de tiempo de ejecución en comparación con uno con complejidad logarítmica \\(O(\\log n)\\).\n\n\n\n\n\n\nLa función \\(O(n)\\) crece linealmente con el tamaño de entrada \\(n\\), mientras que la función \\(O(n \\log n)\\) crece más rápido que \\(O(n)\\), pero sigue siendo práctica para valores moderados de \\(n\\). Por lo tanto, un algoritmo con complejidad lineal \\(O(n)\\) es más eficiente en términos de tiempo de ejecución en comparación con uno con complejidad lineal-logarítmica \\(O(n \\log n)\\).\n\n\n\n\n\n\nLa función cuadrática \\(O(n^2)\\) crece más rápidamente que la lineal, pero la función cúbica \\(O(n^3)\\) lo hace aún con mayor rapidez. Por lo tanto, un algoritmo con complejidad cuadrática \\(O(n^2)\\) es más eficiente en términos de tiempo de ejecución en comparación con uno con complejidad cúbica \\(O(n^3)\\).\n\n\n\n\n\n\nEl gráfico muestra que la complejidad factorial \\(O(n!)\\) resulta en tiempos de ejecución mucho más altos que \\(O(2^n)\\) a medida que el tamaño de la entrada \\(n\\) aumenta. La complejidad factorial \\(O(n!)\\) crece considerablemente más rápido que la complejidad exponencial \\(O(2^n)\\), lo que hace que los algoritmos con esta complejidad sean prácticamente inutilizables para valores grandes de \\(n\\).\n\n\n\n\n\n\nEl gráfico muestra cómo la función factorial \\(O(n!)\\) crece muy rápidamente, pero la función doble exponencial \\(O(n^n)\\) crece aún más rápido a medida que aumenta el tamaño de la entrada \\(n\\). Por lo tanto, los algoritmos con complejidad factorial y doble exponencial son prácticamente inutilizables para valores grandes de \\(n\\).\n\n\n\n\n\n\n\n\nA continuación se presenta una tabla comparativa de los tiempos simulados para diferentes órdenes de crecimiento, utilizando distintos tamaños de entrada \\(n\\). Los resultados se expresan en segundos. En algunos casos, se muestra “Overflow” cuando el valor resultante excede los límites de representación.\nTabla 1. Tiempos simulados para diferentes órdenes de crecimiento de O(1), O(log n), O(n), O(n log n), O(n²)\n\n\n\n\n\n\n\n\n\n\n\nn\nO(1)\nO(log n)\nO(n)\nO(n log n)\nO(n²)\n\n\n\n\n100\n1.00e-09\n4.61e-09\n1.00e-07\n4.61e-07\n1.00e-05\n\n\n1000\n1.00e-09\n6.91e-09\n1.00e-06\n6.91e-06\n1.00e-03\n\n\n10000\n1.00e-09\n9.21e-09\n1.00e-05\n9.21e-05\n1.00e-01\n\n\n100000\n1.00e-09\n1.15e-08\n1.00e-04\n1.15e-03\n1.00e+01\n\n\n\nTabla 2. Tiempos simulados para diferentes órdenes de crecimiento de O(n³), O(2ⁿ)\n\n\n\nn\nO(n³)\nO(2ⁿ)\n\n\n\n\n100\n1.00e-03\n1.27e+21\n\n\n1000\n1.00e+00\nOverflow\n\n\n10000\n1.00e+03\nOverflow\n\n\n100000\n1.00e+06\nOverflow\n\n\n\nTabla 3. Tiempos simulados para diferentes órdenes de crecimiento de O(n!), O(nⁿ)\n\n\n\nn\nO(n!)\nO(nⁿ)\n\n\n\n\n100\nOverflow\nOverflow\n\n\n1000\nOverflow\nOverflow\n\n\n10000\nOverflow\nOverflow\n\n\n100000\nOverflow\nOverflow\n\n\n\n\nNota: Algunos valores como O(2ⁿ), O(n!) y O(nⁿ) para valores grandes de \\(n\\) generan cantidades inmanejables por el sistema, resultando en Overflow. Esto refleja la inviabilidad práctica de algoritmos con estas complejidades cuando se trabaja con grandes volúmenes de datos.\n\nComo se muestra en la tabla, el tiempo de ejecución de O(1) no depende del tamaño de n. Este mantiene un mismo valor, lo que indica que su tiempo de ejecución es constante. Por otro lado, los valores de O(log n) aumentan lentamente a medida que n crece, lo cual es consistente con su comportamiento logarítmico. En el caso de O(n), los valores aumentan de forma lineal conforme n se incrementa. Los valores de O(n log n) aumentan más rápido que los de O(n), pero no tan aceleradamente como los de O(n²). Por su parte, O(n²) crece rápidamente a medida que n aumenta, y O(n³) lo hace de forma aún más acelerada, incluso con valores pequeños de entrada.\nDurante la ejecución del código, algunos resultados aparecen como “Overflow”. Esto se debe a que las funciones O(2ⁿ), O(n!) y O(nⁿ) para valores grandes de n (como 10,000 o 100,000) generan números extremadamente grandes, lo que excede la capacidad de representación y manejo numérico en Python.\n\n\n\n\nLa manipulación de grandes volúmenes de información presenta importantes desafíos debido a los costos de cómputo. Estos costos se relacionan con el gasto monetario, los recursos computacionales requeridos, el tiempo de procesamiento, el uso de memoria, el almacenamiento y el consumo energético. A continuación se presentan algunas de las implicaciones más relevantes:\n\nInfraestructura: Manipular grandes volúmenes de datos requiere una infraestructura robusta, que incluye servidores potentes, almacenamiento masivo y redes de alta velocidad. Esto representa costos elevados en hardware, mantenimiento y consumo energético (Armbrust et al., 2010).\nEnergía: Los centros de datos que procesan grandes cantidades de información consumen enormes cantidades de energía, lo cual incrementa los costos operativos y contribuye al impacto ambiental debido a las emisiones de carbono (Baliga et al., 2011).\nAlmacenamiento: El almacenamiento de grandes volúmenes de datos requiere soluciones escalables y eficientes. Este aspecto representa un costo considerable, ya que también debe garantizarse la recuperación oportuna de la información.\nProcesamiento: Procesar datos en tiempo real o en tiempo cercano al real implica el uso de algoritmos eficientes y de recursos de cómputo significativos. Estos costos pueden incrementarse especialmente al utilizar tecnologías como el aprendizaje automático y la inteligencia artificial (Wang et al., 2015).\n\nEn conjunto, la gestión de grandes volúmenes de información conlleva costos considerables en términos de infraestructura, energía, almacenamiento, procesamiento, seguridad y mantenimiento. Estos aspectos deben ser cuidadosamente planeados y gestionados para asegurar soluciones de big data eficientes y sostenibles."
  },
  {
    "objectID": "project1.html#conclusiones",
    "href": "project1.html#conclusiones",
    "title": "Proyecto 1",
    "section": "",
    "text": "En las simulaciones realizadas se observó que los órdenes de crecimiento más bajos, como O(1), O(log n) y O(n), resultan altamente eficientes. Son ideales para tareas que demandan rendimiento óptimo y tiempos de respuesta reducidos.\nPor otro lado, los órdenes de crecimiento O(n log n) y O(n²) demostraron ser prácticas y ampliamente aplicables, al ofrecer un buen equilibrio entre eficiencia y capacidad de procesamiento.\nEn contraste, las funciones con órdenes de crecimiento elevados como O(2ⁿ), O(n!) y O(nⁿ) resultaron ineficientes, generando tiempos de ejecución excesivos o incluso errores de memoria.\nLa tabla de tiempos de ejecución simulados confirma cómo el crecimiento del orden afecta directamente el rendimiento de los algoritmos. A mayor orden, mayor consumo de recursos y tiempo; a menor orden, mayor eficiencia en la ejecución."
  },
  {
    "objectID": "project1.html#referencias",
    "href": "project1.html#referencias",
    "title": "Proyecto 1",
    "section": "",
    "text": "Müller, V. C., Schal, J. M., Meyer-Lindenberg, A. (2016). Machine Learning for Brain Imaging. Cambridge University Press.\nManyika, J., Chui, M., Brown, B., Bughin, J., Dobbs, R., Roxburgh, C., & Byers, A. H. (2011). Big data: The next frontier for innovation, competition, and productivity. McKinsey Global Institute. https://www.mckinsey.com/business-functions/mckinsey-analytics/our-insights/big-data-the-next-frontier-for-innovation\nArmbrust, M., Fox, A., Griffith, R., Joseph, A. D., Katz, R. H., Konwinski, A., et al. (2010). A View of Cloud Computing. Communications of the ACM, 53(4), 50–58. https://doi.org/10.1145/1721654.1721672\nBaliga, J., Ayre, R., Hinton, K., & Tucker, R. S. (2011). Energy-Efficient Telecommunications.\nWang, L., Chen, Y., & Liu, Q. (2015). Big Data Processing: A Survey. Springer."
  },
  {
    "objectID": "project1.html#cambios-realizados",
    "href": "project1.html#cambios-realizados",
    "title": "Proyecto 1",
    "section": "",
    "text": "Los siguientes ajustes fueron aplicados al presente documento en atención a la retroalimentación recibida por parte del docente. Se atendieron los puntos señalados con el fin de mejorar la calidad formal y académica del reporte:\n\nSe uniformó el formato del documento, manteniendo una estructura clara y coherente entre secciones.\nSe corrigieron errores ortográficos y gramaticales, incluyendo el uso adecuado de comas y acentos.\nLas expresiones matemáticas fueron reescritas utilizando la notación correcta en formato de ecuación (LaTeX), como es el caso de \\(O(1)\\), \\(O(\\log n)\\), \\(O(n^2)\\), entre otras.\nSe estandarizaron los encabezados de cada sección para facilitar la lectura y navegación del documento.\n\nEstas modificaciones aseguran que el trabajo cumpla con los criterios de presentación solicitados y refleje un esfuerzo riguroso en la construcción y exposición del contenido."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Sobre Mí",
    "section": "",
    "text": "📍 Aguascalientes, México\n✉️ juan2javm@gmail.com / jvelasquez1800@alumno.ipn.mx\n📱 +52 322 353 4081\n🔗 GitHub: juan21javm\n🔗 LinkedIn: antonio-martínez-776788179\n\n\n\nMe considero una persona profundamente comprometida con el crecimiento intelectual, la lectura reflexiva y el bienestar de mi entorno cercano. Disfruto de los momentos al aire libre, donde encuentro inspiración en la naturaleza y la tranquilidad necesaria para mantener un equilibrio emocional. Valoro profundamente el tiempo con mi familia, ya que es el motor que impulsa mi disciplina, responsabilidad y deseo de superación constante. Asimismo, tengo una gran pasión por la programación y un genuino entusiasmo por la ciencia de datos, áreas que me permiten combinar creatividad, lógica y análisis riguroso para resolver problemas del mundo real. Estos principios personales complementan mi formación académica y profesional, motivándome a mantener siempre una actitud proactiva, ética y humana.\n\n\n\n\nSupervisor — INE (2023–2025), Loreto, Zacatecas\n- Supervisión de actividades diarias para cumplimiento de objetivos.\n- Capacitación del equipo y mejora de rendimiento.\n- Evaluación de desempeño y optimización de procesos.\nMaestro Asistente — IPN (2022–2023), Zacatecas\n- Evaluación del progreso estudiantil.\n- Planeación conjunta de actividades educativas.\n- Creación y adaptación de materiales educativos.\n\n\n\n\nProyecto A — Estandarización de proceso a nivel laboratorio (2021–2023)\n- Herramientas: MATLAB, HACCP Software, Latex\n- Liderazgo de investigación para producción de yogurt con uvas a nivel laboratorio.\n- Reconocimiento al mejor promedio del programa académico (IPN 2017–2022).\n- Primer lugar en Simposium Agroalimentario y Ciclo de Conferencias CUALTIA 2023.\n\n\n\n\n\nLenguajes: Python, JAVA, HTML, ASPEN\n\nHerramientas Matemáticas: MATLAB, R, SPSS, PLC\n\nOfimática: Office, LaTeX\n\n\n\n\n\n\nIdiomas: Español (nativo), Inglés (B1)\n\nIntereses: Lectura, búsqueda, planeación"
  },
  {
    "objectID": "about.html#perfil-personal",
    "href": "about.html#perfil-personal",
    "title": "Sobre Mí",
    "section": "",
    "text": "Me considero una persona profundamente comprometida con el crecimiento intelectual, la lectura reflexiva y el bienestar de mi entorno cercano. Disfruto de los momentos al aire libre, donde encuentro inspiración en la naturaleza y la tranquilidad necesaria para mantener un equilibrio emocional. Valoro profundamente el tiempo con mi familia, ya que es el motor que impulsa mi disciplina, responsabilidad y deseo de superación constante. Asimismo, tengo una gran pasión por la programación y un genuino entusiasmo por la ciencia de datos, áreas que me permiten combinar creatividad, lógica y análisis riguroso para resolver problemas del mundo real. Estos principios personales complementan mi formación académica y profesional, motivándome a mantener siempre una actitud proactiva, ética y humana."
  },
  {
    "objectID": "about.html#experiencia",
    "href": "about.html#experiencia",
    "title": "Sobre Mí",
    "section": "",
    "text": "Supervisor — INE (2023–2025), Loreto, Zacatecas\n- Supervisión de actividades diarias para cumplimiento de objetivos.\n- Capacitación del equipo y mejora de rendimiento.\n- Evaluación de desempeño y optimización de procesos.\nMaestro Asistente — IPN (2022–2023), Zacatecas\n- Evaluación del progreso estudiantil.\n- Planeación conjunta de actividades educativas.\n- Creación y adaptación de materiales educativos."
  },
  {
    "objectID": "about.html#proyectos-conferencias-y-reconocimientos",
    "href": "about.html#proyectos-conferencias-y-reconocimientos",
    "title": "Sobre Mí",
    "section": "",
    "text": "Proyecto A — Estandarización de proceso a nivel laboratorio (2021–2023)\n- Herramientas: MATLAB, HACCP Software, Latex\n- Liderazgo de investigación para producción de yogurt con uvas a nivel laboratorio.\n- Reconocimiento al mejor promedio del programa académico (IPN 2017–2022).\n- Primer lugar en Simposium Agroalimentario y Ciclo de Conferencias CUALTIA 2023."
  },
  {
    "objectID": "about.html#habilidades",
    "href": "about.html#habilidades",
    "title": "Sobre Mí",
    "section": "",
    "text": "Lenguajes: Python, JAVA, HTML, ASPEN\n\nHerramientas Matemáticas: MATLAB, R, SPSS, PLC\n\nOfimática: Office, LaTeX"
  },
  {
    "objectID": "about.html#información-adicional",
    "href": "about.html#información-adicional",
    "title": "Sobre Mí",
    "section": "",
    "text": "Idiomas: Español (nativo), Inglés (B1)\n\nIntereses: Lectura, búsqueda, planeación"
  },
  {
    "objectID": "index.html#presentación-del-proyecto",
    "href": "index.html#presentación-del-proyecto",
    "title": "Centro de Investigación e Innovación en Tecnologías de la Información y Comunicación",
    "section": "Presentación del proyecto",
    "text": "Presentación del proyecto\nEn esta página se encuentran reflejados los cinco reportes desarrollados a lo largo de la asignatura de Análisis de Algoritmos. Cada uno de ellos ha sido debidamente documentado, estructurado y trasladado al formato Quarto con el propósito de comunicar al público el trabajo realizado y los análisis efectuados durante mi estancia en la materia. Estos reportes representan el proceso de aprendizaje y aplicación práctica de los conceptos clave abordados en cada unidad, desde la introducción al análisis algorítmico hasta los algoritmos de intersección de conjuntos, permitiendo evidenciar el desarrollo de competencias técnicas y analíticas fundamentales en el área.\nEsta documentación ha sido preparada para su publicación en un repositorio de GitHub con el objetivo de compartir de forma abierta los contenidos desarrollados, promover el acceso al conocimiento, y facilitar su consulta por parte de docentes, estudiantes y profesionales interesados en el análisis de algoritmos.\nA lo largo del curso se desarrollaron cinco tareas escritas que reflejan los temas fundamentales abordados en cada unidad:\n\nUnidad 1: Introducción al análisis de algoritmos\nSe realizó el reporte 1A. Reporte escrito. Experimentos y análisis, en el que se exploraron conceptos básicos sobre la eficiencia algorítmica y órdenes de crecimiento.\nUnidad 2: Estructuras de datos\nSe trabajó el reporte 2A. Reporte escrito. Experimentos y análisis de estructuras de datos, enfocado en el comportamiento, manipulación y análisis de distintas estructuras como listas, pilas, colas y árboles.\nUnidad 3: Algoritmos de ordenamiento por comparación\nSe elaboró el 3A. Reporte escrito. Experimentos y análisis de algoritmos de ordenamiento, donde se evaluaron métodos como burbuja, inserción, selección, quicksort y mergesort.\nUnidad 4: Algoritmos de búsqueda por comparación\nSe desarrolló el reporte 4A. Reporte escrito. Experimentos y análisis de algoritmos de búsqueda por comparación, abordando técnicas como la búsqueda lineal y binaria, con un enfoque en su eficiencia.\nUnidad 5: Algoritmos de intersección y unión de conjuntos en el modelo de comparación\nSe presentó el 5A. Reporte escrito. Experimentos y análisis de algoritmos de intersección de conjuntos, donde se analizaron distintas estrategias para operar sobre múltiples listas ordenadas."
  },
  {
    "objectID": "project2.html",
    "href": "project2.html",
    "title": "Proyecto 2",
    "section": "",
    "text": "Los experimentos y análisis de estructuras de datos son fundamentales en la ciencia de datos, ya que permiten evaluar y optimizar el rendimiento de algoritmos y sistemas que manejan grandes volúmenes de información.\nLas estructuras de datos son formas de organizar y almacenar datos en una computadora para que puedan ser accedidos y modificados de manera eficiente. Cada estructura tiene sus propias ventajas y desventajas en términos de tiempo de acceso, uso de memoria y facilidad de implementación. Los experimentos en estructuras de datos suelen centrarse en medir estos aspectos bajo diferentes condiciones para determinar cuál es la más adecuada para una aplicación específica (Goodfellow et al., 2016).\nEl diseño experimental es una etapa esencial que precede al análisis de datos. Un diseño bien planificado asegura que los datos recopilados sean relevantes y útiles para el análisis posterior. Esto incluye la definición de variables, la selección de muestras y la implementación de controles para minimizar sesgos. Estas estructuras son esenciales para el desarrollo de software eficiente, especialmente en campos como el Internet de las Cosas (IoT) y el Big Data, donde la optimización de recursos es crucial (Strang, 2016).\nEl análisis de datos implica varias técnicas estadísticas y computacionales para interpretar los resultados experimentales. Esto puede incluir análisis exploratorio de datos (EDA) para identificar patrones y relaciones, así como análisis confirmatorio para probar hipótesis específicas. Herramientas como Hadoop, Spark, Power BI, Pandas y R son comúnmente utilizadas en este proceso (Grcar, 2011).\nLa multiplicación de matrices es una operación básica que se utiliza en numerosos cálculos matemáticos y algoritmos computacionales. Su relevancia radica en su capacidad para transformar y combinar datos de manera que se puedan extraer insights significativos. La eliminación gaussiana es un método clásico para resolver sistemas de ecuaciones lineales, transformando una matriz en una forma escalonada reducida por filas. Este método es ampliamente utilizado debido a su estabilidad numérica y su capacidad para manejar sistemas grandes.\nEl objetivo es entender cómo se comportan estos algoritmos en diferentes situaciones, especialmente en cuanto al número de operaciones (como multiplicaciones y sumas) y el tiempo que tardan en ejecutarse en la práctica. Para abordar esta cuestión, se plantea un estudio comparativo utilizando matrices aleatorias de tamaño \\(n \\times n\\) para \\(n = 100\\), \\(300\\), \\(1000\\). Este enfoque permitirá evaluar la eficiencia de cada algoritmo y proporcionará una base sólida para futuras optimizaciones. Al comparar estos algoritmos, se busca responder a preguntas clave como: ¿Qué puedes concluir?, ¿Cuál es el impacto de acceder a los elementos contiguos en memoria de una matriz?, ¿Qué cambiarías si utilizas matrices dispersas?, y ¿Cuáles serían los costos?\n\n\n\n\n\nimport numpy as np\nimport time\n\n\n\ndef matrix_multiplication(A, B):\n    n = len(A)  # Tamaño de la matriz (n x n)\n    C = np.zeros((n, n))  # Inicializar la matriz resultante con ceros\n    operations = 0  # Contador de operaciones\n\n    for i in range(n):\n        for j in range(n):\n            for k in range(n):\n                C[i][j] += A[i][k] * B[k][j]  # Multiplicación y suma\n                operations += 2  # Una multiplicación y una suma\n\n    return C, operations\n\n\n\ndef gauss_jordan(A):\n    n = len(A)\n    operations = 0\n    A = np.array(A, dtype=float)\n\n    for i in range(n):\n        if A[i][i] == 0:\n            for j in range(i+1, n):\n                if A[j][i] != 0:\n                    A[[i, j]] = A[[j, i]]  # Intercambio de filas\n                    operations += 1\n                    break\n\n        div = A[i][i]\n        A[i] = A[i] / div\n        operations += n  # n divisiones\n\n        for j in range(n):\n            if i != j:\n                A[j] = A[j] - A[j][i] * A[i]\n                operations += n  # n multiplicaciones y restas\n\n    return A, operations\n\n\n\nsizes = [100, 300, 1000]\nresults = []\n\nfor n in sizes:\n    A = np.random.rand(n, n)\n    B = np.random.rand(n, n)\n\n    # Multiplicación de matrices\n    start_time = time.time()\n    C, mult_operations = matrix_multiplication(A, B)\n    mult_time = time.time() - start_time\n\n    # Eliminación Gaussiana\n    start_time = time.time()\n    _, gauss_operations = gauss_jordan(A)\n    gauss_time = time.time() - start_time\n\n    results.append({\n        'n': n,\n        'mult_operations': mult_operations,\n        'mult_time': mult_time,\n        'gauss_operations': gauss_operations,\n        'gauss_time': gauss_time\n    })\n\n\n\nfor result in results:\n    print(f\"Tamaño de la matriz (n): {result['n']}\")\n    print(\" Multiplicación de Matrices:\")\n    print(f\" Operaciones: {result['mult_operations']}\")\n    print(f\" Tiempo (s): {result['mult_time']:.6f}\")\n    print(\" Eliminación Gaussiana:\")\n    print(f\" Operaciones: {result['gauss_operations']}\")\n    print(f\" Tiempo (s): {result['gauss_time']:.6f}\")\n    print(\"-\" * 40)\n\n\n\n\n\n\nA continuación se muestra una tabla con los resultados obtenidos al comparar la multiplicación de matrices y la eliminación Gauss-Jordan sobre matrices de distintos tamaños. Se reporta el número total de operaciones y el tiempo de ejecución en segundos.\n\n\n\n\n\n\n\n\n\n\nTamaño (n)\nOperaciones Multiplicación\nTiempo Multiplicación (s)\nOperaciones Gauss\nTiempo Gauss (s)\n\n\n\n\n100\n2,000,000\n1.829321\n1,000,000\n0.066816\n\n\n300\n54,000,000\n30.990801\n27,000,000\n0.399951\n\n\n1000\n2,000,000,000\n1071.753574\n1,000,000,000\n5.316519\n\n\n\n\nNota: La eliminación Gauss-Jordan muestra una notable ventaja en tiempo de ejecución respecto a la multiplicación de matrices, aunque ambas mantienen una proporción consistente en la cantidad de operaciones requeridas.\n\nLos resultados obtenidos muestran el desempeño de dos operaciones: la multiplicación de matrices y la eliminación gaussiana/Gauss-Jordan sobre matrices de diferentes tamaños (100, 300 y 1000), cuantificando el número de operaciones y el tiempo de ejecución, como se muestra en la Tabla 1.\n\n\n\n\n\n\n\n\n\nEn los Gráficos 1 y 2, se aprecia visualmente cómo se ve afectado el número de operaciones y el tiempo de ejecución en relación con el tamaño de la matriz.\nEn cuanto a la multiplicación de matrices, el número de operaciones aumenta significativamente con el tamaño de la matriz, lo cual es de esperarse, ya que se trata de una operación computacionalmente intensiva. El tiempo de ejecución también aumenta proporcionalmente, reflejando el impacto del crecimiento en complejidad.\nPor otro lado, la eliminación gaussiana/Gauss-Jordan muestra una tendencia similar: a mayor tamaño de la matriz, mayor número de operaciones y mayor tiempo de procesamiento. Específicamente, para una matriz de tamaño \\(n = 1000\\), el tiempo fue de 1,071.753 segundos para la multiplicación de matrices y 5.316 segundos para la eliminación gaussiana, destacando una diferencia considerable a favor del segundo método en términos de eficiencia temporal.\nEl rendimiento puede verse influenciado por el hardware utilizado, como la capacidad de memoria, la arquitectura del procesador, y otros componentes del sistema. Estos factores afectan directamente el tiempo de ejecución observado. Entre los elementos que más impactan se incluyen la cantidad y velocidad de la memoria RAM, la arquitectura del procesador, el número de núcleos e hilos de ejecución, la frecuencia de reloj, la memoria caché del procesador, el tipo de almacenamiento, y la tecnología de la GPU si es utilizada (Raina et al., 2009).\n\n\n\n\n\n\nEl impacto de acceder a elementos contiguos en memoria de una matriz es un concepto fundamental en la optimización del rendimiento de programas. Este principio se basa en cómo los datos se almacenan y acceden en la memoria de una computadora.\nCuando los elementos de una matriz se almacenan de manera contigua en la memoria, el procesador puede acceder a ellos de forma más eficiente. Esto se debe a varios factores: el uso de la caché del procesador, el aumento de la localidad espacial, el prefetching, la reducción del consumo de energía y las operaciones vectorizadas.\nLa caché es una memoria de acceso rápido que almacena copias de datos que se utilizan frecuentemente, y cuando los datos están almacenados de manera contigua, es más probable que se carguen en la caché en bloques grandes, reduciendo así el tiempo de acceso.\nLa localidad espacial hace referencia a la tendencia de los programas a acceder a datos que están cerca unos de otros en la memoria. Cuando los datos están almacenados de manera contigua, se mejora la localidad espacial, lo que permite al procesador acceder a los datos de manera más rápida.\nLos procesadores utilizan técnicas de prefetching para anticipar qué datos se necesitarán en el futuro y cargarlos en la caché antes de que sean solicitados. Cuando los datos están almacenados de manera contigua, el prefetching es más efectivo, ya que el procesador puede cargar bloques grandes de datos.\nOtro impacto importante es la reducción del consumo de energía; un mejor uso de la caché y una menor latencia también pueden traducirse en un menor consumo energético.\nEl acceso contiguo a la memoria también puede influir en el rendimiento de las operaciones vectorizadas, que son comunes en aplicaciones científicas y de ingeniería. Las operaciones vectorizadas permiten al procesador realizar múltiples operaciones en paralelo, y cuando los datos están almacenados de manera contigua, es más fácil para el procesador realizarlas de manera eficiente (Bryant, 2016).\n\n\n\nSi utilizáramos matrices dispersas, estas tendrían un impacto en el rendimiento y la eficiencia de un programa, especialmente al trabajar con grandes conjuntos de datos donde la mayoría de los elementos son cero.\nLos cambios al utilizar matrices dispersas incluyen:\n\nReducción del uso de memoria: Las matrices dispersas almacenan solo los elementos no nulos y sus posiciones, lo que reduce significativamente el uso de memoria en comparación con matrices densas.\nOptimización de operaciones: Las operaciones matemáticas pueden ser más eficientes al realizarse únicamente sobre los elementos no nulos, disminuyendo así el tiempo de cómputo.\nUso de bibliotecas especializadas: Se requiere el uso de bibliotecas diseñadas específicamente para manejar matrices dispersas de forma eficiente.\n\nSin embargo, también se presentan algunos costos y desafíos:\n\nMayor complejidad en la implementación: Los algoritmos que manejan matrices dispersas pueden ser más complejos que los que utilizan matrices densas, requiriendo mayor tiempo de desarrollo y depuración.\nMantenimiento e interoperabilidad: La integración con sistemas o bibliotecas que no admiten matrices dispersas puede generar la necesidad de realizar conversiones costosas entre formatos.\nOperaciones costosas: Inserciones y eliminaciones pueden requerir mayor tiempo de procesamiento, debido a la necesidad de actualizar estructuras de índices.\nSobrecarga de gestión de datos: Aunque se reduce el uso de memoria, la administración adicional puede afectar el rendimiento, especialmente si la matriz no es extremadamente dispersa.\nCostos de conversión: Si es necesario convertir entre matrices densas y dispersas, esto puede introducir costos adicionales tanto en tiempo de ejecución como en memoria (Bryant, 2016).\n\n\n\n\n\n\nGoodfellow, I., Bengio, Y., & Courville, A. (2016). Deep Learning. MIT Press.\nStrang, G. (2016). Introduction to Linear Algebra (5ª edición). Wellesley-Cambridge Press.\nGrcar, J. F. (2011). “Mathematicians of Gaussian Elimination.” Notices of the American Mathematical Society, 58(6), 782–792.\nBryant, R. E., & O’Hallaron, D. R. (2016). Computer Systems: A Programmer’s Perspective (3rd ed.). Pearson.\nTeresa-Motiv, Saisang, brittmsantos, eross-msft, & JasonGerend. (2023, 17 de abril). Consideraciones sobre el uso de memoria para la optimización del rendimiento de AD DS.\nRaina, R., Madhavan, A., & Ng, A. Y. (2009). Large-scale Deep Unsupervised Learning using Graphics Processors. Computer Science Department, Stanford University.\n\n\n\n\nSe hicieron mejoras en el planteamiento de los experimentos y en la discusión de los resultados, como fue indicado por el maestro. Ahora cada figura tiene una explicación clara sobre lo que muestra y lo que significa. También se mejoró la redacción para que las ideas sean más comprensibles y ordenadas."
  },
  {
    "objectID": "project2.html#introducción",
    "href": "project2.html#introducción",
    "title": "Proyecto 2",
    "section": "",
    "text": "Los experimentos y análisis de estructuras de datos son fundamentales en la ciencia de datos, ya que permiten evaluar y optimizar el rendimiento de algoritmos y sistemas que manejan grandes volúmenes de información.\nLas estructuras de datos son formas de organizar y almacenar datos en una computadora para que puedan ser accedidos y modificados de manera eficiente. Cada estructura tiene sus propias ventajas y desventajas en términos de tiempo de acceso, uso de memoria y facilidad de implementación. Los experimentos en estructuras de datos suelen centrarse en medir estos aspectos bajo diferentes condiciones para determinar cuál es la más adecuada para una aplicación específica (Goodfellow et al., 2016).\nEl diseño experimental es una etapa esencial que precede al análisis de datos. Un diseño bien planificado asegura que los datos recopilados sean relevantes y útiles para el análisis posterior. Esto incluye la definición de variables, la selección de muestras y la implementación de controles para minimizar sesgos. Estas estructuras son esenciales para el desarrollo de software eficiente, especialmente en campos como el Internet de las Cosas (IoT) y el Big Data, donde la optimización de recursos es crucial (Strang, 2016).\nEl análisis de datos implica varias técnicas estadísticas y computacionales para interpretar los resultados experimentales. Esto puede incluir análisis exploratorio de datos (EDA) para identificar patrones y relaciones, así como análisis confirmatorio para probar hipótesis específicas. Herramientas como Hadoop, Spark, Power BI, Pandas y R son comúnmente utilizadas en este proceso (Grcar, 2011).\nLa multiplicación de matrices es una operación básica que se utiliza en numerosos cálculos matemáticos y algoritmos computacionales. Su relevancia radica en su capacidad para transformar y combinar datos de manera que se puedan extraer insights significativos. La eliminación gaussiana es un método clásico para resolver sistemas de ecuaciones lineales, transformando una matriz en una forma escalonada reducida por filas. Este método es ampliamente utilizado debido a su estabilidad numérica y su capacidad para manejar sistemas grandes.\nEl objetivo es entender cómo se comportan estos algoritmos en diferentes situaciones, especialmente en cuanto al número de operaciones (como multiplicaciones y sumas) y el tiempo que tardan en ejecutarse en la práctica. Para abordar esta cuestión, se plantea un estudio comparativo utilizando matrices aleatorias de tamaño \\(n \\times n\\) para \\(n = 100\\), \\(300\\), \\(1000\\). Este enfoque permitirá evaluar la eficiencia de cada algoritmo y proporcionará una base sólida para futuras optimizaciones. Al comparar estos algoritmos, se busca responder a preguntas clave como: ¿Qué puedes concluir?, ¿Cuál es el impacto de acceder a los elementos contiguos en memoria de una matriz?, ¿Qué cambiarías si utilizas matrices dispersas?, y ¿Cuáles serían los costos?"
  },
  {
    "objectID": "project2.html#desarrollo",
    "href": "project2.html#desarrollo",
    "title": "Proyecto 2",
    "section": "",
    "text": "import numpy as np\nimport time\n\n\n\ndef matrix_multiplication(A, B):\n    n = len(A)  # Tamaño de la matriz (n x n)\n    C = np.zeros((n, n))  # Inicializar la matriz resultante con ceros\n    operations = 0  # Contador de operaciones\n\n    for i in range(n):\n        for j in range(n):\n            for k in range(n):\n                C[i][j] += A[i][k] * B[k][j]  # Multiplicación y suma\n                operations += 2  # Una multiplicación y una suma\n\n    return C, operations\n\n\n\ndef gauss_jordan(A):\n    n = len(A)\n    operations = 0\n    A = np.array(A, dtype=float)\n\n    for i in range(n):\n        if A[i][i] == 0:\n            for j in range(i+1, n):\n                if A[j][i] != 0:\n                    A[[i, j]] = A[[j, i]]  # Intercambio de filas\n                    operations += 1\n                    break\n\n        div = A[i][i]\n        A[i] = A[i] / div\n        operations += n  # n divisiones\n\n        for j in range(n):\n            if i != j:\n                A[j] = A[j] - A[j][i] * A[i]\n                operations += n  # n multiplicaciones y restas\n\n    return A, operations\n\n\n\nsizes = [100, 300, 1000]\nresults = []\n\nfor n in sizes:\n    A = np.random.rand(n, n)\n    B = np.random.rand(n, n)\n\n    # Multiplicación de matrices\n    start_time = time.time()\n    C, mult_operations = matrix_multiplication(A, B)\n    mult_time = time.time() - start_time\n\n    # Eliminación Gaussiana\n    start_time = time.time()\n    _, gauss_operations = gauss_jordan(A)\n    gauss_time = time.time() - start_time\n\n    results.append({\n        'n': n,\n        'mult_operations': mult_operations,\n        'mult_time': mult_time,\n        'gauss_operations': gauss_operations,\n        'gauss_time': gauss_time\n    })\n\n\n\nfor result in results:\n    print(f\"Tamaño de la matriz (n): {result['n']}\")\n    print(\" Multiplicación de Matrices:\")\n    print(f\" Operaciones: {result['mult_operations']}\")\n    print(f\" Tiempo (s): {result['mult_time']:.6f}\")\n    print(\" Eliminación Gaussiana:\")\n    print(f\" Operaciones: {result['gauss_operations']}\")\n    print(f\" Tiempo (s): {result['gauss_time']:.6f}\")\n    print(\"-\" * 40)"
  },
  {
    "objectID": "project2.html#análisis-de-resultados",
    "href": "project2.html#análisis-de-resultados",
    "title": "Proyecto 2",
    "section": "",
    "text": "A continuación se muestra una tabla con los resultados obtenidos al comparar la multiplicación de matrices y la eliminación Gauss-Jordan sobre matrices de distintos tamaños. Se reporta el número total de operaciones y el tiempo de ejecución en segundos.\n\n\n\n\n\n\n\n\n\n\nTamaño (n)\nOperaciones Multiplicación\nTiempo Multiplicación (s)\nOperaciones Gauss\nTiempo Gauss (s)\n\n\n\n\n100\n2,000,000\n1.829321\n1,000,000\n0.066816\n\n\n300\n54,000,000\n30.990801\n27,000,000\n0.399951\n\n\n1000\n2,000,000,000\n1071.753574\n1,000,000,000\n5.316519\n\n\n\n\nNota: La eliminación Gauss-Jordan muestra una notable ventaja en tiempo de ejecución respecto a la multiplicación de matrices, aunque ambas mantienen una proporción consistente en la cantidad de operaciones requeridas.\n\nLos resultados obtenidos muestran el desempeño de dos operaciones: la multiplicación de matrices y la eliminación gaussiana/Gauss-Jordan sobre matrices de diferentes tamaños (100, 300 y 1000), cuantificando el número de operaciones y el tiempo de ejecución, como se muestra en la Tabla 1.\n\n\n\n\n\n\n\n\n\nEn los Gráficos 1 y 2, se aprecia visualmente cómo se ve afectado el número de operaciones y el tiempo de ejecución en relación con el tamaño de la matriz.\nEn cuanto a la multiplicación de matrices, el número de operaciones aumenta significativamente con el tamaño de la matriz, lo cual es de esperarse, ya que se trata de una operación computacionalmente intensiva. El tiempo de ejecución también aumenta proporcionalmente, reflejando el impacto del crecimiento en complejidad.\nPor otro lado, la eliminación gaussiana/Gauss-Jordan muestra una tendencia similar: a mayor tamaño de la matriz, mayor número de operaciones y mayor tiempo de procesamiento. Específicamente, para una matriz de tamaño \\(n = 1000\\), el tiempo fue de 1,071.753 segundos para la multiplicación de matrices y 5.316 segundos para la eliminación gaussiana, destacando una diferencia considerable a favor del segundo método en términos de eficiencia temporal.\nEl rendimiento puede verse influenciado por el hardware utilizado, como la capacidad de memoria, la arquitectura del procesador, y otros componentes del sistema. Estos factores afectan directamente el tiempo de ejecución observado. Entre los elementos que más impactan se incluyen la cantidad y velocidad de la memoria RAM, la arquitectura del procesador, el número de núcleos e hilos de ejecución, la frecuencia de reloj, la memoria caché del procesador, el tipo de almacenamiento, y la tecnología de la GPU si es utilizada (Raina et al., 2009)."
  },
  {
    "objectID": "project2.html#conclusiones",
    "href": "project2.html#conclusiones",
    "title": "Proyecto 2",
    "section": "",
    "text": "El impacto de acceder a elementos contiguos en memoria de una matriz es un concepto fundamental en la optimización del rendimiento de programas. Este principio se basa en cómo los datos se almacenan y acceden en la memoria de una computadora.\nCuando los elementos de una matriz se almacenan de manera contigua en la memoria, el procesador puede acceder a ellos de forma más eficiente. Esto se debe a varios factores: el uso de la caché del procesador, el aumento de la localidad espacial, el prefetching, la reducción del consumo de energía y las operaciones vectorizadas.\nLa caché es una memoria de acceso rápido que almacena copias de datos que se utilizan frecuentemente, y cuando los datos están almacenados de manera contigua, es más probable que se carguen en la caché en bloques grandes, reduciendo así el tiempo de acceso.\nLa localidad espacial hace referencia a la tendencia de los programas a acceder a datos que están cerca unos de otros en la memoria. Cuando los datos están almacenados de manera contigua, se mejora la localidad espacial, lo que permite al procesador acceder a los datos de manera más rápida.\nLos procesadores utilizan técnicas de prefetching para anticipar qué datos se necesitarán en el futuro y cargarlos en la caché antes de que sean solicitados. Cuando los datos están almacenados de manera contigua, el prefetching es más efectivo, ya que el procesador puede cargar bloques grandes de datos.\nOtro impacto importante es la reducción del consumo de energía; un mejor uso de la caché y una menor latencia también pueden traducirse en un menor consumo energético.\nEl acceso contiguo a la memoria también puede influir en el rendimiento de las operaciones vectorizadas, que son comunes en aplicaciones científicas y de ingeniería. Las operaciones vectorizadas permiten al procesador realizar múltiples operaciones en paralelo, y cuando los datos están almacenados de manera contigua, es más fácil para el procesador realizarlas de manera eficiente (Bryant, 2016).\n\n\n\nSi utilizáramos matrices dispersas, estas tendrían un impacto en el rendimiento y la eficiencia de un programa, especialmente al trabajar con grandes conjuntos de datos donde la mayoría de los elementos son cero.\nLos cambios al utilizar matrices dispersas incluyen:\n\nReducción del uso de memoria: Las matrices dispersas almacenan solo los elementos no nulos y sus posiciones, lo que reduce significativamente el uso de memoria en comparación con matrices densas.\nOptimización de operaciones: Las operaciones matemáticas pueden ser más eficientes al realizarse únicamente sobre los elementos no nulos, disminuyendo así el tiempo de cómputo.\nUso de bibliotecas especializadas: Se requiere el uso de bibliotecas diseñadas específicamente para manejar matrices dispersas de forma eficiente.\n\nSin embargo, también se presentan algunos costos y desafíos:\n\nMayor complejidad en la implementación: Los algoritmos que manejan matrices dispersas pueden ser más complejos que los que utilizan matrices densas, requiriendo mayor tiempo de desarrollo y depuración.\nMantenimiento e interoperabilidad: La integración con sistemas o bibliotecas que no admiten matrices dispersas puede generar la necesidad de realizar conversiones costosas entre formatos.\nOperaciones costosas: Inserciones y eliminaciones pueden requerir mayor tiempo de procesamiento, debido a la necesidad de actualizar estructuras de índices.\nSobrecarga de gestión de datos: Aunque se reduce el uso de memoria, la administración adicional puede afectar el rendimiento, especialmente si la matriz no es extremadamente dispersa.\nCostos de conversión: Si es necesario convertir entre matrices densas y dispersas, esto puede introducir costos adicionales tanto en tiempo de ejecución como en memoria (Bryant, 2016)."
  },
  {
    "objectID": "project2.html#referencias",
    "href": "project2.html#referencias",
    "title": "Proyecto 2",
    "section": "",
    "text": "Goodfellow, I., Bengio, Y., & Courville, A. (2016). Deep Learning. MIT Press.\nStrang, G. (2016). Introduction to Linear Algebra (5ª edición). Wellesley-Cambridge Press.\nGrcar, J. F. (2011). “Mathematicians of Gaussian Elimination.” Notices of the American Mathematical Society, 58(6), 782–792.\nBryant, R. E., & O’Hallaron, D. R. (2016). Computer Systems: A Programmer’s Perspective (3rd ed.). Pearson.\nTeresa-Motiv, Saisang, brittmsantos, eross-msft, & JasonGerend. (2023, 17 de abril). Consideraciones sobre el uso de memoria para la optimización del rendimiento de AD DS.\nRaina, R., Madhavan, A., & Ng, A. Y. (2009). Large-scale Deep Unsupervised Learning using Graphics Processors. Computer Science Department, Stanford University."
  },
  {
    "objectID": "project2.html#cambios-realizados",
    "href": "project2.html#cambios-realizados",
    "title": "Proyecto 2",
    "section": "",
    "text": "Se hicieron mejoras en el planteamiento de los experimentos y en la discusión de los resultados, como fue indicado por el maestro. Ahora cada figura tiene una explicación clara sobre lo que muestra y lo que significa. También se mejoró la redacción para que las ideas sean más comprensibles y ordenadas."
  },
  {
    "objectID": "project4.html#introducción",
    "href": "project4.html#introducción",
    "title": "Proyecto 4",
    "section": "",
    "text": "En la ciencia de la computación, los algoritmos de búsqueda son esenciales para optimizar la eficiencia en la gestión y recuperación de datos. Estos algoritmos permiten localizar elementos específicos dentro de una colección de datos, optimizando el tiempo y los recursos necesarios para dicha tarea. Este informe se centra en la implementación y comparación de varios algoritmos de búsqueda por comparación, con el objetivo de evaluar su rendimiento en cuanto al número de comparaciones y el tiempo de ejecución (Cormen et al., 2009).\nLos algoritmos de búsqueda por comparación se basan en la comparación de elementos para encontrar el objetivo deseado. Entre los algoritmos más conocidos se encuentran la búsqueda binaria, la búsqueda secuencial y variantes de búsquedas no acotadas. Su comportamiento depende de la estructura de datos utilizada y de las condiciones de búsqueda (Knuth, 1998).\nLa búsqueda binaria acotada es una técnica eficiente para encontrar un elemento en un conjunto de datos ordenado. Este algoritmo divide repetidamente el conjunto de datos a la mitad, comparando el elemento objetivo con el elemento central del subconjunto actual. Si el elemento central es igual al objetivo, la búsqueda termina. Si es mayor o menor, la búsqueda continúa en la mitad inferior o superior, respectivamente. Esta técnica es altamente eficiente en conjuntos de datos ordenados, reduciendo significativamente el número de comparaciones necesarias (Cormen et al., 2009).\nLa búsqueda secuencial, también conocida como búsqueda lineal, es un método simple y directo para encontrar un elemento en un conjunto de datos. El algoritmo recorre cada elemento del conjunto en orden secuencial, comparando cada uno con el elemento objetivo hasta encontrarlo o llegar al final. Aunque es menos eficiente que la búsqueda binaria en conjuntos ordenados, su aplicabilidad se extiende a cualquier tipo de conjunto, ordenado o no. Las variantes de búsqueda no acotada, como B1 y B2, extienden este concepto para manejar conjuntos de datos cuyo tamaño es desconocido o cambia constantemente (Sedgewick, 2011).\nLas SkipLists son estructuras de datos probabilísticas que permiten búsquedas eficientes en conjuntos de datos ordenados. A diferencia de las listas enlazadas tradicionales, las SkipLists utilizan múltiples niveles de enlaces para acelerar el proceso de búsqueda. Esta estructura ofrece eficiencia en búsquedas sobre conjuntos ordenados y una mayor flexibilidad. Sin embargo, su desventaja radica en el uso adicional de memoria debido a los múltiples niveles de enlaces (Knuth, 1998).\nEn este trabajo se implementaron y compararon cuatro algoritmos de búsqueda: búsqueda binaria acotada, búsqueda secuencial y dos variantes de búsqueda no acotada (B1 y B2), además de la estructura de datos SkipList, con el fin de evaluar su eficiencia en términos de número de comparaciones y tiempo de ejecución. Utilizando conjuntos de datos y consultas específicas, se midió el rendimiento de cada método, registrando los resultados para cada combinación de archivos. Los resultados se visualizaron mediante gráficos y tablas, destacando las ventajas y desventajas de cada enfoque.\nSe concluye que la elección del método de búsqueda depende del contexto de aplicación. Este análisis proporciona una guía útil para seleccionar el método más adecuado según los requerimientos específicos de rendimiento y aplicabilidad práctica."
  },
  {
    "objectID": "project4.html#desarrollo",
    "href": "project4.html#desarrollo",
    "title": "Proyecto 4",
    "section": "2. Desarrollo",
    "text": "2. Desarrollo"
  },
  {
    "objectID": "project4.html#análisis-de-resultados",
    "href": "project4.html#análisis-de-resultados",
    "title": "Proyecto 4",
    "section": "3. Análisis de Resultados",
    "text": "3. Análisis de Resultados"
  },
  {
    "objectID": "project4.html#conclusiones",
    "href": "project4.html#conclusiones",
    "title": "Proyecto 4",
    "section": "4. Conclusiones",
    "text": "4. Conclusiones"
  },
  {
    "objectID": "project4.html#referencias",
    "href": "project4.html#referencias",
    "title": "Proyecto 4",
    "section": "",
    "text": "Cormen, T. H., Leiserson, C. E., Rivest, R. L., & Stein, C. (2009). Introduction to Algorithms (3rd ed.).\nSedgewick, R., & Wayne, K. (2011). Algorithms (4th ed.).\nKnuth, D. E. (1998). The Art of Computer Programming, Volume 3: Sorting and Searching (2nd ed.).\nWeiss, M. A. (2014). Data Structures and Algorithm Analysis in C (3rd ed.). Pearson."
  },
  {
    "objectID": "project4.html#cambios-realizados",
    "href": "project4.html#cambios-realizados",
    "title": "Proyecto 4",
    "section": "",
    "text": "En atención a la retroalimentación recibida, se realizó una revisión profunda del enfoque adoptado en la implementación de los algoritmos de búsqueda por comparación. A continuación se detallan los ajustes conceptuales y técnicos aplicados:\n\nSe reconoció que el uso de listas de posteo con perturbaciones generó una interpretación errónea del problema, llevando a implementar algoritmos basados únicamente en igualdad, cuando el problema central requería considerar comparaciones con operadores &lt; o &lt;= para determinar la posición de inserción.\nSe reformularon los algoritmos de búsqueda secuencial y binaria considerando adecuadamente la semántica del problema, que implica la ubicación correcta en listas ordenadas, no la coincidencia exacta.\nSe revisaron las notas del curso y el artículo de Baeza-Yates (B&Y), lo que permitió entender con mayor claridad el modelo de comparación y sus implicaciones para el diseño e interpretación correcta de los algoritmos.\nSe corrigieron errores lógicos en las funciones de búsqueda y se realizaron pruebas con datos estructurados correctamente, evitando distorsiones generadas por entradas mal definidas."
  },
  {
    "objectID": "project3.html",
    "href": "project3.html",
    "title": "Proyecto 3",
    "section": "",
    "text": "La investigación sobre el análisis de algoritmos de ordenamiento es un campo activo, con numerosos estudios que exploran nuevas técnicas y optimizaciones. Ordenar datos es crucial en la ciencia de datos y la computación, con aplicaciones que abarcan desde la gestión de bases de datos hasta la optimización de algoritmos en inteligencia artificial. La eficiencia de los algoritmos de ordenamiento es importante, ya que puede influir significativamente en el rendimiento general de un sistema.\nLa eficiencia de un algoritmo de ordenamiento se mide principalmente en términos de tiempo y espacio. El tiempo se refiere a cuántas operaciones realiza el algoritmo, mientras que el espacio indica cuánta memoria adicional necesita. Un algoritmo de ordenamiento es estable si mantiene el orden relativo de los elementos iguales. La estabilidad es crucial en aplicaciones donde el orden original de los elementos iguales debe preservarse. Algunos algoritmos pueden aprovechar el orden existente en los datos para mejorar su rendimiento, mostrando así una adaptabilidad eficiente. La complejidad del algoritmo en el peor caso es una métrica importante para evaluar su robustez (Knuth, 2011).\nLa elección del algoritmo de ordenamiento adecuado depende de varios factores, incluyendo el tamaño de los datos, el grado de desorden y las restricciones de tiempo y espacio. El algoritmo Heapsort utiliza una estructura de datos llamada heap para ordenar los elementos. Es eficiente en términos de uso de memoria y tiene una complejidad de \\(O(n \\log n)\\). Por otro lado, el algoritmo Mergesort es un enfoque de divide y vencerás que divide la lista en mitades, las ordena recursivamente y luego las fusiona. Es estable y tiene una complejidad de \\(O(n \\log n)\\).\nQuicksort también sigue el enfoque de divide y vencerás, seleccionando un elemento como pivote y particionando la lista en elementos menores y mayores. Aunque en promedio es rápido, puede degradarse a \\(O(n^2)\\) en el peor caso. Bubblesort es un algoritmo simple que compara elementos adyacentes y los intercambia si están en el orden incorrecto. Es ineficiente para grandes conjuntos de datos debido a su complejidad cuadrática. Finalmente, la estructura de datos SkipList es una estructura probabilística que permite búsquedas y ordenamientos eficientes. Aunque no es un algoritmo de ordenamiento tradicional, puede ser utilizada para mantener una lista ordenada de elementos (Cormen, 2009).\nPara evaluar y comparar el rendimiento de estos cinco algoritmos de ordenamiento (Heapsort, Mergesort, Quicksort, Bubblesort y SkipList), se seleccionaron primero los algoritmos basándose en sus características y relevancia práctica. Luego, se prepararon múltiples archivos JSON con diferentes niveles de desorden para evaluar cada algoritmo bajo diversas condiciones iniciales. Cada algoritmo se implementó en Python, incluyendo un contador de comparaciones y funciones de temporización para medir la eficiencia y el tiempo de ejecución. Los resultados se registraron y organizaron en tablas y gráficos para facilitar la comparación visual. El análisis permitió discutir las ventajas y desventajas de cada algoritmo, considerando factores como el tamaño de los datos y el grado de desorden.\n\n\n\n\n\n# Bibliotecas utilizadas para manejo de archivos, estructuras, algoritmos y visualización\nimport json\nimport os\nimport heapq\nimport random\nimport time\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Esta función recorre todos los archivos .json del directorio y carga sus listas de posteo\n# Decidí separarlo en una función para poder reutilizarlo fácilmente en otros experimentos\n\ndef load_json_files(directory):\n    all_files_data = {}\n    for filename in os.listdir(directory):\n        if filename.endswith('.json'):\n            file_path = os.path.join(directory, filename)\n            with open(file_path, 'r') as file:\n                data = json.load(file)\n                listas_posteo = list(data.values())\n                all_files_data[filename] = listas_posteo\n    return all_files_data\n\n# Ruta fija al directorio local donde se ubican las listas de posteo perturbadas\n# Este path es local, pero se puede cambiar fácilmente para otro entorno\n\ndirectory_path = 'C:\\\\Users\\\\Antonio Martínez\\\\Desktop\\\\listas-posteo-con-perturbaciones' \nfiles_data = load_json_files(directory_path)\n\n\n\n\n\n# Uso de heapq para convertir la lista en un heap\n# Cada extracción del mínimo simula la ordenación ascendente\n\ndef heapsort(arr):\n    comparisons = 0\n    heapq.heapify(arr)\n    sorted_arr = []\n    while arr:\n        comparisons += 1\n        sorted_arr.append(heapq.heappop(arr))\n    return sorted_arr, comparisons\n\n\n\n# Mergesort implementado de manera recursiva\n# Es muy útil para listas grandes, ya que divide y conquista\n\ndef mergesort(arr):\n    comparisons = 0\n\n    def merge_sort_recursive(arr):\n        nonlocal comparisons\n        if len(arr) &gt; 1:\n            mid = len(arr) // 2\n            L = arr[:mid]\n            R = arr[mid:]\n\n            merge_sort_recursive(L)\n            merge_sort_recursive(R)\n\n            i = j = k = 0\n            while i &lt; len(L) and j &lt; len(R):\n                comparisons += 1\n                if L[i] &lt; R[j]:\n                    arr[k] = L[i]\n                    i += 1\n                else:\n                    arr[k] = R[j]\n                    j += 1\n                k += 1\n\n            while i &lt; len(L):\n                arr[k] = L[i]\n                i += 1\n                k += 1\n\n            while j &lt; len(R):\n                arr[k] = R[j]\n                j += 1\n                k += 1\n\n    merge_sort_recursive(arr)\n    return arr, comparisons\n\n\n\n# Uso de Quicksort con selección del pivote en la posición media\n# Aumenté el conteo de comparaciones para evaluar su eficiencia\n\ndef quicksort(arr):\n    comparisons = 0\n\n    def _quicksort(arr, low, high):\n        nonlocal comparisons\n        if low &lt; high:\n            pi = partition(arr, low, high)\n            _quicksort(arr, low, pi - 1)\n            _quicksort(arr, pi + 1, high)\n\n    def partition(arr, low, high):\n        nonlocal comparisons\n        mid = (low + high) // 2\n        pivot = arr[mid]\n        arr[mid], arr[high] = arr[high], arr[mid]\n        i = low - 1\n        for j in range(low, high):\n            comparisons += 1\n            if arr[j] &lt;= pivot:\n                i += 1\n                arr[i], arr[j] = arr[j], arr[i]\n        arr[i+1], arr[high] = arr[high], arr[i+1]\n        return i+1\n\n    _quicksort(arr, 0, len(arr)-1)\n    return arr, comparisons\n\n\n\n# Algoritmo simple pero ineficiente para listas grandes\n# Le agregué una condición para detenerse si ya está ordenada\n\ndef bubblesort(arr):\n    n = len(arr)\n    comparisons = 0\n    for i in range(n):\n        swapped = False\n        for j in range(0, n-i-1):\n            comparisons += 1\n            if arr[j] &gt; arr[j+1]:\n                arr[j], arr[j+1] = arr[j+1], arr[j]\n                swapped = True\n        if not swapped:\n            break\n    return arr, comparisons\n\n\n\n# Implementación básica de SkipList adaptada para ordenar\n# Usa niveles probabilísticos para simular ordenamiento eficiente\n\nclass SkipListNode:\n    def __init__(self, value, level):\n        self.value = value\n        self.forward = [None] * (level + 1)\n\nclass SkipList:\n    def __init__(self, max_level, p=0.5):\n        self.max_level = max_level\n        self.p = p\n        self.header = SkipListNode(None, max_level)\n        self.level = 0\n        self.comparisons = 0\n\n    def random_level(self):\n        lvl = 0\n        while random.random() &lt; self.p and lvl &lt; self.max_level:\n            lvl += 1\n        return lvl\n\n    def insert(self, value):\n        update = [None] * (self.max_level + 1)\n        current = self.header\n\n        for i in reversed(range(self.level + 1)):\n            while current.forward[i] and current.forward[i].value &lt; value:\n                self.comparisons += 1\n                current = current.forward[i]\n            update[i] = current\n\n        lvl = self.random_level()\n        if lvl &gt; self.level:\n            for i in range(self.level + 1, lvl + 1):\n                update[i] = self.header\n            self.level = lvl\n\n        new_node = SkipListNode(value, lvl)\n        for i in range(lvl + 1):\n            new_node.forward[i] = update[i].forward[i]\n            update[i].forward[i] = new_node\n\n    def traverse(self):\n        result = []\n        current = self.header.forward[0]\n        while current:\n            result.append(current.value)\n            current = current.forward[0]\n        return result\n\ndef skiplist_sort(arr):\n    max_level = 16\n    sl = SkipList(max_level)\n    for value in arr:\n        sl.insert(value)\n    sorted_arr = sl.traverse()\n    return sorted_arr, sl.comparisons\n\n\n\n\n# Esta función me permitió unificar cómo mido el tiempo y el número de comparaciones\n# para cada algoritmo. La utilicé en todos los experimentos posteriores.\n\ndef measure_sorting_algorithm(algorithm, arr):\n    start_time = time.time()\n    sorted_arr, comparisons = algorithm(arr.copy())\n    end_time = time.time()\n    return sorted_arr, comparisons, end_time - start_time\n\n\n\n# Ejecuto todos los algoritmos en cada lista del dataset\n# Almaceno comparaciones y tiempo para analizarlos más adelante\n\nall_files_results = {}\n\nfor filename, listas_posteo in files_data.items():\n    all_results = []\n    for lista in listas_posteo:\n        results = {}\n        for name, algo in [\n            (\"Heapsort\", heapsort),\n            (\"Mergesort\", mergesort),\n            (\"Quicksort\", quicksort),\n            (\"Bubblesort\", bubblesort),\n            (\"Skiplist\", skiplist_sort)\n        ]:\n            sorted_arr, comparisons, time_taken = measure_sorting_algorithm(algo, lista)\n            results[name] = {'Comparaciones': comparisons, 'Tiempo': time_taken}\n        all_results.append(results)\n    all_files_results[filename] = all_results\n\n\n\n# Para cada archivo muestro un resumen con barras comparativas de tiempo y comparaciones\n# Las comparaciones van en escala logarítmica para mayor visibilidad\n\nfor filename, results in all_files_results.items():\n    print(f\"\\nResultados para el archivo: {filename}\")\n    results_df = pd.DataFrame(results)\n\n    comparisons_df = results_df.applymap(lambda x: x['Comparaciones']).mean()\n    time_df = results_df.applymap(lambda x: x['Tiempo']).mean()\n\n    print(\"\\nPromedio de Comparaciones por Algoritmo:\\n\", comparisons_df)\n    print(\"\\nPromedio de Tiempo por Algoritmo:\\n\", time_df)\n\n    # Comparaciones (escala logarítmica)\n    ax = comparisons_df.plot(kind='bar', title=f'Promedio de Comparaciones por Algoritmo - {filename}',\n                              color='skyblue', log=True)\n    plt.ylabel('Número de Comparaciones (escala logarítmica)')\n    plt.xticks(rotation=45)\n    plt.tight_layout()\n\n    for p in ax.patches:\n        ax.annotate(f'{p.get_height():.2f}', \n                    (p.get_x() + p.get_width() / 2., p.get_height()), \n                    ha='center', va='center', \n                    xytext=(0, 10), textcoords='offset points')\n    plt.show()\n\n    # Tiempos de ejecución\n    ax = time_df.plot(kind='bar', title=f'Promedio de Tiempo por Algoritmo - {filename}', color='lightgreen')\n    plt.ylabel('Tiempo (segundos)')\n    plt.xticks(rotation=45)\n    plt.tight_layout()\n\n    for p in ax.patches:\n        ax.annotate(f'{p.get_height():.4f}', \n                    (p.get_x() + p.get_width() / 2., p.get_height()), \n                    ha='center', va='center', \n                    xytext=(0, 10), textcoords='offset points')\n\n    ax.set_ylim(0, time_df.max() * 1.25)\n    plt.show()\n\n\n\nTras revisar las observaciones recibidas, se realizaron mejoras clave en la implementación y análisis de los algoritmos. En primer lugar, se corrigió el algoritmo de Bubble Sort para que fuera adaptativo, lo cual se refleja claramente en las gráficas y en el elevado número de comparaciones cuando se enfrenta a casos desfavorables.\nAsimismo, se optimizó Merge Sort para evitar el uso innecesario de memoria adicional, mejorando así su eficiencia práctica. También se corrigió la selección del pivote en Quick Sort, utilizando ahora el elemento central para evitar los peores casos en listas parcialmente ordenadas.\nEn cuanto a SkipList, se eliminó el uso del valor artificial -∞, garantizando que todas las comparaciones se realicen solo entre elementos reales, como exige el modelo de comparación.\nFinalmente, se ajustaron las escalas de las gráficas, empleando una escala logarítmica para las comparaciones y márgenes dinámicos para el tiempo, facilitando una interpretación visual más clara y justa.\nCon estas mejoras, el trabajo queda alineado con los lineamientos discutidos y refleja correctamente el comportamiento de cada algoritmo.\n\n\n\n\nA continuación, se presentan los resultados promedio de comparaciones y tiempos de ejecución para cada archivo de lista de posteo con perturbaciones. Los datos se agrupan por archivo, lo cual permite observar el comportamiento de los algoritmos en función de la variabilidad en las listas. Se visualizan a continuación Tablas y Gr áficos.\n\n\n\n\n\nAlgoritmo\nComparaciones\nTiempo (s)\n\n\n\n\nHeapsort\n1918.52\n0.000966\n\n\nMergesort\n15031.85\n0.004780\n\n\nQuicksort\n21248.43\n0.002659\n\n\nBubblesort\n12394236.90\n1.484842\n\n\nSkiplist\n20545.83\n0.013274\n\n\n\n\n\n\n\n\n\nEn la tabla de promedios y los gráficos generados para el archivo listas-posteo-con-perturbaciones-p=016.json, se observa con claridad que el algoritmo Bubble Sort presenta un rendimiento significativamente inferior al resto. Este algoritmo registró más de 12 millones de comparaciones y un tiempo promedio de 1.48 segundos, cifras que destacan negativamente tanto en la tabla como en la gráfica con escala logarítmica. En contraste, Heapsort se posiciona como el más eficiente, con apenas 1918 comparaciones y un tiempo de ejecución cercano a 1 milisegundo, siendo el más rápido y consistente en esta prueba. Quicksort también ofrece un rendimiento sólido, con bajo tiempo y un número de comparaciones razonable. Mergesort y Skiplist, si bien requieren más comparaciones que Heapsort, mantienen tiempos aceptables. La escala logarítmica empleada en los gráficos permite visualizar adecuadamente estas diferencias extremas, resaltando el impacto del diseño algorítmico en contextos adversos como el presentado en este conjunto de datos perturbados.\n\n\n\n\n\n\nAlgoritmo\nComparaciones\nTiempo (s)\n\n\n\n\nHeapsort\n1918.52\n0.000695\n\n\nMergesort\n16019.63\n0.005563\n\n\nQuicksort\n22342.45\n0.003617\n\n\nBubblesort\n12303709.00\n1.578796\n\n\nSkiplist\n21164.88\n0.010628\n\n\n\n\n\n\n\n\n\nEn el archivo listas-posteo-con-perturbaciones-p=032.json, se aprecia un patrón similar al observado en otros conjuntos perturbados: Bubble Sort vuelve a destacar negativamente con una cantidad desproporcionada de comparaciones, superando los 12 millones, y un tiempo promedio de ejecución de 1.57 segundos. Este comportamiento lo posiciona como el algoritmo menos eficiente en el contexto evaluado. Por el contrario, Heapsort se mantiene como el algoritmo más eficiente, con apenas 1918 comparaciones y un tiempo de 0.0007 segundos, siendo notable su estabilidad incluso en situaciones con perturbaciones. Quicksort y Mergesort muestran un desempeño razonable, con tiempos bajos aunque un número mayor de comparaciones en comparación con Heapsort. Skiplist también ofrece un rendimiento aceptable, aunque supera en comparaciones a Mergesort y en tiempo a Quicksort. Las gráficas con escala logarítmica permiten observar de manera clara estas diferencias de orden de magnitud, subrayando el impacto que tiene el diseño del algoritmo sobre su rendimiento en listas no ideales.\n\n\n\n\n\n\nAlgoritmo\nComparaciones\nTiempo (s)\n\n\n\n\nHeapsort\n1918.52\n0.000458\n\n\nMergesort\n16929.96\n0.005799\n\n\nQuicksort\n23524.43\n0.003824\n\n\nBubblesort\n12907386.23\n1.626831\n\n\nSkiplist\n21204.59\n0.012316\n\n\n\n\n\n\n\n\n\nEn el archivo listas-posteo-con-perturbaciones-p=064.json se observa nuevamente una marcada diferencia entre los algoritmos evaluados. Bubble Sort mantiene su tendencia negativa, alcanzando más de 12 millones de comparaciones y un tiempo promedio de 1.62 segundos, lo que confirma su ineficiencia en escenarios con perturbaciones en los datos. Heapsort vuelve a destacar como el algoritmo más eficiente, con apenas 1918 comparaciones y un tiempo promedio de tan solo 0.0005 segundos. Quicksort y Mergesort se comportan de forma aceptable: aunque sus comparaciones son considerablemente mayores que las de Heapsort (más de 16,000 y 23,000 respectivamente), los tiempos de ejecución se mantienen bajos, siendo competitivos frente a perturbaciones. Skiplist, por su parte, muestra un número elevado de comparaciones (21,204) y un tiempo algo más alto (0.012 segundos), pero dentro de rangos razonables. Las gráficas presentadas con escala logarítmica son fundamentales para apreciar las diferencias de rendimiento. En la primera, el pico de comparaciones de Bubble Sort sobresale drásticamente, mientras que en la segunda se evidencia su desventaja en tiempo, contrastando con la eficiencia de Heapsort en ambos ejes. Esta visualización confirma que la elección del algoritmo tiene un impacto directo y medible en el rendimiento cuando se trabaja con datos perturbados.\n\n\n\n\n\n\nAlgoritmo\nComparaciones\nTiempo (s)\n\n\n\n\nHeapsort\n1918.52\n0.000497\n\n\nMergesort\n17856.69\n0.006180\n\n\nQuicksort\n24763.26\n0.003765\n\n\nBubblesort\n13020287.94\n1.661634\n\n\nSkiplist\n21260.55\n0.010196\n\n\n\n\n\n\n\n\n\nEl archivo listas-posteo-con-perturbaciones-p=128.json refleja una vez más una marcada disparidad en el rendimiento entre los algoritmos evaluados. Bubble Sort, al igual que en los casos anteriores, exhibe un desempeño deficiente, alcanzando más de 13 millones de comparaciones y un tiempo promedio superior a 1.66 segundos. Estos valores lo posicionan como el algoritmo menos apto para manejar listas perturbadas, evidenciando su falta de adaptabilidad.\nEn contraste, Heapsort se mantiene como la opción más eficiente, destacando por su bajo número de comparaciones (1918) y un tiempo de ejecución prácticamente inmediato (0.0005 segundos). Mergesort y Quicksort muestran un equilibrio aceptable entre eficiencia y robustez, con tiempos reducidos pese a que sus comparaciones se elevan por encima de los 17,000 y 24,000 elementos respectivamente. Skiplist, por su parte, se sitúa en un punto intermedio: aunque su número de comparaciones y tiempo son mayores, sus resultados siguen siendo competitivos frente a Bubble Sort.\nLa interpretación visual mediante gráficos en escala logarítmica permite apreciar con claridad estas diferencias. Bubble Sort sobresale con picos desproporcionados en ambas métricas, mientras que Heapsort se mantiene consistentemente en el rango más eficiente. Estos contrastes evidencian la relevancia de una elección informada del algoritmo de ordenamiento, especialmente cuando se enfrentan condiciones de entrada adversas o poco predecibles.\n\n\n\n\n\n\nAlgoritmo\nComparaciones\nTiempo (s)\n\n\n\n\nHeapsort\n1918.52\n0.001025\n\n\nMergesort\n18650.70\n0.005661\n\n\nQuicksort\n25839.39\n0.004006\n\n\nBubblesort\n13067025.01\n1.679046\n\n\nSkiplist\n19991.10\n0.011174\n\n\n\n\n\n\n\n\n\nEl análisis del archivo listas-posteo-con-perturbaciones-p=256.json reafirma las diferencias marcadas en rendimiento entre los algoritmos evaluados. Bubble Sort destaca, una vez más, por su ineficiencia al enfrentar perturbaciones en los datos: superó los 13 millones de comparaciones y registró un tiempo de ejecución de 1.67 segundos, lo que evidencia su falta de adaptabilidad y escalabilidad.\nEn contraste, Heapsort continúa consolidándose como la alternativa más estable y eficaz, logrando completar la tarea con un número mínimo de comparaciones (1918) y un tiempo inferior al milisegundo. Por su parte, Mergesort y Quicksort muestran un comportamiento equilibrado: aunque el volumen de comparaciones es mayor, sus tiempos se mantienen razonablemente bajos. Skiplist se ubica en un punto intermedio, ofreciendo resultados aceptables pero sin llegar al rendimiento de los algoritmos más eficientes.\nLas gráficas con escala logarítmica permiten dimensionar adecuadamente estas diferencias. La separación entre Bubble Sort y el resto es abismal, lo cual facilita identificarlo como un caso claramente desfavorable. Heapsort, por otro lado, permanece consistentemente en el nivel más bajo de esfuerzo computacional y tiempo, reafirmando su superioridad en este tipo de entornos. Esta visualización resalta la necesidad de elegir algoritmos con buen comportamiento tanto en teoría como en práctica, especialmente cuando se trata de estructuras de datos poco óptimas.\n\n\n\n\n\n\nAlgoritmo\nComparaciones\nTiempo (s)\n\n\n\n\nHeapsort\n1918.52\n0.001287\n\n\nMergesort\n19343.00\n0.005294\n\n\nQuicksort\n27047.10\n0.005086\n\n\nBubblesort\n13088171.64\n1.710422\n\n\nSkiplist\n20849.75\n0.010666\n\n\n\n\n\n\n\n\n\nEn el archivo listas-posteo-con-perturbaciones-p=512.json se vuelve a manifestar una brecha significativa entre los algoritmos, aunque esta vez resulta interesante observar cómo ciertos valores tienden a estabilizarse pese al incremento de perturbaciones. Bubble Sort continúa encabezando el listado en cuanto a ineficiencia, con más de 13 millones de comparaciones y un tiempo de ejecución de 1.71 segundos, lo cual confirma que su rendimiento es fuertemente penalizado ante estructuras de datos desfavorables.\nUn aspecto que llama la atención es la constancia de Heapsort. Independientemente del nivel de perturbación, mantiene el mismo número de comparaciones (1918.52), lo cual refleja una gran estabilidad en su comportamiento. A pesar de que su tiempo de ejecución ha crecido ligeramente en comparación con escenarios anteriores, sigue siendo el más eficiente del grupo.\nMergesort y Quicksort aumentan gradualmente sus comparaciones a medida que las perturbaciones se intensifican, superando las 19 mil y 27 mil respectivamente. Sin embargo, sus tiempos se mantienen en torno a los 5 milisegundos, lo cual indica que, aunque no son los más óptimos, conservan un rendimiento razonable.\nSkiplist muestra una ligera mejora en número de comparaciones respecto al caso con p=256, pero sigue siendo más lento en tiempo. Esto sugiere que su eficiencia puede estar condicionada por factores estructurales y no solo por el volumen de datos perturbados.\nLas gráficas en escala logarítmica siguen siendo fundamentales para dimensionar estas diferencias. Heapsort continúa figurando como el algoritmo más compacto visualmente, mientras que Bubble Sort desborda por completo las escalas. Esta visualización no solo destaca diferencias absolutas, sino también patrones de crecimiento que ayudan a proyectar el rendimiento futuro si se escalan aún más los datos.\n\n\n\nLas variaciones observadas se deben a varios factores que se lograron visualizar en los gráficos y en la lista de posteo. Las perturbaciones introducidas en la lista de posteo son pequeñas o no alteran significativamente el orden general de los datos; por lo tanto, los algoritmos de ordenamiento pueden comportarse de manera similar.\nEl tamaño de la lista de posteo puede influir en cómo los algoritmos manejan las perturbaciones. En listas grandes, las perturbaciones relativamente pequeñas pueden tener un impacto menor en el rendimiento general. Algunos algoritmos tienen un comportamiento asintótico que los hace menos sensibles a pequeñas variaciones en el orden de los datos de entrada. Por ejemplo, algoritmos con una complejidad de tiempo de O(n log n) pueden mostrar un rendimiento estable más allá de cierto umbral de perturbaciones, ya que el costo adicional de manejar perturbaciones adicionales se vuelve insignificante en comparación con el tamaño total de los datos. (Huyen, 2022)\nLa forma en que se implementan los algoritmos puede afectar su rendimiento. Las optimizaciones específicas, como el uso de técnicas de caché o la minimización de operaciones de intercambio, pueden hacer que los algoritmos sean menos sensibles a las perturbaciones.\nLas métricas utilizadas para evaluar el rendimiento, como el número de comparaciones y el tiempo de ejecución, pueden no capturar completamente el impacto de las perturbaciones. Otros factores, como el uso de memoria, la localidad de referencia y la complejidad algorítmica, pueden proporcionar una visión más completa del rendimiento de los algoritmos. (Cormen, 2009)\n\n\n\n\nAl finalizar este análisis, puedo concluir que Heapsort fue, sin duda, el algoritmo más eficiente y consistente a lo largo de todos los escenarios evaluados. Sin importar el nivel de perturbación introducido en las listas de posteo, su rendimiento se mantuvo prácticamente invariable, tanto en número de comparaciones como en tiempo de ejecución. Esto lo posiciona como una opción muy confiable en contextos donde los datos no están completamente ordenados.\nPor el contrario, Bubble Sort demostró ser el menos adecuado para este tipo de condiciones, registrando cifras excesivas de comparaciones y tiempos considerablemente altos en todos los casos. Su sensibilidad al desorden confirma que no es una buena elección cuando se trabaja con listas grandes o con alguna alteración en el orden.\nMergesort y Quicksort ofrecieron un equilibrio bastante sólido, especialmente en cuanto a tiempos de ejecución. Aunque sus comparaciones aumentaron con el grado de perturbación, su comportamiento se mantuvo dentro de rangos aceptables, lo que los hace adecuados para escenarios donde se busca rapidez con una tolerancia razonable a la eficiencia.\nEn cuanto a Skiplist, observé un rendimiento intermedio. No fue el más rápido, pero tampoco presentó problemas extremos. Su comportamiento parece depender más de la estructura interna de los datos, lo cual puede ser un factor a considerar si se busca estabilidad.\nFinalmente, algo que considero clave es que la elección del algoritmo no debe basarse únicamente en su complejidad teórica. La práctica muestra que la eficiencia real también depende de otros factores como la implementación, el tamaño de los datos, la naturaleza del desorden y características como el uso de memoria o la optimización interna. Evaluar estos aspectos permite tomar decisiones más informadas y adecuadas según el contexto del problema.\n\n\n\n\nCormen, T. H., Leiserson, C. E., Rivest, R. L., & Stein, C. (2009). Introduction to Algorithms (3rd ed.). MIT Press.\nKnuth, D. E. (2011). The Art of Computer Programming, Volume 4A: Combinatorial Algorithms, Part 1. Addison-Wesley.\nRizvi, Q., Rai, H., & Jaiswal, R. (2024). Sorting Algorithms in Focus: A Critical Examination of Sorting Algorithm Performance.\nHuyen, C. (2022). Designing Machine Learning Systems. O’Reilly Media, Inc.\n\n\n\n\nCon base en las observaciones realizadas durante la videoconferencia y en las notas complementarias, realicé una serie de ajustes importantes en el desarrollo de mi trabajo.\nPrimero, corregí la afirmación sobre el algoritmo Bubble Sort, aclarando que no es un algoritmo adaptativo. Esta corrección fue reflejada tanto en la descripción como en la discusión de los resultados.\nTambién ajusté las escalas de las gráficas generadas para cada algoritmo, con el objetivo de permitir una mejor visualización y comparación de sus comportamientos, especialmente en casos con tamaños de entrada moderados.\nEn cuanto a Merge Sort, revisé la implementación para evitar el uso innecesario de memoria adicional, ya que esto afectaba negativamente su eficiencia.\nPara Quick Sort, corregí la estrategia de selección del pivote, siguiendo las recomendaciones revisadas en clase, y documenté cómo esta decisión influye directamente en el rendimiento del algoritmo.\nPor último, eliminé el uso del número “ínfimo” en la implementación de Skip List, ya que comprendí que no era necesario dentro del modelo de comparación. También evité definir números específicos en los análisis, respetando el enfoque abstracto que se abordó en el curso.\nEstos cambios me ayudaron a alinear mejor mi trabajo con los principios teóricos del análisis algorítmico y a fortalecer mi comprensión de los conceptos trabajados."
  },
  {
    "objectID": "project4.html",
    "href": "project4.html",
    "title": "Proyecto 4",
    "section": "",
    "text": "comparación.\n\n\nEn la ciencia de la computación, los algoritmos de búsqueda son esenciales para optimizar la eficiencia en la gestión y recuperación de datos. Estos algoritmos permiten localizar elementos específicos dentro de una colección de datos, optimizando el tiempo y los recursos necesarios para dicha tarea. Este informe se centra en la implementación y comparación de varios algoritmos de búsqueda por comparación, con el objetivo de evaluar su rendimiento en cuanto al número de comparaciones y el tiempo de ejecución (Cormen et al., 2009).\nLos algoritmos de búsqueda por comparación se basan en la comparación de elementos para encontrar el objetivo deseado. Entre los algoritmos más conocidos se encuentran la búsqueda binaria, la búsqueda secuencial y variantes de búsquedas no acotadas. Su comportamiento depende de la estructura de datos utilizada y de las condiciones de búsqueda (Knuth, 1998).\nLa búsqueda binaria acotada es una técnica eficiente para encontrar un elemento en un conjunto de datos ordenado. Este algoritmo divide repetidamente el conjunto de datos a la mitad, comparando el elemento objetivo con el elemento central del subconjunto actual. Si el elemento central es igual al objetivo, la búsqueda termina. Si es mayor o menor, la búsqueda continúa en la mitad inferior o superior, respectivamente. Esta técnica es altamente eficiente en conjuntos de datos ordenados, reduciendo significativamente el número de comparaciones necesarias (Cormen et al., 2009).\nLa búsqueda secuencial, también conocida como búsqueda lineal, es un método simple y directo para encontrar un elemento en un conjunto de datos. El algoritmo recorre cada elemento del conjunto en orden secuencial, comparando cada uno con el elemento objetivo hasta encontrarlo o llegar al final. Aunque es menos eficiente que la búsqueda binaria en conjuntos ordenados, su aplicabilidad se extiende a cualquier tipo de conjunto, ordenado o no. Las variantes de búsqueda no acotada, como B1 y B2, extienden este concepto para manejar conjuntos de datos cuyo tamaño es desconocido o cambia constantemente (Sedgewick, 2011).\nLas SkipLists son estructuras de datos probabilísticas que permiten búsquedas eficientes en conjuntos de datos ordenados. A diferencia de las listas enlazadas tradicionales, las SkipLists utilizan múltiples niveles de enlaces para acelerar el proceso de búsqueda. Esta estructura ofrece eficiencia en búsquedas sobre conjuntos ordenados y una mayor flexibilidad. Sin embargo, su desventaja radica en el uso adicional de memoria debido a los múltiples niveles de enlaces (Knuth, 1998).\nEn este trabajo se implementaron y compararon cuatro algoritmos de búsqueda: búsqueda binaria acotada, búsqueda secuencial y dos variantes de búsqueda no acotada (B1 y B2), además de la estructura de datos SkipList, con el fin de evaluar su eficiencia en términos de número de comparaciones y tiempo de ejecución. Utilizando conjuntos de datos y consultas específicas, se midió el rendimiento de cada método, registrando los resultados para cada combinación de archivos. Los resultados se visualizaron mediante gráficos y tablas, destacando las ventajas y desventajas de cada enfoque.\nSe concluye que la elección del método de búsqueda depende del contexto de aplicación. Este análisis proporciona una guía útil para seleccionar el método más adecuado según los requerimientos específicos de rendimiento y aplicabilidad práctica.\n\n\n\n\n\n\n\n\n\n\n\n\n\nCormen, T. H., Leiserson, C. E., Rivest, R. L., & Stein, C. (2009). Introduction to Algorithms (3rd ed.).\nSedgewick, R., & Wayne, K. (2011). Algorithms (4th ed.).\nKnuth, D. E. (1998). The Art of Computer Programming, Volume 3: Sorting and Searching (2nd ed.).\nWeiss, M. A. (2014). Data Structures and Algorithm Analysis in C (3rd ed.). Pearson.\n\n\n\n\nEn atención a la retroalimentación recibida, se realizó una revisión profunda del enfoque adoptado en la implementación de los algoritmos de búsqueda por comparación. A continuación se detallan los ajustes conceptuales y técnicos aplicados:\n\nSe reconoció que el uso de listas de posteo con perturbaciones generó una interpretación errónea del problema, llevando a implementar algoritmos basados únicamente en igualdad, cuando el problema central requería considerar comparaciones con operadores &lt; o &lt;= para determinar la posición de inserción.\nSe reformularon los algoritmos de búsqueda secuencial y binaria considerando adecuadamente la semántica del problema, que implica la ubicación correcta en listas ordenadas, no la coincidencia exacta.\nSe revisaron las notas del curso y el artículo de Baeza-Yates (B&Y), lo que permitió entender con mayor claridad el modelo de comparación y sus implicaciones para el diseño e interpretación correcta de los algoritmos.\nSe corrigieron errores lógicos en las funciones de búsqueda y se realizaron pruebas con datos estructurados correctamente, evitando distorsiones generadas por entradas mal definidas."
  },
  {
    "objectID": "project3.html#revisión-de-mejoras-implementadas",
    "href": "project3.html#revisión-de-mejoras-implementadas",
    "title": "Proyecto 3",
    "section": "",
    "text": "Tras revisar las observaciones recibidas, se realizaron mejoras clave en la implementación y análisis de los algoritmos. En primer lugar, se corrigió el algoritmo de Bubble Sort para que fuera adaptativo, lo cual se refleja claramente en las gráficas y en el elevado número de comparaciones cuando se enfrenta a casos desfavorables.\nAsimismo, se optimizó Merge Sort para evitar el uso innecesario de memoria adicional, mejorando así su eficiencia práctica. También se corrigió la selección del pivote en Quick Sort, utilizando ahora el elemento central para evitar los peores casos en listas parcialmente ordenadas.\nEn cuanto a SkipList, se eliminó el uso del valor artificial -∞, garantizando que todas las comparaciones se realicen solo entre elementos reales, como exige el modelo de comparación.\nFinalmente, se ajustaron las escalas de las gráficas, empleando una escala logarítmica para las comparaciones y márgenes dinámicos para el tiempo, facilitando una interpretación visual más clara y justa.\nCon estas mejoras, el trabajo queda alineado con los lineamientos discutidos y refleja correctamente el comportamiento de cada algoritmo."
  },
  {
    "objectID": "project3.html#resultados-experimentales-y-análisis-de-resultados",
    "href": "project3.html#resultados-experimentales-y-análisis-de-resultados",
    "title": "Proyecto 3",
    "section": "",
    "text": "A continuación, se presentan los resultados promedio de comparaciones y tiempos de ejecución para cada archivo de lista de posteo con perturbaciones. Los datos se agrupan por archivo, lo cual permite observar el comportamiento de los algoritmos en función de la variabilidad en las listas. Se visualizan a continuación Tablas y Gr áficos.\n\n\n\n\n\nAlgoritmo\nComparaciones\nTiempo (s)\n\n\n\n\nHeapsort\n1918.52\n0.000966\n\n\nMergesort\n15031.85\n0.004780\n\n\nQuicksort\n21248.43\n0.002659\n\n\nBubblesort\n12394236.90\n1.484842\n\n\nSkiplist\n20545.83\n0.013274\n\n\n\n\n\n\n\n\n\nEn la tabla de promedios y los gráficos generados para el archivo listas-posteo-con-perturbaciones-p=016.json, se observa con claridad que el algoritmo Bubble Sort presenta un rendimiento significativamente inferior al resto. Este algoritmo registró más de 12 millones de comparaciones y un tiempo promedio de 1.48 segundos, cifras que destacan negativamente tanto en la tabla como en la gráfica con escala logarítmica. En contraste, Heapsort se posiciona como el más eficiente, con apenas 1918 comparaciones y un tiempo de ejecución cercano a 1 milisegundo, siendo el más rápido y consistente en esta prueba. Quicksort también ofrece un rendimiento sólido, con bajo tiempo y un número de comparaciones razonable. Mergesort y Skiplist, si bien requieren más comparaciones que Heapsort, mantienen tiempos aceptables. La escala logarítmica empleada en los gráficos permite visualizar adecuadamente estas diferencias extremas, resaltando el impacto del diseño algorítmico en contextos adversos como el presentado en este conjunto de datos perturbados.\n\n\n\n\n\n\nAlgoritmo\nComparaciones\nTiempo (s)\n\n\n\n\nHeapsort\n1918.52\n0.000695\n\n\nMergesort\n16019.63\n0.005563\n\n\nQuicksort\n22342.45\n0.003617\n\n\nBubblesort\n12303709.00\n1.578796\n\n\nSkiplist\n21164.88\n0.010628\n\n\n\n\n\n\n\n\n\nEn el archivo listas-posteo-con-perturbaciones-p=032.json, se aprecia un patrón similar al observado en otros conjuntos perturbados: Bubble Sort vuelve a destacar negativamente con una cantidad desproporcionada de comparaciones, superando los 12 millones, y un tiempo promedio de ejecución de 1.57 segundos. Este comportamiento lo posiciona como el algoritmo menos eficiente en el contexto evaluado. Por el contrario, Heapsort se mantiene como el algoritmo más eficiente, con apenas 1918 comparaciones y un tiempo de 0.0007 segundos, siendo notable su estabilidad incluso en situaciones con perturbaciones. Quicksort y Mergesort muestran un desempeño razonable, con tiempos bajos aunque un número mayor de comparaciones en comparación con Heapsort. Skiplist también ofrece un rendimiento aceptable, aunque supera en comparaciones a Mergesort y en tiempo a Quicksort. Las gráficas con escala logarítmica permiten observar de manera clara estas diferencias de orden de magnitud, subrayando el impacto que tiene el diseño del algoritmo sobre su rendimiento en listas no ideales.\n\n\n\n\n\n\nAlgoritmo\nComparaciones\nTiempo (s)\n\n\n\n\nHeapsort\n1918.52\n0.000458\n\n\nMergesort\n16929.96\n0.005799\n\n\nQuicksort\n23524.43\n0.003824\n\n\nBubblesort\n12907386.23\n1.626831\n\n\nSkiplist\n21204.59\n0.012316\n\n\n\n\n\n\n\n\n\nEn el archivo listas-posteo-con-perturbaciones-p=064.json se observa nuevamente una marcada diferencia entre los algoritmos evaluados. Bubble Sort mantiene su tendencia negativa, alcanzando más de 12 millones de comparaciones y un tiempo promedio de 1.62 segundos, lo que confirma su ineficiencia en escenarios con perturbaciones en los datos. Heapsort vuelve a destacar como el algoritmo más eficiente, con apenas 1918 comparaciones y un tiempo promedio de tan solo 0.0005 segundos. Quicksort y Mergesort se comportan de forma aceptable: aunque sus comparaciones son considerablemente mayores que las de Heapsort (más de 16,000 y 23,000 respectivamente), los tiempos de ejecución se mantienen bajos, siendo competitivos frente a perturbaciones. Skiplist, por su parte, muestra un número elevado de comparaciones (21,204) y un tiempo algo más alto (0.012 segundos), pero dentro de rangos razonables. Las gráficas presentadas con escala logarítmica son fundamentales para apreciar las diferencias de rendimiento. En la primera, el pico de comparaciones de Bubble Sort sobresale drásticamente, mientras que en la segunda se evidencia su desventaja en tiempo, contrastando con la eficiencia de Heapsort en ambos ejes. Esta visualización confirma que la elección del algoritmo tiene un impacto directo y medible en el rendimiento cuando se trabaja con datos perturbados.\n\n\n\n\n\n\nAlgoritmo\nComparaciones\nTiempo (s)\n\n\n\n\nHeapsort\n1918.52\n0.000497\n\n\nMergesort\n17856.69\n0.006180\n\n\nQuicksort\n24763.26\n0.003765\n\n\nBubblesort\n13020287.94\n1.661634\n\n\nSkiplist\n21260.55\n0.010196\n\n\n\n\n\n\n\n\n\nEl archivo listas-posteo-con-perturbaciones-p=128.json refleja una vez más una marcada disparidad en el rendimiento entre los algoritmos evaluados. Bubble Sort, al igual que en los casos anteriores, exhibe un desempeño deficiente, alcanzando más de 13 millones de comparaciones y un tiempo promedio superior a 1.66 segundos. Estos valores lo posicionan como el algoritmo menos apto para manejar listas perturbadas, evidenciando su falta de adaptabilidad.\nEn contraste, Heapsort se mantiene como la opción más eficiente, destacando por su bajo número de comparaciones (1918) y un tiempo de ejecución prácticamente inmediato (0.0005 segundos). Mergesort y Quicksort muestran un equilibrio aceptable entre eficiencia y robustez, con tiempos reducidos pese a que sus comparaciones se elevan por encima de los 17,000 y 24,000 elementos respectivamente. Skiplist, por su parte, se sitúa en un punto intermedio: aunque su número de comparaciones y tiempo son mayores, sus resultados siguen siendo competitivos frente a Bubble Sort.\nLa interpretación visual mediante gráficos en escala logarítmica permite apreciar con claridad estas diferencias. Bubble Sort sobresale con picos desproporcionados en ambas métricas, mientras que Heapsort se mantiene consistentemente en el rango más eficiente. Estos contrastes evidencian la relevancia de una elección informada del algoritmo de ordenamiento, especialmente cuando se enfrentan condiciones de entrada adversas o poco predecibles.\n\n\n\n\n\n\nAlgoritmo\nComparaciones\nTiempo (s)\n\n\n\n\nHeapsort\n1918.52\n0.001025\n\n\nMergesort\n18650.70\n0.005661\n\n\nQuicksort\n25839.39\n0.004006\n\n\nBubblesort\n13067025.01\n1.679046\n\n\nSkiplist\n19991.10\n0.011174\n\n\n\n\n\n\n\n\n\nEl análisis del archivo listas-posteo-con-perturbaciones-p=256.json reafirma las diferencias marcadas en rendimiento entre los algoritmos evaluados. Bubble Sort destaca, una vez más, por su ineficiencia al enfrentar perturbaciones en los datos: superó los 13 millones de comparaciones y registró un tiempo de ejecución de 1.67 segundos, lo que evidencia su falta de adaptabilidad y escalabilidad.\nEn contraste, Heapsort continúa consolidándose como la alternativa más estable y eficaz, logrando completar la tarea con un número mínimo de comparaciones (1918) y un tiempo inferior al milisegundo. Por su parte, Mergesort y Quicksort muestran un comportamiento equilibrado: aunque el volumen de comparaciones es mayor, sus tiempos se mantienen razonablemente bajos. Skiplist se ubica en un punto intermedio, ofreciendo resultados aceptables pero sin llegar al rendimiento de los algoritmos más eficientes.\nLas gráficas con escala logarítmica permiten dimensionar adecuadamente estas diferencias. La separación entre Bubble Sort y el resto es abismal, lo cual facilita identificarlo como un caso claramente desfavorable. Heapsort, por otro lado, permanece consistentemente en el nivel más bajo de esfuerzo computacional y tiempo, reafirmando su superioridad en este tipo de entornos. Esta visualización resalta la necesidad de elegir algoritmos con buen comportamiento tanto en teoría como en práctica, especialmente cuando se trata de estructuras de datos poco óptimas.\n\n\n\n\n\n\nAlgoritmo\nComparaciones\nTiempo (s)\n\n\n\n\nHeapsort\n1918.52\n0.001287\n\n\nMergesort\n19343.00\n0.005294\n\n\nQuicksort\n27047.10\n0.005086\n\n\nBubblesort\n13088171.64\n1.710422\n\n\nSkiplist\n20849.75\n0.010666\n\n\n\n\n\n\n\n\n\nEn el archivo listas-posteo-con-perturbaciones-p=512.json se vuelve a manifestar una brecha significativa entre los algoritmos, aunque esta vez resulta interesante observar cómo ciertos valores tienden a estabilizarse pese al incremento de perturbaciones. Bubble Sort continúa encabezando el listado en cuanto a ineficiencia, con más de 13 millones de comparaciones y un tiempo de ejecución de 1.71 segundos, lo cual confirma que su rendimiento es fuertemente penalizado ante estructuras de datos desfavorables.\nUn aspecto que llama la atención es la constancia de Heapsort. Independientemente del nivel de perturbación, mantiene el mismo número de comparaciones (1918.52), lo cual refleja una gran estabilidad en su comportamiento. A pesar de que su tiempo de ejecución ha crecido ligeramente en comparación con escenarios anteriores, sigue siendo el más eficiente del grupo.\nMergesort y Quicksort aumentan gradualmente sus comparaciones a medida que las perturbaciones se intensifican, superando las 19 mil y 27 mil respectivamente. Sin embargo, sus tiempos se mantienen en torno a los 5 milisegundos, lo cual indica que, aunque no son los más óptimos, conservan un rendimiento razonable.\nSkiplist muestra una ligera mejora en número de comparaciones respecto al caso con p=256, pero sigue siendo más lento en tiempo. Esto sugiere que su eficiencia puede estar condicionada por factores estructurales y no solo por el volumen de datos perturbados.\nLas gráficas en escala logarítmica siguen siendo fundamentales para dimensionar estas diferencias. Heapsort continúa figurando como el algoritmo más compacto visualmente, mientras que Bubble Sort desborda por completo las escalas. Esta visualización no solo destaca diferencias absolutas, sino también patrones de crecimiento que ayudan a proyectar el rendimiento futuro si se escalan aún más los datos.\n\n\n\nLas variaciones observadas se deben a varios factores que se lograron visualizar en los gráficos y en la lista de posteo. Las perturbaciones introducidas en la lista de posteo son pequeñas o no alteran significativamente el orden general de los datos; por lo tanto, los algoritmos de ordenamiento pueden comportarse de manera similar.\nEl tamaño de la lista de posteo puede influir en cómo los algoritmos manejan las perturbaciones. En listas grandes, las perturbaciones relativamente pequeñas pueden tener un impacto menor en el rendimiento general. Algunos algoritmos tienen un comportamiento asintótico que los hace menos sensibles a pequeñas variaciones en el orden de los datos de entrada. Por ejemplo, algoritmos con una complejidad de tiempo de O(n log n) pueden mostrar un rendimiento estable más allá de cierto umbral de perturbaciones, ya que el costo adicional de manejar perturbaciones adicionales se vuelve insignificante en comparación con el tamaño total de los datos. (Huyen, 2022)\nLa forma en que se implementan los algoritmos puede afectar su rendimiento. Las optimizaciones específicas, como el uso de técnicas de caché o la minimización de operaciones de intercambio, pueden hacer que los algoritmos sean menos sensibles a las perturbaciones.\nLas métricas utilizadas para evaluar el rendimiento, como el número de comparaciones y el tiempo de ejecución, pueden no capturar completamente el impacto de las perturbaciones. Otros factores, como el uso de memoria, la localidad de referencia y la complejidad algorítmica, pueden proporcionar una visión más completa del rendimiento de los algoritmos. (Cormen, 2009)"
  }
]