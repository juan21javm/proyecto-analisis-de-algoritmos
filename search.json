[
  {
    "objectID": "project5.html#introducci√≥n",
    "href": "project5.html#introducci√≥n",
    "title": "Proyecto 5",
    "section": "1. Introducci√≥n",
    "text": "1. Introducci√≥n"
  },
  {
    "objectID": "project5.html#desarrollo",
    "href": "project5.html#desarrollo",
    "title": "Proyecto 5",
    "section": "2. Desarrollo",
    "text": "2. Desarrollo"
  },
  {
    "objectID": "project5.html#an√°lisis-de-resultados",
    "href": "project5.html#an√°lisis-de-resultados",
    "title": "Proyecto 5",
    "section": "3. An√°lisis de Resultados",
    "text": "3. An√°lisis de Resultados"
  },
  {
    "objectID": "project5.html#conclusiones",
    "href": "project5.html#conclusiones",
    "title": "Proyecto 5",
    "section": "4. Conclusiones",
    "text": "4. Conclusiones"
  },
  {
    "objectID": "project5.html#referencias",
    "href": "project5.html#referencias",
    "title": "Proyecto 5",
    "section": "5. Referencias",
    "text": "5. Referencias"
  },
  {
    "objectID": "project5.html#cambios-realizados",
    "href": "project5.html#cambios-realizados",
    "title": "Proyecto 5",
    "section": "6. Cambios Realizados",
    "text": "6. Cambios Realizados"
  },
  {
    "objectID": "project3.html#introducci√≥n",
    "href": "project3.html#introducci√≥n",
    "title": "Proyecto 3",
    "section": "1. Introducci√≥n",
    "text": "1. Introducci√≥n"
  },
  {
    "objectID": "project3.html#desarrollo",
    "href": "project3.html#desarrollo",
    "title": "Proyecto 3",
    "section": "2. Desarrollo",
    "text": "2. Desarrollo"
  },
  {
    "objectID": "project3.html#an√°lisis-de-resultados",
    "href": "project3.html#an√°lisis-de-resultados",
    "title": "Proyecto 3",
    "section": "3. An√°lisis de Resultados",
    "text": "3. An√°lisis de Resultados"
  },
  {
    "objectID": "project3.html#conclusiones",
    "href": "project3.html#conclusiones",
    "title": "Proyecto 3",
    "section": "4. Conclusiones",
    "text": "4. Conclusiones"
  },
  {
    "objectID": "project3.html#referencias",
    "href": "project3.html#referencias",
    "title": "Proyecto 3",
    "section": "5. Referencias",
    "text": "5. Referencias"
  },
  {
    "objectID": "project3.html#cambios-realizados",
    "href": "project3.html#cambios-realizados",
    "title": "Proyecto 3",
    "section": "6. Cambios Realizados",
    "text": "6. Cambios Realizados"
  },
  {
    "objectID": "project1.html",
    "href": "project1.html",
    "title": "Proyecto 1",
    "section": "",
    "text": "En la era digital actual, el big data ha emergido como un recurso valioso para las organizaciones, permiti√©ndoles extraer, procesar y analizar datos significativos. El big data se caracteriza por sus cuatro V: volumen, velocidad, variedad y veracidad. El volumen se refiere a la gran cantidad de datos generados y almacenados; la velocidad, a la rapidez con la que se generan y procesan; la variedad, a los diferentes tipos de datos (estructurados y no estructurados); y la veracidad, a la calidad y precisi√≥n de dichos datos. La capacidad de manejar y analizar estos grandes vol√∫menes de informaci√≥n de manera eficiente es crucial para aprovechar al m√°ximo el potencial del big data (M√ºller et al., 2016).\nEl an√°lisis de big data permite a las organizaciones identificar patrones, predecir tendencias y optimizar procesos, lo que puede resultar en mejoras significativas en la eficiencia operativa y en la toma de decisiones estrat√©gicas. Sin embargo, la manipulaci√≥n de grandes vol√∫menes de datos presenta desaf√≠os importantes en t√©rminos de infraestructura, almacenamiento, procesamiento y an√°lisis. Los algoritmos eficientes son fundamentales para enfrentar estos desaf√≠os y garantizar que los sistemas de big data funcionen de manera √≥ptima (Manyika et al., 2011).\nEste reporte se enfoca en comparar diferentes √≥rdenes de crecimiento mediante simulaciones en un entorno de Jupyter. Se analizar√°n los siguientes casos: \\(O(1)\\) vs \\(O(\\log n)\\), \\(O(n)\\) vs \\(O(n \\log n)\\), \\(O(n^2)\\) vs \\(O(n^3)\\), \\(O(a^n)\\) vs \\(O(n!)\\) y \\(O(n!)\\) vs \\(O(n^n)\\). Para cada comparaci√≥n, se seleccionar√°n rangos adecuados de \\(n\\) que permitan visualizar claramente las diferencias entre estos √≥rdenes. Se generar√°n gr√°ficas para cada caso y se discutir√°n las observaciones correspondientes. Adem√°s, se incluir√° una tabla con tiempos de ejecuci√≥n simulados para algoritmos ficticios asociados a los √≥rdenes de crecimiento mencionados, utilizando distintos tama√±os de entrada \\(n\\). Este an√°lisis proporciona una visi√≥n clara de c√≥mo los diferentes √≥rdenes de crecimiento afectan el rendimiento y la eficiencia de los algoritmos.\n\n\n\n\n\nimport numpy as np\nimport pandas as pd\nimport math\nimport matplotlib.pyplot as plt\n\n\n\ndef constant(n): return 1\ndef logarithmic(n): return np.log(n)\ndef linear(n): return n\ndef linear_logarithmic(n): return n * np.log(n)\ndef quadratic(n): return n**2\ndef cubic(n): return n**3\ndef exponential(n, a=2): return a**n\ndef factorial(n): return math.factorial(n)\ndef double_exponential(n): return n**n\n\n\n\ndef plot_comparison(funcs, labels, title, x_range):\n    plt.figure(figsize=(7, 5))\n    for func, label in zip(funcs, labels):\n        plt.plot(x_range, [func(x) / 1e9 for x in x_range], label=label)\n    plt.xlabel('n')\n    plt.ylabel('Tiempo (s)')\n    plt.title(title)\n    plt.legend()\n    plt.yscale('log')\n    plt.xscale('log')\n    plt.grid(True)\n    plt.show()\n\n\n\nx_range_small = np.arange(1, 10)\nx_range_medium = np.arange(1, 100)\n\n# Comparaci√≥n 1: O(1) vs O(log n)\nplot_comparison([constant, logarithmic], ['O(1)', 'O(log n)'], 'Figura 1 - Comparaci√≥n de O(1) con O(log n)', x_range_small)\n\n# Comparaci√≥n 2: O(n) vs O(n log n)\nplot_comparison([linear, linear_logarithmic], ['O(n)', 'O(n log n)'], 'Figura 2 - Comparaci√≥n de O(n) con O(n log n)', x_range_medium)\n\n# Comparaci√≥n 3: O(n^2) vs O(n^3)\nplot_comparison([quadratic, cubic], ['O(n^2)', 'O(n^3)'], 'Figura 3 - Comparaci√≥n de O(n^2) con O(n^3)', x_range_medium)\n\n# Comparaci√≥n 4: O(2^n) vs O(n!)\nplot_comparison([lambda n: exponential(n, 2), factorial], ['O(2^n)', 'O(n!)'], 'Figura 4 - Comparaci√≥n de O(2^n) con O(n!)', x_range_small)\n\n# Comparaci√≥n 5: O(n!) vs O(n^n)\nplot_comparison([factorial, double_exponential], ['O(n!)', 'O(n^n)'], 'Figura 5 - Comparaci√≥n de O(n!) con O(n^n)', x_range_small)\n\n\n\nNota: Se cre√≥ una tabla con tiempos de ejecuci√≥n simulados para algoritmos ficticios con los √≥rdenes de crecimiento mencionados.\n# Tama√±os de entrada\nn_values = [100, 1000, 10000, 100000]\n\n# Funciones de costo\ncost_functions = {\n    'O(1)': constant,\n    'O(log n)': logarithmic,\n    'O(n)': linear,\n    'O(n log n)': linear_logarithmic,\n    'O(n^2)': quadratic,\n    'O(n^3)': cubic,\n    'O(2^n)': lambda n: exponential(n, 2),\n    'O(n!)': factorial,\n    'O(n^n)': double_exponential\n}\n\n# Elaboraci√≥n de la tabla\nresults = []\nfor n in n_values:\n    row = {'n': n}\n    for label, func in cost_functions.items():\n        try:\n            row[label] = func(n) / 1e9  # Conversi√≥n a segundos\n        except OverflowError:\n            row[label] = 'Overflow'\n    results.append(row)\n\nimport pandas as pd\ndf = pd.DataFrame(results)\ndf.set_index('n', inplace=True)\ndf\n\n\n\n\n\n\n\n\n\n\n\n\nEl gr√°fico muestra que la funci√≥n constante \\(O(1)\\) permanece constante independientemente del tama√±o de la entrada, mientras que \\(O(\\log n)\\) crece lentamente conforme \\(n\\) aumenta. Por lo tanto, un algoritmo con complejidad constante \\(O(1)\\) es m√°s eficiente en t√©rminos de tiempo de ejecuci√≥n en comparaci√≥n con uno con complejidad logar√≠tmica \\(O(\\log n)\\).\n\n\n\n\n\n\nLa funci√≥n \\(O(n)\\) crece linealmente con el tama√±o de entrada \\(n\\), mientras que la funci√≥n \\(O(n \\log n)\\) crece m√°s r√°pido que \\(O(n)\\), pero sigue siendo pr√°ctica para valores moderados de \\(n\\). Por lo tanto, un algoritmo con complejidad lineal \\(O(n)\\) es m√°s eficiente en t√©rminos de tiempo de ejecuci√≥n en comparaci√≥n con uno con complejidad lineal-logar√≠tmica \\(O(n \\log n)\\).\n\n\n\n\n\n\nLa funci√≥n cuadr√°tica \\(O(n^2)\\) crece m√°s r√°pidamente que la lineal, pero la funci√≥n c√∫bica \\(O(n^3)\\) lo hace a√∫n con mayor rapidez. Por lo tanto, un algoritmo con complejidad cuadr√°tica \\(O(n^2)\\) es m√°s eficiente en t√©rminos de tiempo de ejecuci√≥n en comparaci√≥n con uno con complejidad c√∫bica \\(O(n^3)\\).\n\n\n\n\n\n\nEl gr√°fico muestra que la complejidad factorial \\(O(n!)\\) resulta en tiempos de ejecuci√≥n mucho m√°s altos que \\(O(2^n)\\) a medida que el tama√±o de la entrada \\(n\\) aumenta. La complejidad factorial \\(O(n!)\\) crece considerablemente m√°s r√°pido que la complejidad exponencial \\(O(2^n)\\), lo que hace que los algoritmos con esta complejidad sean pr√°cticamente inutilizables para valores grandes de \\(n\\).\n\n\n\n\n\n\nEl gr√°fico muestra c√≥mo la funci√≥n factorial \\(O(n!)\\) crece muy r√°pidamente, pero la funci√≥n doble exponencial \\(O(n^n)\\) crece a√∫n m√°s r√°pido a medida que aumenta el tama√±o de la entrada \\(n\\). Por lo tanto, los algoritmos con complejidad factorial y doble exponencial son pr√°cticamente inutilizables para valores grandes de \\(n\\).\n\n\n\n\n\nA continuaci√≥n se presenta una tabla comparativa de los tiempos simulados para diferentes √≥rdenes de crecimiento, utilizando distintos tama√±os de entrada \\(n\\). Los resultados se expresan en segundos. En algunos casos, se muestra ‚ÄúOverflow‚Äù cuando el valor resultante excede los l√≠mites de representaci√≥n.\nTabla 1. Tiempos simulados para diferentes √≥rdenes de crecimiento de O(1), O(log n), O(n), O(n log n), O(n¬≤)\n\n\n\n\n\n\n\n\n\n\n\nn\nO(1)\nO(log n)\nO(n)\nO(n log n)\nO(n¬≤)\n\n\n\n\n100\n1.00e-09\n4.61e-09\n1.00e-07\n4.61e-07\n1.00e-05\n\n\n1000\n1.00e-09\n6.91e-09\n1.00e-06\n6.91e-06\n1.00e-03\n\n\n10000\n1.00e-09\n9.21e-09\n1.00e-05\n9.21e-05\n1.00e-01\n\n\n100000\n1.00e-09\n1.15e-08\n1.00e-04\n1.15e-03\n1.00e+01\n\n\n\nTabla 2. Tiempos simulados para diferentes √≥rdenes de crecimiento de O(n¬≥), O(2‚Åø)\n\n\n\nn\nO(n¬≥)\nO(2‚Åø)\n\n\n\n\n100\n1.00e-03\n1.27e+21\n\n\n1000\n1.00e+00\nOverflow\n\n\n10000\n1.00e+03\nOverflow\n\n\n100000\n1.00e+06\nOverflow\n\n\n\nTabla 3. Tiempos simulados para diferentes √≥rdenes de crecimiento de O(n!), O(n‚Åø)\n\n\n\nn\nO(n!)\nO(n‚Åø)\n\n\n\n\n100\nOverflow\nOverflow\n\n\n1000\nOverflow\nOverflow\n\n\n10000\nOverflow\nOverflow\n\n\n100000\nOverflow\nOverflow\n\n\n\n\nNota: Algunos valores como O(2‚Åø), O(n!) y O(n‚Åø) para valores grandes de \\(n\\) generan cantidades inmanejables por el sistema, resultando en Overflow. Esto refleja la inviabilidad pr√°ctica de algoritmos con estas complejidades cuando se trabaja con grandes vol√∫menes de datos.\n\nComo se muestra en la tabla, el tiempo de ejecuci√≥n de O(1) no depende del tama√±o de n.¬†Este mantiene un mismo valor, lo que indica que su tiempo de ejecuci√≥n es constante. Por otro lado, los valores de O(log n) aumentan lentamente a medida que n crece, lo cual es consistente con su comportamiento logar√≠tmico. En el caso de O(n), los valores aumentan de forma lineal conforme n se incrementa. Los valores de O(n log n) aumentan m√°s r√°pido que los de O(n), pero no tan aceleradamente como los de O(n¬≤). Por su parte, O(n¬≤) crece r√°pidamente a medida que n aumenta, y O(n¬≥) lo hace de forma a√∫n m√°s acelerada, incluso con valores peque√±os de entrada.\nDurante la ejecuci√≥n del c√≥digo, algunos resultados aparecen como ‚ÄúOverflow‚Äù. Esto se debe a que las funciones O(2‚Åø), O(n!) y O(n‚Åø) para valores grandes de n (como 10,000 o 100,000) generan n√∫meros extremadamente grandes, lo que excede la capacidad de representaci√≥n y manejo num√©rico en Python.\n\n\n\n\nLa manipulaci√≥n de grandes vol√∫menes de informaci√≥n presenta importantes desaf√≠os debido a los costos de c√≥mputo. Estos costos se relacionan con el gasto monetario, los recursos computacionales requeridos, el tiempo de procesamiento, el uso de memoria, el almacenamiento y el consumo energ√©tico. A continuaci√≥n se presentan algunas de las implicaciones m√°s relevantes:\n\nInfraestructura: Manipular grandes vol√∫menes de datos requiere una infraestructura robusta, que incluye servidores potentes, almacenamiento masivo y redes de alta velocidad. Esto representa costos elevados en hardware, mantenimiento y consumo energ√©tico (Armbrust et al., 2010).\nEnerg√≠a: Los centros de datos que procesan grandes cantidades de informaci√≥n consumen enormes cantidades de energ√≠a, lo cual incrementa los costos operativos y contribuye al impacto ambiental debido a las emisiones de carbono (Baliga et al., 2011).\nAlmacenamiento: El almacenamiento de grandes vol√∫menes de datos requiere soluciones escalables y eficientes. Este aspecto representa un costo considerable, ya que tambi√©n debe garantizarse la recuperaci√≥n oportuna de la informaci√≥n.\nProcesamiento: Procesar datos en tiempo real o en tiempo cercano al real implica el uso de algoritmos eficientes y de recursos de c√≥mputo significativos. Estos costos pueden incrementarse especialmente al utilizar tecnolog√≠as como el aprendizaje autom√°tico y la inteligencia artificial (Wang et al., 2015).\n\nEn conjunto, la gesti√≥n de grandes vol√∫menes de informaci√≥n conlleva costos considerables en t√©rminos de infraestructura, energ√≠a, almacenamiento, procesamiento, seguridad y mantenimiento. Estos aspectos deben ser cuidadosamente planeados y gestionados para asegurar soluciones de big data eficientes y sostenibles.\n\n\n\n\nEn las simulaciones realizadas se observ√≥ que los √≥rdenes de crecimiento m√°s bajos, como O(1), O(log n) y O(n), resultan altamente eficientes. Son ideales para tareas que demandan rendimiento √≥ptimo y tiempos de respuesta reducidos.\nPor otro lado, los √≥rdenes de crecimiento O(n log n) y O(n¬≤) demostraron ser pr√°cticas y ampliamente aplicables, al ofrecer un buen equilibrio entre eficiencia y capacidad de procesamiento.\nEn contraste, las funciones con √≥rdenes de crecimiento elevados como O(2‚Åø), O(n!) y O(n‚Åø) resultaron ineficientes, generando tiempos de ejecuci√≥n excesivos o incluso errores de memoria.\nLa tabla de tiempos de ejecuci√≥n simulados confirma c√≥mo el crecimiento del orden afecta directamente el rendimiento de los algoritmos. A mayor orden, mayor consumo de recursos y tiempo; a menor orden, mayor eficiencia en la ejecuci√≥n.\n\n\n\n\nM√ºller, V. C., Schal, J. M., Meyer-Lindenberg, A. (2016). Machine Learning for Brain Imaging. Cambridge University Press.\nManyika, J., Chui, M., Brown, B., Bughin, J., Dobbs, R., Roxburgh, C., & Byers, A. H. (2011). Big data: The next frontier for innovation, competition, and productivity. McKinsey Global Institute. https://www.mckinsey.com/business-functions/mckinsey-analytics/our-insights/big-data-the-next-frontier-for-innovation\nArmbrust, M., Fox, A., Griffith, R., Joseph, A. D., Katz, R. H., Konwinski, A., et al.¬†(2010). A View of Cloud Computing. Communications of the ACM, 53(4), 50‚Äì58. https://doi.org/10.1145/1721654.1721672\nBaliga, J., Ayre, R., Hinton, K., & Tucker, R. S. (2011). Energy-Efficient Telecommunications.\nWang, L., Chen, Y., & Liu, Q. (2015). Big Data Processing: A Survey. Springer.\n\n\n\n\n\n\nLos siguientes ajustes fueron aplicados al presente documento en atenci√≥n a la retroalimentaci√≥n recibida por parte del docente. Se atendieron los puntos se√±alados con el fin de mejorar la calidad formal y acad√©mica del reporte:\n\nSe uniform√≥ el formato del documento, manteniendo una estructura clara y coherente entre secciones.\nSe corrigieron errores ortogr√°ficos y gramaticales, incluyendo el uso adecuado de comas y acentos.\nLas expresiones matem√°ticas fueron reescritas utilizando la notaci√≥n correcta en formato de ecuaci√≥n (LaTeX), como es el caso de \\(O(1)\\), \\(O(\\log n)\\), \\(O(n^2)\\), entre otras.\nSe estandarizaron los encabezados de cada secci√≥n para facilitar la lectura y navegaci√≥n del documento.\n\nEstas modificaciones aseguran que el trabajo cumpla con los criterios de presentaci√≥n solicitados y refleje un esfuerzo riguroso en la construcci√≥n y exposici√≥n del contenido."
  },
  {
    "objectID": "project1.html#introducci√≥n",
    "href": "project1.html#introducci√≥n",
    "title": "Proyecto 1",
    "section": "",
    "text": "En la era digital actual, el big data ha emergido como un recurso valioso para las organizaciones, permiti√©ndoles extraer, procesar y analizar datos significativos. El big data se caracteriza por sus cuatro V: volumen, velocidad, variedad y veracidad. El volumen se refiere a la gran cantidad de datos generados y almacenados; la velocidad, a la rapidez con la que se generan y procesan; la variedad, a los diferentes tipos de datos (estructurados y no estructurados); y la veracidad, a la calidad y precisi√≥n de dichos datos. La capacidad de manejar y analizar estos grandes vol√∫menes de informaci√≥n de manera eficiente es crucial para aprovechar al m√°ximo el potencial del big data (M√ºller et al., 2016).\nEl an√°lisis de big data permite a las organizaciones identificar patrones, predecir tendencias y optimizar procesos, lo que puede resultar en mejoras significativas en la eficiencia operativa y en la toma de decisiones estrat√©gicas. Sin embargo, la manipulaci√≥n de grandes vol√∫menes de datos presenta desaf√≠os importantes en t√©rminos de infraestructura, almacenamiento, procesamiento y an√°lisis. Los algoritmos eficientes son fundamentales para enfrentar estos desaf√≠os y garantizar que los sistemas de big data funcionen de manera √≥ptima (Manyika et al., 2011).\nEste reporte se enfoca en comparar diferentes √≥rdenes de crecimiento mediante simulaciones en un entorno de Jupyter. Se analizar√°n los siguientes casos: \\(O(1)\\) vs \\(O(\\log n)\\), \\(O(n)\\) vs \\(O(n \\log n)\\), \\(O(n^2)\\) vs \\(O(n^3)\\), \\(O(a^n)\\) vs \\(O(n!)\\) y \\(O(n!)\\) vs \\(O(n^n)\\). Para cada comparaci√≥n, se seleccionar√°n rangos adecuados de \\(n\\) que permitan visualizar claramente las diferencias entre estos √≥rdenes. Se generar√°n gr√°ficas para cada caso y se discutir√°n las observaciones correspondientes. Adem√°s, se incluir√° una tabla con tiempos de ejecuci√≥n simulados para algoritmos ficticios asociados a los √≥rdenes de crecimiento mencionados, utilizando distintos tama√±os de entrada \\(n\\). Este an√°lisis proporciona una visi√≥n clara de c√≥mo los diferentes √≥rdenes de crecimiento afectan el rendimiento y la eficiencia de los algoritmos."
  },
  {
    "objectID": "project1.html#desarrollo",
    "href": "project1.html#desarrollo",
    "title": "Proyecto 1",
    "section": "",
    "text": "import numpy as np\nimport pandas as pd\nimport math\nimport matplotlib.pyplot as plt\n\n\n\ndef constant(n): return 1\ndef logarithmic(n): return np.log(n)\ndef linear(n): return n\ndef linear_logarithmic(n): return n * np.log(n)\ndef quadratic(n): return n**2\ndef cubic(n): return n**3\ndef exponential(n, a=2): return a**n\ndef factorial(n): return math.factorial(n)\ndef double_exponential(n): return n**n\n\n\n\ndef plot_comparison(funcs, labels, title, x_range):\n    plt.figure(figsize=(7, 5))\n    for func, label in zip(funcs, labels):\n        plt.plot(x_range, [func(x) / 1e9 for x in x_range], label=label)\n    plt.xlabel('n')\n    plt.ylabel('Tiempo (s)')\n    plt.title(title)\n    plt.legend()\n    plt.yscale('log')\n    plt.xscale('log')\n    plt.grid(True)\n    plt.show()\n\n\n\nx_range_small = np.arange(1, 10)\nx_range_medium = np.arange(1, 100)\n\n# Comparaci√≥n 1: O(1) vs O(log n)\nplot_comparison([constant, logarithmic], ['O(1)', 'O(log n)'], 'Figura 1 - Comparaci√≥n de O(1) con O(log n)', x_range_small)\n\n# Comparaci√≥n 2: O(n) vs O(n log n)\nplot_comparison([linear, linear_logarithmic], ['O(n)', 'O(n log n)'], 'Figura 2 - Comparaci√≥n de O(n) con O(n log n)', x_range_medium)\n\n# Comparaci√≥n 3: O(n^2) vs O(n^3)\nplot_comparison([quadratic, cubic], ['O(n^2)', 'O(n^3)'], 'Figura 3 - Comparaci√≥n de O(n^2) con O(n^3)', x_range_medium)\n\n# Comparaci√≥n 4: O(2^n) vs O(n!)\nplot_comparison([lambda n: exponential(n, 2), factorial], ['O(2^n)', 'O(n!)'], 'Figura 4 - Comparaci√≥n de O(2^n) con O(n!)', x_range_small)\n\n# Comparaci√≥n 5: O(n!) vs O(n^n)\nplot_comparison([factorial, double_exponential], ['O(n!)', 'O(n^n)'], 'Figura 5 - Comparaci√≥n de O(n!) con O(n^n)', x_range_small)\n\n\n\nNota: Se cre√≥ una tabla con tiempos de ejecuci√≥n simulados para algoritmos ficticios con los √≥rdenes de crecimiento mencionados.\n# Tama√±os de entrada\nn_values = [100, 1000, 10000, 100000]\n\n# Funciones de costo\ncost_functions = {\n    'O(1)': constant,\n    'O(log n)': logarithmic,\n    'O(n)': linear,\n    'O(n log n)': linear_logarithmic,\n    'O(n^2)': quadratic,\n    'O(n^3)': cubic,\n    'O(2^n)': lambda n: exponential(n, 2),\n    'O(n!)': factorial,\n    'O(n^n)': double_exponential\n}\n\n# Elaboraci√≥n de la tabla\nresults = []\nfor n in n_values:\n    row = {'n': n}\n    for label, func in cost_functions.items():\n        try:\n            row[label] = func(n) / 1e9  # Conversi√≥n a segundos\n        except OverflowError:\n            row[label] = 'Overflow'\n    results.append(row)\n\nimport pandas as pd\ndf = pd.DataFrame(results)\ndf.set_index('n', inplace=True)\ndf"
  },
  {
    "objectID": "project1.html#an√°lisis-de-resultados",
    "href": "project1.html#an√°lisis-de-resultados",
    "title": "Proyecto 1",
    "section": "",
    "text": "El gr√°fico muestra que la funci√≥n constante \\(O(1)\\) permanece constante independientemente del tama√±o de la entrada, mientras que \\(O(\\log n)\\) crece lentamente conforme \\(n\\) aumenta. Por lo tanto, un algoritmo con complejidad constante \\(O(1)\\) es m√°s eficiente en t√©rminos de tiempo de ejecuci√≥n en comparaci√≥n con uno con complejidad logar√≠tmica \\(O(\\log n)\\).\n\n\n\n\n\n\nLa funci√≥n \\(O(n)\\) crece linealmente con el tama√±o de entrada \\(n\\), mientras que la funci√≥n \\(O(n \\log n)\\) crece m√°s r√°pido que \\(O(n)\\), pero sigue siendo pr√°ctica para valores moderados de \\(n\\). Por lo tanto, un algoritmo con complejidad lineal \\(O(n)\\) es m√°s eficiente en t√©rminos de tiempo de ejecuci√≥n en comparaci√≥n con uno con complejidad lineal-logar√≠tmica \\(O(n \\log n)\\).\n\n\n\n\n\n\nLa funci√≥n cuadr√°tica \\(O(n^2)\\) crece m√°s r√°pidamente que la lineal, pero la funci√≥n c√∫bica \\(O(n^3)\\) lo hace a√∫n con mayor rapidez. Por lo tanto, un algoritmo con complejidad cuadr√°tica \\(O(n^2)\\) es m√°s eficiente en t√©rminos de tiempo de ejecuci√≥n en comparaci√≥n con uno con complejidad c√∫bica \\(O(n^3)\\).\n\n\n\n\n\n\nEl gr√°fico muestra que la complejidad factorial \\(O(n!)\\) resulta en tiempos de ejecuci√≥n mucho m√°s altos que \\(O(2^n)\\) a medida que el tama√±o de la entrada \\(n\\) aumenta. La complejidad factorial \\(O(n!)\\) crece considerablemente m√°s r√°pido que la complejidad exponencial \\(O(2^n)\\), lo que hace que los algoritmos con esta complejidad sean pr√°cticamente inutilizables para valores grandes de \\(n\\).\n\n\n\n\n\n\nEl gr√°fico muestra c√≥mo la funci√≥n factorial \\(O(n!)\\) crece muy r√°pidamente, pero la funci√≥n doble exponencial \\(O(n^n)\\) crece a√∫n m√°s r√°pido a medida que aumenta el tama√±o de la entrada \\(n\\). Por lo tanto, los algoritmos con complejidad factorial y doble exponencial son pr√°cticamente inutilizables para valores grandes de \\(n\\).\n\n\n\n\n\nA continuaci√≥n se presenta una tabla comparativa de los tiempos simulados para diferentes √≥rdenes de crecimiento, utilizando distintos tama√±os de entrada \\(n\\). Los resultados se expresan en segundos. En algunos casos, se muestra ‚ÄúOverflow‚Äù cuando el valor resultante excede los l√≠mites de representaci√≥n.\nTabla 1. Tiempos simulados para diferentes √≥rdenes de crecimiento de O(1), O(log n), O(n), O(n log n), O(n¬≤)\n\n\n\n\n\n\n\n\n\n\n\nn\nO(1)\nO(log n)\nO(n)\nO(n log n)\nO(n¬≤)\n\n\n\n\n100\n1.00e-09\n4.61e-09\n1.00e-07\n4.61e-07\n1.00e-05\n\n\n1000\n1.00e-09\n6.91e-09\n1.00e-06\n6.91e-06\n1.00e-03\n\n\n10000\n1.00e-09\n9.21e-09\n1.00e-05\n9.21e-05\n1.00e-01\n\n\n100000\n1.00e-09\n1.15e-08\n1.00e-04\n1.15e-03\n1.00e+01\n\n\n\nTabla 2. Tiempos simulados para diferentes √≥rdenes de crecimiento de O(n¬≥), O(2‚Åø)\n\n\n\nn\nO(n¬≥)\nO(2‚Åø)\n\n\n\n\n100\n1.00e-03\n1.27e+21\n\n\n1000\n1.00e+00\nOverflow\n\n\n10000\n1.00e+03\nOverflow\n\n\n100000\n1.00e+06\nOverflow\n\n\n\nTabla 3. Tiempos simulados para diferentes √≥rdenes de crecimiento de O(n!), O(n‚Åø)\n\n\n\nn\nO(n!)\nO(n‚Åø)\n\n\n\n\n100\nOverflow\nOverflow\n\n\n1000\nOverflow\nOverflow\n\n\n10000\nOverflow\nOverflow\n\n\n100000\nOverflow\nOverflow\n\n\n\n\nNota: Algunos valores como O(2‚Åø), O(n!) y O(n‚Åø) para valores grandes de \\(n\\) generan cantidades inmanejables por el sistema, resultando en Overflow. Esto refleja la inviabilidad pr√°ctica de algoritmos con estas complejidades cuando se trabaja con grandes vol√∫menes de datos.\n\nComo se muestra en la tabla, el tiempo de ejecuci√≥n de O(1) no depende del tama√±o de n.¬†Este mantiene un mismo valor, lo que indica que su tiempo de ejecuci√≥n es constante. Por otro lado, los valores de O(log n) aumentan lentamente a medida que n crece, lo cual es consistente con su comportamiento logar√≠tmico. En el caso de O(n), los valores aumentan de forma lineal conforme n se incrementa. Los valores de O(n log n) aumentan m√°s r√°pido que los de O(n), pero no tan aceleradamente como los de O(n¬≤). Por su parte, O(n¬≤) crece r√°pidamente a medida que n aumenta, y O(n¬≥) lo hace de forma a√∫n m√°s acelerada, incluso con valores peque√±os de entrada.\nDurante la ejecuci√≥n del c√≥digo, algunos resultados aparecen como ‚ÄúOverflow‚Äù. Esto se debe a que las funciones O(2‚Åø), O(n!) y O(n‚Åø) para valores grandes de n (como 10,000 o 100,000) generan n√∫meros extremadamente grandes, lo que excede la capacidad de representaci√≥n y manejo num√©rico en Python.\n\n\n\n\nLa manipulaci√≥n de grandes vol√∫menes de informaci√≥n presenta importantes desaf√≠os debido a los costos de c√≥mputo. Estos costos se relacionan con el gasto monetario, los recursos computacionales requeridos, el tiempo de procesamiento, el uso de memoria, el almacenamiento y el consumo energ√©tico. A continuaci√≥n se presentan algunas de las implicaciones m√°s relevantes:\n\nInfraestructura: Manipular grandes vol√∫menes de datos requiere una infraestructura robusta, que incluye servidores potentes, almacenamiento masivo y redes de alta velocidad. Esto representa costos elevados en hardware, mantenimiento y consumo energ√©tico (Armbrust et al., 2010).\nEnerg√≠a: Los centros de datos que procesan grandes cantidades de informaci√≥n consumen enormes cantidades de energ√≠a, lo cual incrementa los costos operativos y contribuye al impacto ambiental debido a las emisiones de carbono (Baliga et al., 2011).\nAlmacenamiento: El almacenamiento de grandes vol√∫menes de datos requiere soluciones escalables y eficientes. Este aspecto representa un costo considerable, ya que tambi√©n debe garantizarse la recuperaci√≥n oportuna de la informaci√≥n.\nProcesamiento: Procesar datos en tiempo real o en tiempo cercano al real implica el uso de algoritmos eficientes y de recursos de c√≥mputo significativos. Estos costos pueden incrementarse especialmente al utilizar tecnolog√≠as como el aprendizaje autom√°tico y la inteligencia artificial (Wang et al., 2015).\n\nEn conjunto, la gesti√≥n de grandes vol√∫menes de informaci√≥n conlleva costos considerables en t√©rminos de infraestructura, energ√≠a, almacenamiento, procesamiento, seguridad y mantenimiento. Estos aspectos deben ser cuidadosamente planeados y gestionados para asegurar soluciones de big data eficientes y sostenibles."
  },
  {
    "objectID": "project1.html#conclusiones",
    "href": "project1.html#conclusiones",
    "title": "Proyecto 1",
    "section": "",
    "text": "En las simulaciones realizadas se observ√≥ que los √≥rdenes de crecimiento m√°s bajos, como O(1), O(log n) y O(n), resultan altamente eficientes. Son ideales para tareas que demandan rendimiento √≥ptimo y tiempos de respuesta reducidos.\nPor otro lado, los √≥rdenes de crecimiento O(n log n) y O(n¬≤) demostraron ser pr√°cticas y ampliamente aplicables, al ofrecer un buen equilibrio entre eficiencia y capacidad de procesamiento.\nEn contraste, las funciones con √≥rdenes de crecimiento elevados como O(2‚Åø), O(n!) y O(n‚Åø) resultaron ineficientes, generando tiempos de ejecuci√≥n excesivos o incluso errores de memoria.\nLa tabla de tiempos de ejecuci√≥n simulados confirma c√≥mo el crecimiento del orden afecta directamente el rendimiento de los algoritmos. A mayor orden, mayor consumo de recursos y tiempo; a menor orden, mayor eficiencia en la ejecuci√≥n."
  },
  {
    "objectID": "project1.html#referencias",
    "href": "project1.html#referencias",
    "title": "Proyecto 1",
    "section": "",
    "text": "M√ºller, V. C., Schal, J. M., Meyer-Lindenberg, A. (2016). Machine Learning for Brain Imaging. Cambridge University Press.\nManyika, J., Chui, M., Brown, B., Bughin, J., Dobbs, R., Roxburgh, C., & Byers, A. H. (2011). Big data: The next frontier for innovation, competition, and productivity. McKinsey Global Institute. https://www.mckinsey.com/business-functions/mckinsey-analytics/our-insights/big-data-the-next-frontier-for-innovation\nArmbrust, M., Fox, A., Griffith, R., Joseph, A. D., Katz, R. H., Konwinski, A., et al.¬†(2010). A View of Cloud Computing. Communications of the ACM, 53(4), 50‚Äì58. https://doi.org/10.1145/1721654.1721672\nBaliga, J., Ayre, R., Hinton, K., & Tucker, R. S. (2011). Energy-Efficient Telecommunications.\nWang, L., Chen, Y., & Liu, Q. (2015). Big Data Processing: A Survey. Springer."
  },
  {
    "objectID": "project1.html#cambios-realizados",
    "href": "project1.html#cambios-realizados",
    "title": "Proyecto 1",
    "section": "",
    "text": "Los siguientes ajustes fueron aplicados al presente documento en atenci√≥n a la retroalimentaci√≥n recibida por parte del docente. Se atendieron los puntos se√±alados con el fin de mejorar la calidad formal y acad√©mica del reporte:\n\nSe uniform√≥ el formato del documento, manteniendo una estructura clara y coherente entre secciones.\nSe corrigieron errores ortogr√°ficos y gramaticales, incluyendo el uso adecuado de comas y acentos.\nLas expresiones matem√°ticas fueron reescritas utilizando la notaci√≥n correcta en formato de ecuaci√≥n (LaTeX), como es el caso de \\(O(1)\\), \\(O(\\log n)\\), \\(O(n^2)\\), entre otras.\nSe estandarizaron los encabezados de cada secci√≥n para facilitar la lectura y navegaci√≥n del documento.\n\nEstas modificaciones aseguran que el trabajo cumpla con los criterios de presentaci√≥n solicitados y refleje un esfuerzo riguroso en la construcci√≥n y exposici√≥n del contenido."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Sobre M√≠",
    "section": "",
    "text": "üìç Aguascalientes, M√©xico\n‚úâÔ∏è juan2javm@gmail.com / jvelasquez1800@alumno.ipn.mx\nüì± +52 322 353 4081\nüîó GitHub: juan21javm\nüîó LinkedIn: antonio-mart√≠nez-776788179\n\n\n\nMe considero una persona profundamente comprometida con el crecimiento intelectual, la lectura reflexiva y el bienestar de mi entorno cercano. Disfruto de los momentos al aire libre, donde encuentro inspiraci√≥n en la naturaleza y la tranquilidad necesaria para mantener un equilibrio emocional. Valoro profundamente el tiempo con mi familia, ya que es el motor que impulsa mi disciplina, responsabilidad y deseo de superaci√≥n constante. Asimismo, tengo una gran pasi√≥n por la programaci√≥n y un genuino entusiasmo por la ciencia de datos, √°reas que me permiten combinar creatividad, l√≥gica y an√°lisis riguroso para resolver problemas del mundo real. Estos principios personales complementan mi formaci√≥n acad√©mica y profesional, motiv√°ndome a mantener siempre una actitud proactiva, √©tica y humana.\n\n\n\n\nSupervisor ‚Äî INE (2023‚Äì2025), Loreto, Zacatecas\n- Supervisi√≥n de actividades diarias para cumplimiento de objetivos.\n- Capacitaci√≥n del equipo y mejora de rendimiento.\n- Evaluaci√≥n de desempe√±o y optimizaci√≥n de procesos.\nMaestro Asistente ‚Äî IPN (2022‚Äì2023), Zacatecas\n- Evaluaci√≥n del progreso estudiantil.\n- Planeaci√≥n conjunta de actividades educativas.\n- Creaci√≥n y adaptaci√≥n de materiales educativos.\n\n\n\n\nProyecto A ‚Äî Estandarizaci√≥n de proceso a nivel laboratorio (2021‚Äì2023)\n- Herramientas: MATLAB, HACCP Software, Latex\n- Liderazgo de investigaci√≥n para producci√≥n de yogurt con uvas a nivel laboratorio.\n- Reconocimiento al mejor promedio del programa acad√©mico (IPN 2017‚Äì2022).\n- Primer lugar en Simposium Agroalimentario y Ciclo de Conferencias CUALTIA 2023.\n\n\n\n\n\nLenguajes: Python, JAVA, HTML, ASPEN\n\nHerramientas Matem√°ticas: MATLAB, R, SPSS, PLC\n\nOfim√°tica: Office, LaTeX\n\n\n\n\n\n\nIdiomas: Espa√±ol (nativo), Ingl√©s (B1)\n\nIntereses: Lectura, b√∫squeda, planeaci√≥n"
  },
  {
    "objectID": "about.html#perfil-personal",
    "href": "about.html#perfil-personal",
    "title": "Sobre M√≠",
    "section": "",
    "text": "Me considero una persona profundamente comprometida con el crecimiento intelectual, la lectura reflexiva y el bienestar de mi entorno cercano. Disfruto de los momentos al aire libre, donde encuentro inspiraci√≥n en la naturaleza y la tranquilidad necesaria para mantener un equilibrio emocional. Valoro profundamente el tiempo con mi familia, ya que es el motor que impulsa mi disciplina, responsabilidad y deseo de superaci√≥n constante. Asimismo, tengo una gran pasi√≥n por la programaci√≥n y un genuino entusiasmo por la ciencia de datos, √°reas que me permiten combinar creatividad, l√≥gica y an√°lisis riguroso para resolver problemas del mundo real. Estos principios personales complementan mi formaci√≥n acad√©mica y profesional, motiv√°ndome a mantener siempre una actitud proactiva, √©tica y humana."
  },
  {
    "objectID": "about.html#experiencia",
    "href": "about.html#experiencia",
    "title": "Sobre M√≠",
    "section": "",
    "text": "Supervisor ‚Äî INE (2023‚Äì2025), Loreto, Zacatecas\n- Supervisi√≥n de actividades diarias para cumplimiento de objetivos.\n- Capacitaci√≥n del equipo y mejora de rendimiento.\n- Evaluaci√≥n de desempe√±o y optimizaci√≥n de procesos.\nMaestro Asistente ‚Äî IPN (2022‚Äì2023), Zacatecas\n- Evaluaci√≥n del progreso estudiantil.\n- Planeaci√≥n conjunta de actividades educativas.\n- Creaci√≥n y adaptaci√≥n de materiales educativos."
  },
  {
    "objectID": "about.html#proyectos-conferencias-y-reconocimientos",
    "href": "about.html#proyectos-conferencias-y-reconocimientos",
    "title": "Sobre M√≠",
    "section": "",
    "text": "Proyecto A ‚Äî Estandarizaci√≥n de proceso a nivel laboratorio (2021‚Äì2023)\n- Herramientas: MATLAB, HACCP Software, Latex\n- Liderazgo de investigaci√≥n para producci√≥n de yogurt con uvas a nivel laboratorio.\n- Reconocimiento al mejor promedio del programa acad√©mico (IPN 2017‚Äì2022).\n- Primer lugar en Simposium Agroalimentario y Ciclo de Conferencias CUALTIA 2023."
  },
  {
    "objectID": "about.html#habilidades",
    "href": "about.html#habilidades",
    "title": "Sobre M√≠",
    "section": "",
    "text": "Lenguajes: Python, JAVA, HTML, ASPEN\n\nHerramientas Matem√°ticas: MATLAB, R, SPSS, PLC\n\nOfim√°tica: Office, LaTeX"
  },
  {
    "objectID": "about.html#informaci√≥n-adicional",
    "href": "about.html#informaci√≥n-adicional",
    "title": "Sobre M√≠",
    "section": "",
    "text": "Idiomas: Espa√±ol (nativo), Ingl√©s (B1)\n\nIntereses: Lectura, b√∫squeda, planeaci√≥n"
  },
  {
    "objectID": "index.html#presentaci√≥n-del-proyecto",
    "href": "index.html#presentaci√≥n-del-proyecto",
    "title": "Centro de Investigaci√≥n e Innovaci√≥n en Tecnolog√≠as de la Informaci√≥n y Comunicaci√≥n",
    "section": "Presentaci√≥n del proyecto",
    "text": "Presentaci√≥n del proyecto\nEn esta p√°gina se encuentran reflejados los cinco reportes desarrollados a lo largo de la asignatura de An√°lisis de Algoritmos. Cada uno de ellos ha sido debidamente documentado, estructurado y trasladado al formato Quarto con el prop√≥sito de comunicar al p√∫blico el trabajo realizado y los an√°lisis efectuados durante mi estancia en la materia. Estos reportes representan el proceso de aprendizaje y aplicaci√≥n pr√°ctica de los conceptos clave abordados en cada unidad, desde la introducci√≥n al an√°lisis algor√≠tmico hasta los algoritmos de intersecci√≥n de conjuntos, permitiendo evidenciar el desarrollo de competencias t√©cnicas y anal√≠ticas fundamentales en el √°rea.\nEsta documentaci√≥n ha sido preparada para su publicaci√≥n en un repositorio de GitHub con el objetivo de compartir de forma abierta los contenidos desarrollados, promover el acceso al conocimiento, y facilitar su consulta por parte de docentes, estudiantes y profesionales interesados en el an√°lisis de algoritmos.\nA lo largo del curso se desarrollaron cinco tareas escritas que reflejan los temas fundamentales abordados en cada unidad:\n\nUnidad 1: Introducci√≥n al an√°lisis de algoritmos\nSe realiz√≥ el reporte 1A. Reporte escrito. Experimentos y an√°lisis, en el que se exploraron conceptos b√°sicos sobre la eficiencia algor√≠tmica y √≥rdenes de crecimiento.\nUnidad 2: Estructuras de datos\nSe trabaj√≥ el reporte 2A. Reporte escrito. Experimentos y an√°lisis de estructuras de datos, enfocado en el comportamiento, manipulaci√≥n y an√°lisis de distintas estructuras como listas, pilas, colas y √°rboles.\nUnidad 3: Algoritmos de ordenamiento por comparaci√≥n\nSe elabor√≥ el 3A. Reporte escrito. Experimentos y an√°lisis de algoritmos de ordenamiento, donde se evaluaron m√©todos como burbuja, inserci√≥n, selecci√≥n, quicksort y mergesort.\nUnidad 4: Algoritmos de b√∫squeda por comparaci√≥n\nSe desarroll√≥ el reporte 4A. Reporte escrito. Experimentos y an√°lisis de algoritmos de b√∫squeda por comparaci√≥n, abordando t√©cnicas como la b√∫squeda lineal y binaria, con un enfoque en su eficiencia.\nUnidad 5: Algoritmos de intersecci√≥n y uni√≥n de conjuntos en el modelo de comparaci√≥n\nSe present√≥ el 5A. Reporte escrito. Experimentos y an√°lisis de algoritmos de intersecci√≥n de conjuntos, donde se analizaron distintas estrategias para operar sobre m√∫ltiples listas ordenadas."
  },
  {
    "objectID": "project2.html",
    "href": "project2.html",
    "title": "Proyecto 2",
    "section": "",
    "text": "Los experimentos y an√°lisis de estructuras de datos son fundamentales en la ciencia de datos, ya que permiten evaluar y optimizar el rendimiento de algoritmos y sistemas que manejan grandes vol√∫menes de informaci√≥n.\nLas estructuras de datos son formas de organizar y almacenar datos en una computadora para que puedan ser accedidos y modificados de manera eficiente. Cada estructura tiene sus propias ventajas y desventajas en t√©rminos de tiempo de acceso, uso de memoria y facilidad de implementaci√≥n. Los experimentos en estructuras de datos suelen centrarse en medir estos aspectos bajo diferentes condiciones para determinar cu√°l es la m√°s adecuada para una aplicaci√≥n espec√≠fica (Goodfellow et al., 2016).\nEl dise√±o experimental es una etapa esencial que precede al an√°lisis de datos. Un dise√±o bien planificado asegura que los datos recopilados sean relevantes y √∫tiles para el an√°lisis posterior. Esto incluye la definici√≥n de variables, la selecci√≥n de muestras y la implementaci√≥n de controles para minimizar sesgos. Estas estructuras son esenciales para el desarrollo de software eficiente, especialmente en campos como el Internet de las Cosas (IoT) y el Big Data, donde la optimizaci√≥n de recursos es crucial (Strang, 2016).\nEl an√°lisis de datos implica varias t√©cnicas estad√≠sticas y computacionales para interpretar los resultados experimentales. Esto puede incluir an√°lisis exploratorio de datos (EDA) para identificar patrones y relaciones, as√≠ como an√°lisis confirmatorio para probar hip√≥tesis espec√≠ficas. Herramientas como Hadoop, Spark, Power BI, Pandas y R son com√∫nmente utilizadas en este proceso (Grcar, 2011).\nLa multiplicaci√≥n de matrices es una operaci√≥n b√°sica que se utiliza en numerosos c√°lculos matem√°ticos y algoritmos computacionales. Su relevancia radica en su capacidad para transformar y combinar datos de manera que se puedan extraer insights significativos. La eliminaci√≥n gaussiana es un m√©todo cl√°sico para resolver sistemas de ecuaciones lineales, transformando una matriz en una forma escalonada reducida por filas. Este m√©todo es ampliamente utilizado debido a su estabilidad num√©rica y su capacidad para manejar sistemas grandes.\nEl objetivo es entender c√≥mo se comportan estos algoritmos en diferentes situaciones, especialmente en cuanto al n√∫mero de operaciones (como multiplicaciones y sumas) y el tiempo que tardan en ejecutarse en la pr√°ctica. Para abordar esta cuesti√≥n, se plantea un estudio comparativo utilizando matrices aleatorias de tama√±o \\(n \\times n\\) para \\(n = 100\\), \\(300\\), \\(1000\\). Este enfoque permitir√° evaluar la eficiencia de cada algoritmo y proporcionar√° una base s√≥lida para futuras optimizaciones. Al comparar estos algoritmos, se busca responder a preguntas clave como: ¬øQu√© puedes concluir?, ¬øCu√°l es el impacto de acceder a los elementos contiguos en memoria de una matriz?, ¬øQu√© cambiar√≠as si utilizas matrices dispersas?, y ¬øCu√°les ser√≠an los costos?\n\n\n\n\n\nimport numpy as np\nimport time\n\n\n\ndef matrix_multiplication(A, B):\n    n = len(A)  # Tama√±o de la matriz (n x n)\n    C = np.zeros((n, n))  # Inicializar la matriz resultante con ceros\n    operations = 0  # Contador de operaciones\n\n    for i in range(n):\n        for j in range(n):\n            for k in range(n):\n                C[i][j] += A[i][k] * B[k][j]  # Multiplicaci√≥n y suma\n                operations += 2  # Una multiplicaci√≥n y una suma\n\n    return C, operations\n\n\n\ndef gauss_jordan(A):\n    n = len(A)\n    operations = 0\n    A = np.array(A, dtype=float)\n\n    for i in range(n):\n        if A[i][i] == 0:\n            for j in range(i+1, n):\n                if A[j][i] != 0:\n                    A[[i, j]] = A[[j, i]]  # Intercambio de filas\n                    operations += 1\n                    break\n\n        div = A[i][i]\n        A[i] = A[i] / div\n        operations += n  # n divisiones\n\n        for j in range(n):\n            if i != j:\n                A[j] = A[j] - A[j][i] * A[i]\n                operations += n  # n multiplicaciones y restas\n\n    return A, operations\n\n\n\nsizes = [100, 300, 1000]\nresults = []\n\nfor n in sizes:\n    A = np.random.rand(n, n)\n    B = np.random.rand(n, n)\n\n    # Multiplicaci√≥n de matrices\n    start_time = time.time()\n    C, mult_operations = matrix_multiplication(A, B)\n    mult_time = time.time() - start_time\n\n    # Eliminaci√≥n Gaussiana\n    start_time = time.time()\n    _, gauss_operations = gauss_jordan(A)\n    gauss_time = time.time() - start_time\n\n    results.append({\n        'n': n,\n        'mult_operations': mult_operations,\n        'mult_time': mult_time,\n        'gauss_operations': gauss_operations,\n        'gauss_time': gauss_time\n    })\n\n\n\nfor result in results:\n    print(f\"Tama√±o de la matriz (n): {result['n']}\")\n    print(\" Multiplicaci√≥n de Matrices:\")\n    print(f\" Operaciones: {result['mult_operations']}\")\n    print(f\" Tiempo (s): {result['mult_time']:.6f}\")\n    print(\" Eliminaci√≥n Gaussiana:\")\n    print(f\" Operaciones: {result['gauss_operations']}\")\n    print(f\" Tiempo (s): {result['gauss_time']:.6f}\")\n    print(\"-\" * 40)\n\n\n\n\n\n\nA continuaci√≥n se muestra una tabla con los resultados obtenidos al comparar la multiplicaci√≥n de matrices y la eliminaci√≥n Gauss-Jordan sobre matrices de distintos tama√±os. Se reporta el n√∫mero total de operaciones y el tiempo de ejecuci√≥n en segundos.\n\n\n\n\n\n\n\n\n\n\nTama√±o (n)\nOperaciones Multiplicaci√≥n\nTiempo Multiplicaci√≥n (s)\nOperaciones Gauss\nTiempo Gauss (s)\n\n\n\n\n100\n2,000,000\n1.829321\n1,000,000\n0.066816\n\n\n300\n54,000,000\n30.990801\n27,000,000\n0.399951\n\n\n1000\n2,000,000,000\n1071.753574\n1,000,000,000\n5.316519\n\n\n\n\nNota: La eliminaci√≥n Gauss-Jordan muestra una notable ventaja en tiempo de ejecuci√≥n respecto a la multiplicaci√≥n de matrices, aunque ambas mantienen una proporci√≥n consistente en la cantidad de operaciones requeridas.\n\nLos resultados obtenidos muestran el desempe√±o de dos operaciones: la multiplicaci√≥n de matrices y la eliminaci√≥n gaussiana/Gauss-Jordan sobre matrices de diferentes tama√±os (100, 300 y 1000), cuantificando el n√∫mero de operaciones y el tiempo de ejecuci√≥n, como se muestra en la Tabla 1.\n\n\n\n\n\n\n\n\n\nEn los Gr√°ficos 1 y 2, se aprecia visualmente c√≥mo se ve afectado el n√∫mero de operaciones y el tiempo de ejecuci√≥n en relaci√≥n con el tama√±o de la matriz.\nEn cuanto a la multiplicaci√≥n de matrices, el n√∫mero de operaciones aumenta significativamente con el tama√±o de la matriz, lo cual es de esperarse, ya que se trata de una operaci√≥n computacionalmente intensiva. El tiempo de ejecuci√≥n tambi√©n aumenta proporcionalmente, reflejando el impacto del crecimiento en complejidad.\nPor otro lado, la eliminaci√≥n gaussiana/Gauss-Jordan muestra una tendencia similar: a mayor tama√±o de la matriz, mayor n√∫mero de operaciones y mayor tiempo de procesamiento. Espec√≠ficamente, para una matriz de tama√±o \\(n = 1000\\), el tiempo fue de 1,071.753 segundos para la multiplicaci√≥n de matrices y 5.316 segundos para la eliminaci√≥n gaussiana, destacando una diferencia considerable a favor del segundo m√©todo en t√©rminos de eficiencia temporal.\nEl rendimiento puede verse influenciado por el hardware utilizado, como la capacidad de memoria, la arquitectura del procesador, y otros componentes del sistema. Estos factores afectan directamente el tiempo de ejecuci√≥n observado. Entre los elementos que m√°s impactan se incluyen la cantidad y velocidad de la memoria RAM, la arquitectura del procesador, el n√∫mero de n√∫cleos e hilos de ejecuci√≥n, la frecuencia de reloj, la memoria cach√© del procesador, el tipo de almacenamiento, y la tecnolog√≠a de la GPU si es utilizada (Raina et al., 2009).\n\n\n\n\n\n\nEl impacto de acceder a elementos contiguos en memoria de una matriz es un concepto fundamental en la optimizaci√≥n del rendimiento de programas. Este principio se basa en c√≥mo los datos se almacenan y acceden en la memoria de una computadora.\nCuando los elementos de una matriz se almacenan de manera contigua en la memoria, el procesador puede acceder a ellos de forma m√°s eficiente. Esto se debe a varios factores: el uso de la cach√© del procesador, el aumento de la localidad espacial, el prefetching, la reducci√≥n del consumo de energ√≠a y las operaciones vectorizadas.\nLa cach√© es una memoria de acceso r√°pido que almacena copias de datos que se utilizan frecuentemente, y cuando los datos est√°n almacenados de manera contigua, es m√°s probable que se carguen en la cach√© en bloques grandes, reduciendo as√≠ el tiempo de acceso.\nLa localidad espacial hace referencia a la tendencia de los programas a acceder a datos que est√°n cerca unos de otros en la memoria. Cuando los datos est√°n almacenados de manera contigua, se mejora la localidad espacial, lo que permite al procesador acceder a los datos de manera m√°s r√°pida.\nLos procesadores utilizan t√©cnicas de prefetching para anticipar qu√© datos se necesitar√°n en el futuro y cargarlos en la cach√© antes de que sean solicitados. Cuando los datos est√°n almacenados de manera contigua, el prefetching es m√°s efectivo, ya que el procesador puede cargar bloques grandes de datos.\nOtro impacto importante es la reducci√≥n del consumo de energ√≠a; un mejor uso de la cach√© y una menor latencia tambi√©n pueden traducirse en un menor consumo energ√©tico.\nEl acceso contiguo a la memoria tambi√©n puede influir en el rendimiento de las operaciones vectorizadas, que son comunes en aplicaciones cient√≠ficas y de ingenier√≠a. Las operaciones vectorizadas permiten al procesador realizar m√∫ltiples operaciones en paralelo, y cuando los datos est√°n almacenados de manera contigua, es m√°s f√°cil para el procesador realizarlas de manera eficiente (Bryant, 2016).\n\n\n\nSi utiliz√°ramos matrices dispersas, estas tendr√≠an un impacto en el rendimiento y la eficiencia de un programa, especialmente al trabajar con grandes conjuntos de datos donde la mayor√≠a de los elementos son cero.\nLos cambios al utilizar matrices dispersas incluyen:\n\nReducci√≥n del uso de memoria: Las matrices dispersas almacenan solo los elementos no nulos y sus posiciones, lo que reduce significativamente el uso de memoria en comparaci√≥n con matrices densas.\nOptimizaci√≥n de operaciones: Las operaciones matem√°ticas pueden ser m√°s eficientes al realizarse √∫nicamente sobre los elementos no nulos, disminuyendo as√≠ el tiempo de c√≥mputo.\nUso de bibliotecas especializadas: Se requiere el uso de bibliotecas dise√±adas espec√≠ficamente para manejar matrices dispersas de forma eficiente.\n\nSin embargo, tambi√©n se presentan algunos costos y desaf√≠os:\n\nMayor complejidad en la implementaci√≥n: Los algoritmos que manejan matrices dispersas pueden ser m√°s complejos que los que utilizan matrices densas, requiriendo mayor tiempo de desarrollo y depuraci√≥n.\nMantenimiento e interoperabilidad: La integraci√≥n con sistemas o bibliotecas que no admiten matrices dispersas puede generar la necesidad de realizar conversiones costosas entre formatos.\nOperaciones costosas: Inserciones y eliminaciones pueden requerir mayor tiempo de procesamiento, debido a la necesidad de actualizar estructuras de √≠ndices.\nSobrecarga de gesti√≥n de datos: Aunque se reduce el uso de memoria, la administraci√≥n adicional puede afectar el rendimiento, especialmente si la matriz no es extremadamente dispersa.\nCostos de conversi√≥n: Si es necesario convertir entre matrices densas y dispersas, esto puede introducir costos adicionales tanto en tiempo de ejecuci√≥n como en memoria (Bryant, 2016).\n\n\n\n\n\n\nGoodfellow, I., Bengio, Y., & Courville, A. (2016). Deep Learning. MIT Press.\nStrang, G. (2016). Introduction to Linear Algebra (5¬™ edici√≥n). Wellesley-Cambridge Press.\nGrcar, J. F. (2011). ‚ÄúMathematicians of Gaussian Elimination.‚Äù Notices of the American Mathematical Society, 58(6), 782‚Äì792.\nBryant, R. E., & O‚ÄôHallaron, D. R. (2016). Computer Systems: A Programmer‚Äôs Perspective (3rd ed.). Pearson.\nTeresa-Motiv, Saisang, brittmsantos, eross-msft, & JasonGerend. (2023, 17 de abril). Consideraciones sobre el uso de memoria para la optimizaci√≥n del rendimiento de AD DS.\nRaina, R., Madhavan, A., & Ng, A. Y. (2009). Large-scale Deep Unsupervised Learning using Graphics Processors. Computer Science Department, Stanford University.\n\n\n\n\nSe hicieron mejoras en el planteamiento de los experimentos y en la discusi√≥n de los resultados, como fue indicado por el maestro. Ahora cada figura tiene una explicaci√≥n clara sobre lo que muestra y lo que significa. Tambi√©n se mejor√≥ la redacci√≥n para que las ideas sean m√°s comprensibles y ordenadas."
  },
  {
    "objectID": "project2.html#introducci√≥n",
    "href": "project2.html#introducci√≥n",
    "title": "Proyecto 2",
    "section": "",
    "text": "Los experimentos y an√°lisis de estructuras de datos son fundamentales en la ciencia de datos, ya que permiten evaluar y optimizar el rendimiento de algoritmos y sistemas que manejan grandes vol√∫menes de informaci√≥n.\nLas estructuras de datos son formas de organizar y almacenar datos en una computadora para que puedan ser accedidos y modificados de manera eficiente. Cada estructura tiene sus propias ventajas y desventajas en t√©rminos de tiempo de acceso, uso de memoria y facilidad de implementaci√≥n. Los experimentos en estructuras de datos suelen centrarse en medir estos aspectos bajo diferentes condiciones para determinar cu√°l es la m√°s adecuada para una aplicaci√≥n espec√≠fica (Goodfellow et al., 2016).\nEl dise√±o experimental es una etapa esencial que precede al an√°lisis de datos. Un dise√±o bien planificado asegura que los datos recopilados sean relevantes y √∫tiles para el an√°lisis posterior. Esto incluye la definici√≥n de variables, la selecci√≥n de muestras y la implementaci√≥n de controles para minimizar sesgos. Estas estructuras son esenciales para el desarrollo de software eficiente, especialmente en campos como el Internet de las Cosas (IoT) y el Big Data, donde la optimizaci√≥n de recursos es crucial (Strang, 2016).\nEl an√°lisis de datos implica varias t√©cnicas estad√≠sticas y computacionales para interpretar los resultados experimentales. Esto puede incluir an√°lisis exploratorio de datos (EDA) para identificar patrones y relaciones, as√≠ como an√°lisis confirmatorio para probar hip√≥tesis espec√≠ficas. Herramientas como Hadoop, Spark, Power BI, Pandas y R son com√∫nmente utilizadas en este proceso (Grcar, 2011).\nLa multiplicaci√≥n de matrices es una operaci√≥n b√°sica que se utiliza en numerosos c√°lculos matem√°ticos y algoritmos computacionales. Su relevancia radica en su capacidad para transformar y combinar datos de manera que se puedan extraer insights significativos. La eliminaci√≥n gaussiana es un m√©todo cl√°sico para resolver sistemas de ecuaciones lineales, transformando una matriz en una forma escalonada reducida por filas. Este m√©todo es ampliamente utilizado debido a su estabilidad num√©rica y su capacidad para manejar sistemas grandes.\nEl objetivo es entender c√≥mo se comportan estos algoritmos en diferentes situaciones, especialmente en cuanto al n√∫mero de operaciones (como multiplicaciones y sumas) y el tiempo que tardan en ejecutarse en la pr√°ctica. Para abordar esta cuesti√≥n, se plantea un estudio comparativo utilizando matrices aleatorias de tama√±o \\(n \\times n\\) para \\(n = 100\\), \\(300\\), \\(1000\\). Este enfoque permitir√° evaluar la eficiencia de cada algoritmo y proporcionar√° una base s√≥lida para futuras optimizaciones. Al comparar estos algoritmos, se busca responder a preguntas clave como: ¬øQu√© puedes concluir?, ¬øCu√°l es el impacto de acceder a los elementos contiguos en memoria de una matriz?, ¬øQu√© cambiar√≠as si utilizas matrices dispersas?, y ¬øCu√°les ser√≠an los costos?"
  },
  {
    "objectID": "project2.html#desarrollo",
    "href": "project2.html#desarrollo",
    "title": "Proyecto 2",
    "section": "",
    "text": "import numpy as np\nimport time\n\n\n\ndef matrix_multiplication(A, B):\n    n = len(A)  # Tama√±o de la matriz (n x n)\n    C = np.zeros((n, n))  # Inicializar la matriz resultante con ceros\n    operations = 0  # Contador de operaciones\n\n    for i in range(n):\n        for j in range(n):\n            for k in range(n):\n                C[i][j] += A[i][k] * B[k][j]  # Multiplicaci√≥n y suma\n                operations += 2  # Una multiplicaci√≥n y una suma\n\n    return C, operations\n\n\n\ndef gauss_jordan(A):\n    n = len(A)\n    operations = 0\n    A = np.array(A, dtype=float)\n\n    for i in range(n):\n        if A[i][i] == 0:\n            for j in range(i+1, n):\n                if A[j][i] != 0:\n                    A[[i, j]] = A[[j, i]]  # Intercambio de filas\n                    operations += 1\n                    break\n\n        div = A[i][i]\n        A[i] = A[i] / div\n        operations += n  # n divisiones\n\n        for j in range(n):\n            if i != j:\n                A[j] = A[j] - A[j][i] * A[i]\n                operations += n  # n multiplicaciones y restas\n\n    return A, operations\n\n\n\nsizes = [100, 300, 1000]\nresults = []\n\nfor n in sizes:\n    A = np.random.rand(n, n)\n    B = np.random.rand(n, n)\n\n    # Multiplicaci√≥n de matrices\n    start_time = time.time()\n    C, mult_operations = matrix_multiplication(A, B)\n    mult_time = time.time() - start_time\n\n    # Eliminaci√≥n Gaussiana\n    start_time = time.time()\n    _, gauss_operations = gauss_jordan(A)\n    gauss_time = time.time() - start_time\n\n    results.append({\n        'n': n,\n        'mult_operations': mult_operations,\n        'mult_time': mult_time,\n        'gauss_operations': gauss_operations,\n        'gauss_time': gauss_time\n    })\n\n\n\nfor result in results:\n    print(f\"Tama√±o de la matriz (n): {result['n']}\")\n    print(\" Multiplicaci√≥n de Matrices:\")\n    print(f\" Operaciones: {result['mult_operations']}\")\n    print(f\" Tiempo (s): {result['mult_time']:.6f}\")\n    print(\" Eliminaci√≥n Gaussiana:\")\n    print(f\" Operaciones: {result['gauss_operations']}\")\n    print(f\" Tiempo (s): {result['gauss_time']:.6f}\")\n    print(\"-\" * 40)"
  },
  {
    "objectID": "project2.html#an√°lisis-de-resultados",
    "href": "project2.html#an√°lisis-de-resultados",
    "title": "Proyecto 2",
    "section": "",
    "text": "A continuaci√≥n se muestra una tabla con los resultados obtenidos al comparar la multiplicaci√≥n de matrices y la eliminaci√≥n Gauss-Jordan sobre matrices de distintos tama√±os. Se reporta el n√∫mero total de operaciones y el tiempo de ejecuci√≥n en segundos.\n\n\n\n\n\n\n\n\n\n\nTama√±o (n)\nOperaciones Multiplicaci√≥n\nTiempo Multiplicaci√≥n (s)\nOperaciones Gauss\nTiempo Gauss (s)\n\n\n\n\n100\n2,000,000\n1.829321\n1,000,000\n0.066816\n\n\n300\n54,000,000\n30.990801\n27,000,000\n0.399951\n\n\n1000\n2,000,000,000\n1071.753574\n1,000,000,000\n5.316519\n\n\n\n\nNota: La eliminaci√≥n Gauss-Jordan muestra una notable ventaja en tiempo de ejecuci√≥n respecto a la multiplicaci√≥n de matrices, aunque ambas mantienen una proporci√≥n consistente en la cantidad de operaciones requeridas.\n\nLos resultados obtenidos muestran el desempe√±o de dos operaciones: la multiplicaci√≥n de matrices y la eliminaci√≥n gaussiana/Gauss-Jordan sobre matrices de diferentes tama√±os (100, 300 y 1000), cuantificando el n√∫mero de operaciones y el tiempo de ejecuci√≥n, como se muestra en la Tabla 1.\n\n\n\n\n\n\n\n\n\nEn los Gr√°ficos 1 y 2, se aprecia visualmente c√≥mo se ve afectado el n√∫mero de operaciones y el tiempo de ejecuci√≥n en relaci√≥n con el tama√±o de la matriz.\nEn cuanto a la multiplicaci√≥n de matrices, el n√∫mero de operaciones aumenta significativamente con el tama√±o de la matriz, lo cual es de esperarse, ya que se trata de una operaci√≥n computacionalmente intensiva. El tiempo de ejecuci√≥n tambi√©n aumenta proporcionalmente, reflejando el impacto del crecimiento en complejidad.\nPor otro lado, la eliminaci√≥n gaussiana/Gauss-Jordan muestra una tendencia similar: a mayor tama√±o de la matriz, mayor n√∫mero de operaciones y mayor tiempo de procesamiento. Espec√≠ficamente, para una matriz de tama√±o \\(n = 1000\\), el tiempo fue de 1,071.753 segundos para la multiplicaci√≥n de matrices y 5.316 segundos para la eliminaci√≥n gaussiana, destacando una diferencia considerable a favor del segundo m√©todo en t√©rminos de eficiencia temporal.\nEl rendimiento puede verse influenciado por el hardware utilizado, como la capacidad de memoria, la arquitectura del procesador, y otros componentes del sistema. Estos factores afectan directamente el tiempo de ejecuci√≥n observado. Entre los elementos que m√°s impactan se incluyen la cantidad y velocidad de la memoria RAM, la arquitectura del procesador, el n√∫mero de n√∫cleos e hilos de ejecuci√≥n, la frecuencia de reloj, la memoria cach√© del procesador, el tipo de almacenamiento, y la tecnolog√≠a de la GPU si es utilizada (Raina et al., 2009)."
  },
  {
    "objectID": "project2.html#conclusiones",
    "href": "project2.html#conclusiones",
    "title": "Proyecto 2",
    "section": "",
    "text": "El impacto de acceder a elementos contiguos en memoria de una matriz es un concepto fundamental en la optimizaci√≥n del rendimiento de programas. Este principio se basa en c√≥mo los datos se almacenan y acceden en la memoria de una computadora.\nCuando los elementos de una matriz se almacenan de manera contigua en la memoria, el procesador puede acceder a ellos de forma m√°s eficiente. Esto se debe a varios factores: el uso de la cach√© del procesador, el aumento de la localidad espacial, el prefetching, la reducci√≥n del consumo de energ√≠a y las operaciones vectorizadas.\nLa cach√© es una memoria de acceso r√°pido que almacena copias de datos que se utilizan frecuentemente, y cuando los datos est√°n almacenados de manera contigua, es m√°s probable que se carguen en la cach√© en bloques grandes, reduciendo as√≠ el tiempo de acceso.\nLa localidad espacial hace referencia a la tendencia de los programas a acceder a datos que est√°n cerca unos de otros en la memoria. Cuando los datos est√°n almacenados de manera contigua, se mejora la localidad espacial, lo que permite al procesador acceder a los datos de manera m√°s r√°pida.\nLos procesadores utilizan t√©cnicas de prefetching para anticipar qu√© datos se necesitar√°n en el futuro y cargarlos en la cach√© antes de que sean solicitados. Cuando los datos est√°n almacenados de manera contigua, el prefetching es m√°s efectivo, ya que el procesador puede cargar bloques grandes de datos.\nOtro impacto importante es la reducci√≥n del consumo de energ√≠a; un mejor uso de la cach√© y una menor latencia tambi√©n pueden traducirse en un menor consumo energ√©tico.\nEl acceso contiguo a la memoria tambi√©n puede influir en el rendimiento de las operaciones vectorizadas, que son comunes en aplicaciones cient√≠ficas y de ingenier√≠a. Las operaciones vectorizadas permiten al procesador realizar m√∫ltiples operaciones en paralelo, y cuando los datos est√°n almacenados de manera contigua, es m√°s f√°cil para el procesador realizarlas de manera eficiente (Bryant, 2016).\n\n\n\nSi utiliz√°ramos matrices dispersas, estas tendr√≠an un impacto en el rendimiento y la eficiencia de un programa, especialmente al trabajar con grandes conjuntos de datos donde la mayor√≠a de los elementos son cero.\nLos cambios al utilizar matrices dispersas incluyen:\n\nReducci√≥n del uso de memoria: Las matrices dispersas almacenan solo los elementos no nulos y sus posiciones, lo que reduce significativamente el uso de memoria en comparaci√≥n con matrices densas.\nOptimizaci√≥n de operaciones: Las operaciones matem√°ticas pueden ser m√°s eficientes al realizarse √∫nicamente sobre los elementos no nulos, disminuyendo as√≠ el tiempo de c√≥mputo.\nUso de bibliotecas especializadas: Se requiere el uso de bibliotecas dise√±adas espec√≠ficamente para manejar matrices dispersas de forma eficiente.\n\nSin embargo, tambi√©n se presentan algunos costos y desaf√≠os:\n\nMayor complejidad en la implementaci√≥n: Los algoritmos que manejan matrices dispersas pueden ser m√°s complejos que los que utilizan matrices densas, requiriendo mayor tiempo de desarrollo y depuraci√≥n.\nMantenimiento e interoperabilidad: La integraci√≥n con sistemas o bibliotecas que no admiten matrices dispersas puede generar la necesidad de realizar conversiones costosas entre formatos.\nOperaciones costosas: Inserciones y eliminaciones pueden requerir mayor tiempo de procesamiento, debido a la necesidad de actualizar estructuras de √≠ndices.\nSobrecarga de gesti√≥n de datos: Aunque se reduce el uso de memoria, la administraci√≥n adicional puede afectar el rendimiento, especialmente si la matriz no es extremadamente dispersa.\nCostos de conversi√≥n: Si es necesario convertir entre matrices densas y dispersas, esto puede introducir costos adicionales tanto en tiempo de ejecuci√≥n como en memoria (Bryant, 2016)."
  },
  {
    "objectID": "project2.html#referencias",
    "href": "project2.html#referencias",
    "title": "Proyecto 2",
    "section": "",
    "text": "Goodfellow, I., Bengio, Y., & Courville, A. (2016). Deep Learning. MIT Press.\nStrang, G. (2016). Introduction to Linear Algebra (5¬™ edici√≥n). Wellesley-Cambridge Press.\nGrcar, J. F. (2011). ‚ÄúMathematicians of Gaussian Elimination.‚Äù Notices of the American Mathematical Society, 58(6), 782‚Äì792.\nBryant, R. E., & O‚ÄôHallaron, D. R. (2016). Computer Systems: A Programmer‚Äôs Perspective (3rd ed.). Pearson.\nTeresa-Motiv, Saisang, brittmsantos, eross-msft, & JasonGerend. (2023, 17 de abril). Consideraciones sobre el uso de memoria para la optimizaci√≥n del rendimiento de AD DS.\nRaina, R., Madhavan, A., & Ng, A. Y. (2009). Large-scale Deep Unsupervised Learning using Graphics Processors. Computer Science Department, Stanford University."
  },
  {
    "objectID": "project2.html#cambios-realizados",
    "href": "project2.html#cambios-realizados",
    "title": "Proyecto 2",
    "section": "",
    "text": "Se hicieron mejoras en el planteamiento de los experimentos y en la discusi√≥n de los resultados, como fue indicado por el maestro. Ahora cada figura tiene una explicaci√≥n clara sobre lo que muestra y lo que significa. Tambi√©n se mejor√≥ la redacci√≥n para que las ideas sean m√°s comprensibles y ordenadas."
  },
  {
    "objectID": "project4.html#introducci√≥n",
    "href": "project4.html#introducci√≥n",
    "title": "Proyecto 4",
    "section": "1. Introducci√≥n",
    "text": "1. Introducci√≥n"
  },
  {
    "objectID": "project4.html#desarrollo",
    "href": "project4.html#desarrollo",
    "title": "Proyecto 4",
    "section": "2. Desarrollo",
    "text": "2. Desarrollo"
  },
  {
    "objectID": "project4.html#an√°lisis-de-resultados",
    "href": "project4.html#an√°lisis-de-resultados",
    "title": "Proyecto 4",
    "section": "3. An√°lisis de Resultados",
    "text": "3. An√°lisis de Resultados"
  },
  {
    "objectID": "project4.html#conclusiones",
    "href": "project4.html#conclusiones",
    "title": "Proyecto 4",
    "section": "4. Conclusiones",
    "text": "4. Conclusiones"
  },
  {
    "objectID": "project4.html#referencias",
    "href": "project4.html#referencias",
    "title": "Proyecto 4",
    "section": "5. Referencias",
    "text": "5. Referencias"
  },
  {
    "objectID": "project4.html#cambios-realizados",
    "href": "project4.html#cambios-realizados",
    "title": "Proyecto 4",
    "section": "6. Cambios Realizados",
    "text": "6. Cambios Realizados"
  }
]