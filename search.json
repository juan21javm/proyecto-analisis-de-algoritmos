[
  {
    "objectID": "project5.html#introducción",
    "href": "project5.html#introducción",
    "title": "Proyecto 5",
    "section": "1. Introducción",
    "text": "1. Introducción"
  },
  {
    "objectID": "project5.html#desarrollo",
    "href": "project5.html#desarrollo",
    "title": "Proyecto 5",
    "section": "2. Desarrollo",
    "text": "2. Desarrollo"
  },
  {
    "objectID": "project5.html#análisis-de-resultados",
    "href": "project5.html#análisis-de-resultados",
    "title": "Proyecto 5",
    "section": "3. Análisis de Resultados",
    "text": "3. Análisis de Resultados"
  },
  {
    "objectID": "project5.html#conclusiones",
    "href": "project5.html#conclusiones",
    "title": "Proyecto 5",
    "section": "4. Conclusiones",
    "text": "4. Conclusiones"
  },
  {
    "objectID": "project5.html#referencias",
    "href": "project5.html#referencias",
    "title": "Proyecto 5",
    "section": "5. Referencias",
    "text": "5. Referencias"
  },
  {
    "objectID": "project5.html#cambios-realizados",
    "href": "project5.html#cambios-realizados",
    "title": "Proyecto 5",
    "section": "6. Cambios Realizados",
    "text": "6. Cambios Realizados"
  },
  {
    "objectID": "project3.html#introducción",
    "href": "project3.html#introducción",
    "title": "Proyecto 3",
    "section": "1. Introducción",
    "text": "1. Introducción"
  },
  {
    "objectID": "project3.html#desarrollo",
    "href": "project3.html#desarrollo",
    "title": "Proyecto 3",
    "section": "2. Desarrollo",
    "text": "2. Desarrollo"
  },
  {
    "objectID": "project3.html#análisis-de-resultados",
    "href": "project3.html#análisis-de-resultados",
    "title": "Proyecto 3",
    "section": "3. Análisis de Resultados",
    "text": "3. Análisis de Resultados"
  },
  {
    "objectID": "project3.html#conclusiones",
    "href": "project3.html#conclusiones",
    "title": "Proyecto 3",
    "section": "4. Conclusiones",
    "text": "4. Conclusiones"
  },
  {
    "objectID": "project3.html#referencias",
    "href": "project3.html#referencias",
    "title": "Proyecto 3",
    "section": "5. Referencias",
    "text": "5. Referencias"
  },
  {
    "objectID": "project3.html#cambios-realizados",
    "href": "project3.html#cambios-realizados",
    "title": "Proyecto 3",
    "section": "6. Cambios Realizados",
    "text": "6. Cambios Realizados"
  },
  {
    "objectID": "project1.html",
    "href": "project1.html",
    "title": "Proyecto 1",
    "section": "",
    "text": "En la era digital actual, el big data ha emergido como un recurso valioso para las organizaciones, permitiéndoles extraer, procesar y analizar datos significativos. El big data se caracteriza por sus cuatro V: volumen, velocidad, variedad y veracidad. El volumen se refiere a la gran cantidad de datos generados y almacenados; la velocidad, a la rapidez con la que se generan y procesan; la variedad, a los diferentes tipos de datos (estructurados y no estructurados); y la veracidad, a la calidad y precisión de dichos datos. La capacidad de manejar y analizar estos grandes volúmenes de información de manera eficiente es crucial para aprovechar al máximo el potencial del big data (Müller et al., 2016).\nEl análisis de big data permite a las organizaciones identificar patrones, predecir tendencias y optimizar procesos, lo que puede resultar en mejoras significativas en la eficiencia operativa y en la toma de decisiones estratégicas. Sin embargo, la manipulación de grandes volúmenes de datos presenta desafíos importantes en términos de infraestructura, almacenamiento, procesamiento y análisis. Los algoritmos eficientes son fundamentales para enfrentar estos desafíos y garantizar que los sistemas de big data funcionen de manera óptima (Manyika et al., 2011).\nEste reporte se enfoca en comparar diferentes órdenes de crecimiento mediante simulaciones en un entorno de Jupyter. Se analizarán los siguientes casos: \\(O(1)\\) vs \\(O(\\log n)\\), \\(O(n)\\) vs \\(O(n \\log n)\\), \\(O(n^2)\\) vs \\(O(n^3)\\), \\(O(a^n)\\) vs \\(O(n!)\\) y \\(O(n!)\\) vs \\(O(n^n)\\). Para cada comparación, se seleccionarán rangos adecuados de \\(n\\) que permitan visualizar claramente las diferencias entre estos órdenes. Se generarán gráficas para cada caso y se discutirán las observaciones correspondientes. Además, se incluirá una tabla con tiempos de ejecución simulados para algoritmos ficticios asociados a los órdenes de crecimiento mencionados, utilizando distintos tamaños de entrada \\(n\\). Este análisis proporciona una visión clara de cómo los diferentes órdenes de crecimiento afectan el rendimiento y la eficiencia de los algoritmos.\n\n\n\n\n\nimport numpy as np\nimport pandas as pd\nimport math\nimport matplotlib.pyplot as plt\n\n\n\ndef constant(n): return 1\ndef logarithmic(n): return np.log(n)\ndef linear(n): return n\ndef linear_logarithmic(n): return n * np.log(n)\ndef quadratic(n): return n**2\ndef cubic(n): return n**3\ndef exponential(n, a=2): return a**n\ndef factorial(n): return math.factorial(n)\ndef double_exponential(n): return n**n\n\n\n\ndef plot_comparison(funcs, labels, title, x_range):\n    plt.figure(figsize=(7, 5))\n    for func, label in zip(funcs, labels):\n        plt.plot(x_range, [func(x) / 1e9 for x in x_range], label=label)\n    plt.xlabel('n')\n    plt.ylabel('Tiempo (s)')\n    plt.title(title)\n    plt.legend()\n    plt.yscale('log')\n    plt.xscale('log')\n    plt.grid(True)\n    plt.show()\n\n\n\nx_range_small = np.arange(1, 10)\nx_range_medium = np.arange(1, 100)\n\n# Comparación 1: O(1) vs O(log n)\nplot_comparison([constant, logarithmic], ['O(1)', 'O(log n)'], 'Figura 1 - Comparación de O(1) con O(log n)', x_range_small)\n\n# Comparación 2: O(n) vs O(n log n)\nplot_comparison([linear, linear_logarithmic], ['O(n)', 'O(n log n)'], 'Figura 2 - Comparación de O(n) con O(n log n)', x_range_medium)\n\n# Comparación 3: O(n^2) vs O(n^3)\nplot_comparison([quadratic, cubic], ['O(n^2)', 'O(n^3)'], 'Figura 3 - Comparación de O(n^2) con O(n^3)', x_range_medium)\n\n# Comparación 4: O(2^n) vs O(n!)\nplot_comparison([lambda n: exponential(n, 2), factorial], ['O(2^n)', 'O(n!)'], 'Figura 4 - Comparación de O(2^n) con O(n!)', x_range_small)\n\n# Comparación 5: O(n!) vs O(n^n)\nplot_comparison([factorial, double_exponential], ['O(n!)', 'O(n^n)'], 'Figura 5 - Comparación de O(n!) con O(n^n)', x_range_small)\n\n\n\nNota: Se creó una tabla con tiempos de ejecución simulados para algoritmos ficticios con los órdenes de crecimiento mencionados.\n# Tamaños de entrada\nn_values = [100, 1000, 10000, 100000]\n\n# Funciones de costo\ncost_functions = {\n    'O(1)': constant,\n    'O(log n)': logarithmic,\n    'O(n)': linear,\n    'O(n log n)': linear_logarithmic,\n    'O(n^2)': quadratic,\n    'O(n^3)': cubic,\n    'O(2^n)': lambda n: exponential(n, 2),\n    'O(n!)': factorial,\n    'O(n^n)': double_exponential\n}\n\n# Elaboración de la tabla\nresults = []\nfor n in n_values:\n    row = {'n': n}\n    for label, func in cost_functions.items():\n        try:\n            row[label] = func(n) / 1e9  # Conversión a segundos\n        except OverflowError:\n            row[label] = 'Overflow'\n    results.append(row)\n\nimport pandas as pd\ndf = pd.DataFrame(results)\ndf.set_index('n', inplace=True)\ndf\n\n\n\n\n\n\n\n\n\n\n\n\nEl gráfico muestra que la función constante \\(O(1)\\) permanece constante independientemente del tamaño de la entrada, mientras que \\(O(\\log n)\\) crece lentamente conforme \\(n\\) aumenta. Por lo tanto, un algoritmo con complejidad constante \\(O(1)\\) es más eficiente en términos de tiempo de ejecución en comparación con uno con complejidad logarítmica \\(O(\\log n)\\).\n\n\n\n\n\n\nLa función \\(O(n)\\) crece linealmente con el tamaño de entrada \\(n\\), mientras que la función \\(O(n \\log n)\\) crece más rápido que \\(O(n)\\), pero sigue siendo práctica para valores moderados de \\(n\\). Por lo tanto, un algoritmo con complejidad lineal \\(O(n)\\) es más eficiente en términos de tiempo de ejecución en comparación con uno con complejidad lineal-logarítmica \\(O(n \\log n)\\).\n\n\n\n\n\n\nLa función cuadrática \\(O(n^2)\\) crece más rápidamente que la lineal, pero la función cúbica \\(O(n^3)\\) lo hace aún con mayor rapidez. Por lo tanto, un algoritmo con complejidad cuadrática \\(O(n^2)\\) es más eficiente en términos de tiempo de ejecución en comparación con uno con complejidad cúbica \\(O(n^3)\\).\n\n\n\n\n\n\nEl gráfico muestra que la complejidad factorial \\(O(n!)\\) resulta en tiempos de ejecución mucho más altos que \\(O(2^n)\\) a medida que el tamaño de la entrada \\(n\\) aumenta. La complejidad factorial \\(O(n!)\\) crece considerablemente más rápido que la complejidad exponencial \\(O(2^n)\\), lo que hace que los algoritmos con esta complejidad sean prácticamente inutilizables para valores grandes de \\(n\\).\n\n\n\n\n\n\nEl gráfico muestra cómo la función factorial \\(O(n!)\\) crece muy rápidamente, pero la función doble exponencial \\(O(n^n)\\) crece aún más rápido a medida que aumenta el tamaño de la entrada \\(n\\). Por lo tanto, los algoritmos con complejidad factorial y doble exponencial son prácticamente inutilizables para valores grandes de \\(n\\).\n\n\n\n\n\nA continuación se presenta una tabla comparativa de los tiempos simulados para diferentes órdenes de crecimiento, utilizando distintos tamaños de entrada \\(n\\). Los resultados se expresan en segundos. En algunos casos, se muestra “Overflow” cuando el valor resultante excede los límites de representación.\nTabla 1. Tiempos simulados para diferentes órdenes de crecimiento de O(1), O(log n), O(n), O(n log n), O(n²)\n\n\n\n\n\n\n\n\n\n\n\nn\nO(1)\nO(log n)\nO(n)\nO(n log n)\nO(n²)\n\n\n\n\n100\n1.00e-09\n4.61e-09\n1.00e-07\n4.61e-07\n1.00e-05\n\n\n1000\n1.00e-09\n6.91e-09\n1.00e-06\n6.91e-06\n1.00e-03\n\n\n10000\n1.00e-09\n9.21e-09\n1.00e-05\n9.21e-05\n1.00e-01\n\n\n100000\n1.00e-09\n1.15e-08\n1.00e-04\n1.15e-03\n1.00e+01\n\n\n\nTabla 2. Tiempos simulados para diferentes órdenes de crecimiento de O(n³), O(2ⁿ)\n\n\n\nn\nO(n³)\nO(2ⁿ)\n\n\n\n\n100\n1.00e-03\n1.27e+21\n\n\n1000\n1.00e+00\nOverflow\n\n\n10000\n1.00e+03\nOverflow\n\n\n100000\n1.00e+06\nOverflow\n\n\n\nTabla 3. Tiempos simulados para diferentes órdenes de crecimiento de O(n!), O(nⁿ)\n\n\n\nn\nO(n!)\nO(nⁿ)\n\n\n\n\n100\nOverflow\nOverflow\n\n\n1000\nOverflow\nOverflow\n\n\n10000\nOverflow\nOverflow\n\n\n100000\nOverflow\nOverflow\n\n\n\n\nNota: Algunos valores como O(2ⁿ), O(n!) y O(nⁿ) para valores grandes de \\(n\\) generan cantidades inmanejables por el sistema, resultando en Overflow. Esto refleja la inviabilidad práctica de algoritmos con estas complejidades cuando se trabaja con grandes volúmenes de datos.\n\nComo se muestra en la tabla, el tiempo de ejecución de O(1) no depende del tamaño de n. Este mantiene un mismo valor, lo que indica que su tiempo de ejecución es constante. Por otro lado, los valores de O(log n) aumentan lentamente a medida que n crece, lo cual es consistente con su comportamiento logarítmico. En el caso de O(n), los valores aumentan de forma lineal conforme n se incrementa. Los valores de O(n log n) aumentan más rápido que los de O(n), pero no tan aceleradamente como los de O(n²). Por su parte, O(n²) crece rápidamente a medida que n aumenta, y O(n³) lo hace de forma aún más acelerada, incluso con valores pequeños de entrada.\nDurante la ejecución del código, algunos resultados aparecen como “Overflow”. Esto se debe a que las funciones O(2ⁿ), O(n!) y O(nⁿ) para valores grandes de n (como 10,000 o 100,000) generan números extremadamente grandes, lo que excede la capacidad de representación y manejo numérico en Python.\n\n\n\n\nLa manipulación de grandes volúmenes de información presenta importantes desafíos debido a los costos de cómputo. Estos costos se relacionan con el gasto monetario, los recursos computacionales requeridos, el tiempo de procesamiento, el uso de memoria, el almacenamiento y el consumo energético. A continuación se presentan algunas de las implicaciones más relevantes:\n\nInfraestructura: Manipular grandes volúmenes de datos requiere una infraestructura robusta, que incluye servidores potentes, almacenamiento masivo y redes de alta velocidad. Esto representa costos elevados en hardware, mantenimiento y consumo energético (Armbrust et al., 2010).\nEnergía: Los centros de datos que procesan grandes cantidades de información consumen enormes cantidades de energía, lo cual incrementa los costos operativos y contribuye al impacto ambiental debido a las emisiones de carbono (Baliga et al., 2011).\nAlmacenamiento: El almacenamiento de grandes volúmenes de datos requiere soluciones escalables y eficientes. Este aspecto representa un costo considerable, ya que también debe garantizarse la recuperación oportuna de la información.\nProcesamiento: Procesar datos en tiempo real o en tiempo cercano al real implica el uso de algoritmos eficientes y de recursos de cómputo significativos. Estos costos pueden incrementarse especialmente al utilizar tecnologías como el aprendizaje automático y la inteligencia artificial (Wang et al., 2015).\n\nEn conjunto, la gestión de grandes volúmenes de información conlleva costos considerables en términos de infraestructura, energía, almacenamiento, procesamiento, seguridad y mantenimiento. Estos aspectos deben ser cuidadosamente planeados y gestionados para asegurar soluciones de big data eficientes y sostenibles.\n\n\n\n\nEn las simulaciones realizadas se observó que los órdenes de crecimiento más bajos, como O(1), O(log n) y O(n), resultan altamente eficientes. Son ideales para tareas que demandan rendimiento óptimo y tiempos de respuesta reducidos.\nPor otro lado, los órdenes de crecimiento O(n log n) y O(n²) demostraron ser prácticas y ampliamente aplicables, al ofrecer un buen equilibrio entre eficiencia y capacidad de procesamiento.\nEn contraste, las funciones con órdenes de crecimiento elevados como O(2ⁿ), O(n!) y O(nⁿ) resultaron ineficientes, generando tiempos de ejecución excesivos o incluso errores de memoria.\nLa tabla de tiempos de ejecución simulados confirma cómo el crecimiento del orden afecta directamente el rendimiento de los algoritmos. A mayor orden, mayor consumo de recursos y tiempo; a menor orden, mayor eficiencia en la ejecución.\n\n\n\n\nMüller, V. C., Schal, J. M., Meyer-Lindenberg, A. (2016). Machine Learning for Brain Imaging. Cambridge University Press.\nManyika, J., Chui, M., Brown, B., Bughin, J., Dobbs, R., Roxburgh, C., & Byers, A. H. (2011). Big data: The next frontier for innovation, competition, and productivity. McKinsey Global Institute. https://www.mckinsey.com/business-functions/mckinsey-analytics/our-insights/big-data-the-next-frontier-for-innovation\nArmbrust, M., Fox, A., Griffith, R., Joseph, A. D., Katz, R. H., Konwinski, A., et al. (2010). A View of Cloud Computing. Communications of the ACM, 53(4), 50–58. https://doi.org/10.1145/1721654.1721672\nBaliga, J., Ayre, R., Hinton, K., & Tucker, R. S. (2011). Energy-Efficient Telecommunications.\nWang, L., Chen, Y., & Liu, Q. (2015). Big Data Processing: A Survey. Springer.\n\n\n\n\n\n\nLos siguientes ajustes fueron aplicados al presente documento en atención a la retroalimentación recibida por parte del docente. Se atendieron los puntos señalados con el fin de mejorar la calidad formal y académica del reporte:\n\nSe uniformó el formato del documento, manteniendo una estructura clara y coherente entre secciones.\nSe corrigieron errores ortográficos y gramaticales, incluyendo el uso adecuado de comas y acentos.\nLas expresiones matemáticas fueron reescritas utilizando la notación correcta en formato de ecuación (LaTeX), como es el caso de \\(O(1)\\), \\(O(\\log n)\\), \\(O(n^2)\\), entre otras.\nSe estandarizaron los encabezados de cada sección para facilitar la lectura y navegación del documento.\n\nEstas modificaciones aseguran que el trabajo cumpla con los criterios de presentación solicitados y refleje un esfuerzo riguroso en la construcción y exposición del contenido."
  },
  {
    "objectID": "project1.html#introducción",
    "href": "project1.html#introducción",
    "title": "Proyecto 1",
    "section": "",
    "text": "En la era digital actual, el big data ha emergido como un recurso valioso para las organizaciones, permitiéndoles extraer, procesar y analizar datos significativos. El big data se caracteriza por sus cuatro V: volumen, velocidad, variedad y veracidad. El volumen se refiere a la gran cantidad de datos generados y almacenados; la velocidad, a la rapidez con la que se generan y procesan; la variedad, a los diferentes tipos de datos (estructurados y no estructurados); y la veracidad, a la calidad y precisión de dichos datos. La capacidad de manejar y analizar estos grandes volúmenes de información de manera eficiente es crucial para aprovechar al máximo el potencial del big data (Müller et al., 2016).\nEl análisis de big data permite a las organizaciones identificar patrones, predecir tendencias y optimizar procesos, lo que puede resultar en mejoras significativas en la eficiencia operativa y en la toma de decisiones estratégicas. Sin embargo, la manipulación de grandes volúmenes de datos presenta desafíos importantes en términos de infraestructura, almacenamiento, procesamiento y análisis. Los algoritmos eficientes son fundamentales para enfrentar estos desafíos y garantizar que los sistemas de big data funcionen de manera óptima (Manyika et al., 2011).\nEste reporte se enfoca en comparar diferentes órdenes de crecimiento mediante simulaciones en un entorno de Jupyter. Se analizarán los siguientes casos: \\(O(1)\\) vs \\(O(\\log n)\\), \\(O(n)\\) vs \\(O(n \\log n)\\), \\(O(n^2)\\) vs \\(O(n^3)\\), \\(O(a^n)\\) vs \\(O(n!)\\) y \\(O(n!)\\) vs \\(O(n^n)\\). Para cada comparación, se seleccionarán rangos adecuados de \\(n\\) que permitan visualizar claramente las diferencias entre estos órdenes. Se generarán gráficas para cada caso y se discutirán las observaciones correspondientes. Además, se incluirá una tabla con tiempos de ejecución simulados para algoritmos ficticios asociados a los órdenes de crecimiento mencionados, utilizando distintos tamaños de entrada \\(n\\). Este análisis proporciona una visión clara de cómo los diferentes órdenes de crecimiento afectan el rendimiento y la eficiencia de los algoritmos."
  },
  {
    "objectID": "project1.html#desarrollo",
    "href": "project1.html#desarrollo",
    "title": "Proyecto 1",
    "section": "",
    "text": "import numpy as np\nimport pandas as pd\nimport math\nimport matplotlib.pyplot as plt\n\n\n\ndef constant(n): return 1\ndef logarithmic(n): return np.log(n)\ndef linear(n): return n\ndef linear_logarithmic(n): return n * np.log(n)\ndef quadratic(n): return n**2\ndef cubic(n): return n**3\ndef exponential(n, a=2): return a**n\ndef factorial(n): return math.factorial(n)\ndef double_exponential(n): return n**n\n\n\n\ndef plot_comparison(funcs, labels, title, x_range):\n    plt.figure(figsize=(7, 5))\n    for func, label in zip(funcs, labels):\n        plt.plot(x_range, [func(x) / 1e9 for x in x_range], label=label)\n    plt.xlabel('n')\n    plt.ylabel('Tiempo (s)')\n    plt.title(title)\n    plt.legend()\n    plt.yscale('log')\n    plt.xscale('log')\n    plt.grid(True)\n    plt.show()\n\n\n\nx_range_small = np.arange(1, 10)\nx_range_medium = np.arange(1, 100)\n\n# Comparación 1: O(1) vs O(log n)\nplot_comparison([constant, logarithmic], ['O(1)', 'O(log n)'], 'Figura 1 - Comparación de O(1) con O(log n)', x_range_small)\n\n# Comparación 2: O(n) vs O(n log n)\nplot_comparison([linear, linear_logarithmic], ['O(n)', 'O(n log n)'], 'Figura 2 - Comparación de O(n) con O(n log n)', x_range_medium)\n\n# Comparación 3: O(n^2) vs O(n^3)\nplot_comparison([quadratic, cubic], ['O(n^2)', 'O(n^3)'], 'Figura 3 - Comparación de O(n^2) con O(n^3)', x_range_medium)\n\n# Comparación 4: O(2^n) vs O(n!)\nplot_comparison([lambda n: exponential(n, 2), factorial], ['O(2^n)', 'O(n!)'], 'Figura 4 - Comparación de O(2^n) con O(n!)', x_range_small)\n\n# Comparación 5: O(n!) vs O(n^n)\nplot_comparison([factorial, double_exponential], ['O(n!)', 'O(n^n)'], 'Figura 5 - Comparación de O(n!) con O(n^n)', x_range_small)\n\n\n\nNota: Se creó una tabla con tiempos de ejecución simulados para algoritmos ficticios con los órdenes de crecimiento mencionados.\n# Tamaños de entrada\nn_values = [100, 1000, 10000, 100000]\n\n# Funciones de costo\ncost_functions = {\n    'O(1)': constant,\n    'O(log n)': logarithmic,\n    'O(n)': linear,\n    'O(n log n)': linear_logarithmic,\n    'O(n^2)': quadratic,\n    'O(n^3)': cubic,\n    'O(2^n)': lambda n: exponential(n, 2),\n    'O(n!)': factorial,\n    'O(n^n)': double_exponential\n}\n\n# Elaboración de la tabla\nresults = []\nfor n in n_values:\n    row = {'n': n}\n    for label, func in cost_functions.items():\n        try:\n            row[label] = func(n) / 1e9  # Conversión a segundos\n        except OverflowError:\n            row[label] = 'Overflow'\n    results.append(row)\n\nimport pandas as pd\ndf = pd.DataFrame(results)\ndf.set_index('n', inplace=True)\ndf"
  },
  {
    "objectID": "project1.html#análisis-de-resultados",
    "href": "project1.html#análisis-de-resultados",
    "title": "Proyecto 1",
    "section": "",
    "text": "El gráfico muestra que la función constante \\(O(1)\\) permanece constante independientemente del tamaño de la entrada, mientras que \\(O(\\log n)\\) crece lentamente conforme \\(n\\) aumenta. Por lo tanto, un algoritmo con complejidad constante \\(O(1)\\) es más eficiente en términos de tiempo de ejecución en comparación con uno con complejidad logarítmica \\(O(\\log n)\\).\n\n\n\n\n\n\nLa función \\(O(n)\\) crece linealmente con el tamaño de entrada \\(n\\), mientras que la función \\(O(n \\log n)\\) crece más rápido que \\(O(n)\\), pero sigue siendo práctica para valores moderados de \\(n\\). Por lo tanto, un algoritmo con complejidad lineal \\(O(n)\\) es más eficiente en términos de tiempo de ejecución en comparación con uno con complejidad lineal-logarítmica \\(O(n \\log n)\\).\n\n\n\n\n\n\nLa función cuadrática \\(O(n^2)\\) crece más rápidamente que la lineal, pero la función cúbica \\(O(n^3)\\) lo hace aún con mayor rapidez. Por lo tanto, un algoritmo con complejidad cuadrática \\(O(n^2)\\) es más eficiente en términos de tiempo de ejecución en comparación con uno con complejidad cúbica \\(O(n^3)\\).\n\n\n\n\n\n\nEl gráfico muestra que la complejidad factorial \\(O(n!)\\) resulta en tiempos de ejecución mucho más altos que \\(O(2^n)\\) a medida que el tamaño de la entrada \\(n\\) aumenta. La complejidad factorial \\(O(n!)\\) crece considerablemente más rápido que la complejidad exponencial \\(O(2^n)\\), lo que hace que los algoritmos con esta complejidad sean prácticamente inutilizables para valores grandes de \\(n\\).\n\n\n\n\n\n\nEl gráfico muestra cómo la función factorial \\(O(n!)\\) crece muy rápidamente, pero la función doble exponencial \\(O(n^n)\\) crece aún más rápido a medida que aumenta el tamaño de la entrada \\(n\\). Por lo tanto, los algoritmos con complejidad factorial y doble exponencial son prácticamente inutilizables para valores grandes de \\(n\\).\n\n\n\n\n\nA continuación se presenta una tabla comparativa de los tiempos simulados para diferentes órdenes de crecimiento, utilizando distintos tamaños de entrada \\(n\\). Los resultados se expresan en segundos. En algunos casos, se muestra “Overflow” cuando el valor resultante excede los límites de representación.\nTabla 1. Tiempos simulados para diferentes órdenes de crecimiento de O(1), O(log n), O(n), O(n log n), O(n²)\n\n\n\n\n\n\n\n\n\n\n\nn\nO(1)\nO(log n)\nO(n)\nO(n log n)\nO(n²)\n\n\n\n\n100\n1.00e-09\n4.61e-09\n1.00e-07\n4.61e-07\n1.00e-05\n\n\n1000\n1.00e-09\n6.91e-09\n1.00e-06\n6.91e-06\n1.00e-03\n\n\n10000\n1.00e-09\n9.21e-09\n1.00e-05\n9.21e-05\n1.00e-01\n\n\n100000\n1.00e-09\n1.15e-08\n1.00e-04\n1.15e-03\n1.00e+01\n\n\n\nTabla 2. Tiempos simulados para diferentes órdenes de crecimiento de O(n³), O(2ⁿ)\n\n\n\nn\nO(n³)\nO(2ⁿ)\n\n\n\n\n100\n1.00e-03\n1.27e+21\n\n\n1000\n1.00e+00\nOverflow\n\n\n10000\n1.00e+03\nOverflow\n\n\n100000\n1.00e+06\nOverflow\n\n\n\nTabla 3. Tiempos simulados para diferentes órdenes de crecimiento de O(n!), O(nⁿ)\n\n\n\nn\nO(n!)\nO(nⁿ)\n\n\n\n\n100\nOverflow\nOverflow\n\n\n1000\nOverflow\nOverflow\n\n\n10000\nOverflow\nOverflow\n\n\n100000\nOverflow\nOverflow\n\n\n\n\nNota: Algunos valores como O(2ⁿ), O(n!) y O(nⁿ) para valores grandes de \\(n\\) generan cantidades inmanejables por el sistema, resultando en Overflow. Esto refleja la inviabilidad práctica de algoritmos con estas complejidades cuando se trabaja con grandes volúmenes de datos.\n\nComo se muestra en la tabla, el tiempo de ejecución de O(1) no depende del tamaño de n. Este mantiene un mismo valor, lo que indica que su tiempo de ejecución es constante. Por otro lado, los valores de O(log n) aumentan lentamente a medida que n crece, lo cual es consistente con su comportamiento logarítmico. En el caso de O(n), los valores aumentan de forma lineal conforme n se incrementa. Los valores de O(n log n) aumentan más rápido que los de O(n), pero no tan aceleradamente como los de O(n²). Por su parte, O(n²) crece rápidamente a medida que n aumenta, y O(n³) lo hace de forma aún más acelerada, incluso con valores pequeños de entrada.\nDurante la ejecución del código, algunos resultados aparecen como “Overflow”. Esto se debe a que las funciones O(2ⁿ), O(n!) y O(nⁿ) para valores grandes de n (como 10,000 o 100,000) generan números extremadamente grandes, lo que excede la capacidad de representación y manejo numérico en Python.\n\n\n\n\nLa manipulación de grandes volúmenes de información presenta importantes desafíos debido a los costos de cómputo. Estos costos se relacionan con el gasto monetario, los recursos computacionales requeridos, el tiempo de procesamiento, el uso de memoria, el almacenamiento y el consumo energético. A continuación se presentan algunas de las implicaciones más relevantes:\n\nInfraestructura: Manipular grandes volúmenes de datos requiere una infraestructura robusta, que incluye servidores potentes, almacenamiento masivo y redes de alta velocidad. Esto representa costos elevados en hardware, mantenimiento y consumo energético (Armbrust et al., 2010).\nEnergía: Los centros de datos que procesan grandes cantidades de información consumen enormes cantidades de energía, lo cual incrementa los costos operativos y contribuye al impacto ambiental debido a las emisiones de carbono (Baliga et al., 2011).\nAlmacenamiento: El almacenamiento de grandes volúmenes de datos requiere soluciones escalables y eficientes. Este aspecto representa un costo considerable, ya que también debe garantizarse la recuperación oportuna de la información.\nProcesamiento: Procesar datos en tiempo real o en tiempo cercano al real implica el uso de algoritmos eficientes y de recursos de cómputo significativos. Estos costos pueden incrementarse especialmente al utilizar tecnologías como el aprendizaje automático y la inteligencia artificial (Wang et al., 2015).\n\nEn conjunto, la gestión de grandes volúmenes de información conlleva costos considerables en términos de infraestructura, energía, almacenamiento, procesamiento, seguridad y mantenimiento. Estos aspectos deben ser cuidadosamente planeados y gestionados para asegurar soluciones de big data eficientes y sostenibles."
  },
  {
    "objectID": "project1.html#conclusiones",
    "href": "project1.html#conclusiones",
    "title": "Proyecto 1",
    "section": "",
    "text": "En las simulaciones realizadas se observó que los órdenes de crecimiento más bajos, como O(1), O(log n) y O(n), resultan altamente eficientes. Son ideales para tareas que demandan rendimiento óptimo y tiempos de respuesta reducidos.\nPor otro lado, los órdenes de crecimiento O(n log n) y O(n²) demostraron ser prácticas y ampliamente aplicables, al ofrecer un buen equilibrio entre eficiencia y capacidad de procesamiento.\nEn contraste, las funciones con órdenes de crecimiento elevados como O(2ⁿ), O(n!) y O(nⁿ) resultaron ineficientes, generando tiempos de ejecución excesivos o incluso errores de memoria.\nLa tabla de tiempos de ejecución simulados confirma cómo el crecimiento del orden afecta directamente el rendimiento de los algoritmos. A mayor orden, mayor consumo de recursos y tiempo; a menor orden, mayor eficiencia en la ejecución."
  },
  {
    "objectID": "project1.html#referencias",
    "href": "project1.html#referencias",
    "title": "Proyecto 1",
    "section": "",
    "text": "Müller, V. C., Schal, J. M., Meyer-Lindenberg, A. (2016). Machine Learning for Brain Imaging. Cambridge University Press.\nManyika, J., Chui, M., Brown, B., Bughin, J., Dobbs, R., Roxburgh, C., & Byers, A. H. (2011). Big data: The next frontier for innovation, competition, and productivity. McKinsey Global Institute. https://www.mckinsey.com/business-functions/mckinsey-analytics/our-insights/big-data-the-next-frontier-for-innovation\nArmbrust, M., Fox, A., Griffith, R., Joseph, A. D., Katz, R. H., Konwinski, A., et al. (2010). A View of Cloud Computing. Communications of the ACM, 53(4), 50–58. https://doi.org/10.1145/1721654.1721672\nBaliga, J., Ayre, R., Hinton, K., & Tucker, R. S. (2011). Energy-Efficient Telecommunications.\nWang, L., Chen, Y., & Liu, Q. (2015). Big Data Processing: A Survey. Springer."
  },
  {
    "objectID": "project1.html#cambios-realizados",
    "href": "project1.html#cambios-realizados",
    "title": "Proyecto 1",
    "section": "",
    "text": "Los siguientes ajustes fueron aplicados al presente documento en atención a la retroalimentación recibida por parte del docente. Se atendieron los puntos señalados con el fin de mejorar la calidad formal y académica del reporte:\n\nSe uniformó el formato del documento, manteniendo una estructura clara y coherente entre secciones.\nSe corrigieron errores ortográficos y gramaticales, incluyendo el uso adecuado de comas y acentos.\nLas expresiones matemáticas fueron reescritas utilizando la notación correcta en formato de ecuación (LaTeX), como es el caso de \\(O(1)\\), \\(O(\\log n)\\), \\(O(n^2)\\), entre otras.\nSe estandarizaron los encabezados de cada sección para facilitar la lectura y navegación del documento.\n\nEstas modificaciones aseguran que el trabajo cumpla con los criterios de presentación solicitados y refleje un esfuerzo riguroso en la construcción y exposición del contenido."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Sobre Mí",
    "section": "",
    "text": "📍 Aguascalientes, México\n✉️ juan2javm@gmail.com / jvelasquez1800@alumno.ipn.mx\n📱 +52 322 353 4081\n🔗 GitHub: juan21javm\n🔗 LinkedIn: antonio-martínez-776788179\n\n\n\nMe considero una persona profundamente comprometida con el crecimiento intelectual, la lectura reflexiva y el bienestar de mi entorno cercano. Disfruto de los momentos al aire libre, donde encuentro inspiración en la naturaleza y la tranquilidad necesaria para mantener un equilibrio emocional. Valoro profundamente el tiempo con mi familia, ya que es el motor que impulsa mi disciplina, responsabilidad y deseo de superación constante. Asimismo, tengo una gran pasión por la programación y un genuino entusiasmo por la ciencia de datos, áreas que me permiten combinar creatividad, lógica y análisis riguroso para resolver problemas del mundo real. Estos principios personales complementan mi formación académica y profesional, motivándome a mantener siempre una actitud proactiva, ética y humana.\n\n\n\n\nSupervisor — INE (2023–2025), Loreto, Zacatecas\n- Supervisión de actividades diarias para cumplimiento de objetivos.\n- Capacitación del equipo y mejora de rendimiento.\n- Evaluación de desempeño y optimización de procesos.\nMaestro Asistente — IPN (2022–2023), Zacatecas\n- Evaluación del progreso estudiantil.\n- Planeación conjunta de actividades educativas.\n- Creación y adaptación de materiales educativos.\n\n\n\n\nProyecto A — Estandarización de proceso a nivel laboratorio (2021–2023)\n- Herramientas: MATLAB, HACCP Software, Latex\n- Liderazgo de investigación para producción de yogurt con uvas a nivel laboratorio.\n- Reconocimiento al mejor promedio del programa académico (IPN 2017–2022).\n- Primer lugar en Simposium Agroalimentario y Ciclo de Conferencias CUALTIA 2023.\n\n\n\n\n\nLenguajes: Python, JAVA, HTML, ASPEN\n\nHerramientas Matemáticas: MATLAB, R, SPSS, PLC\n\nOfimática: Office, LaTeX\n\n\n\n\n\n\nIdiomas: Español (nativo), Inglés (B1)\n\nIntereses: Lectura, búsqueda, planeación"
  },
  {
    "objectID": "about.html#perfil-personal",
    "href": "about.html#perfil-personal",
    "title": "Sobre Mí",
    "section": "",
    "text": "Me considero una persona profundamente comprometida con el crecimiento intelectual, la lectura reflexiva y el bienestar de mi entorno cercano. Disfruto de los momentos al aire libre, donde encuentro inspiración en la naturaleza y la tranquilidad necesaria para mantener un equilibrio emocional. Valoro profundamente el tiempo con mi familia, ya que es el motor que impulsa mi disciplina, responsabilidad y deseo de superación constante. Asimismo, tengo una gran pasión por la programación y un genuino entusiasmo por la ciencia de datos, áreas que me permiten combinar creatividad, lógica y análisis riguroso para resolver problemas del mundo real. Estos principios personales complementan mi formación académica y profesional, motivándome a mantener siempre una actitud proactiva, ética y humana."
  },
  {
    "objectID": "about.html#experiencia",
    "href": "about.html#experiencia",
    "title": "Sobre Mí",
    "section": "",
    "text": "Supervisor — INE (2023–2025), Loreto, Zacatecas\n- Supervisión de actividades diarias para cumplimiento de objetivos.\n- Capacitación del equipo y mejora de rendimiento.\n- Evaluación de desempeño y optimización de procesos.\nMaestro Asistente — IPN (2022–2023), Zacatecas\n- Evaluación del progreso estudiantil.\n- Planeación conjunta de actividades educativas.\n- Creación y adaptación de materiales educativos."
  },
  {
    "objectID": "about.html#proyectos-conferencias-y-reconocimientos",
    "href": "about.html#proyectos-conferencias-y-reconocimientos",
    "title": "Sobre Mí",
    "section": "",
    "text": "Proyecto A — Estandarización de proceso a nivel laboratorio (2021–2023)\n- Herramientas: MATLAB, HACCP Software, Latex\n- Liderazgo de investigación para producción de yogurt con uvas a nivel laboratorio.\n- Reconocimiento al mejor promedio del programa académico (IPN 2017–2022).\n- Primer lugar en Simposium Agroalimentario y Ciclo de Conferencias CUALTIA 2023."
  },
  {
    "objectID": "about.html#habilidades",
    "href": "about.html#habilidades",
    "title": "Sobre Mí",
    "section": "",
    "text": "Lenguajes: Python, JAVA, HTML, ASPEN\n\nHerramientas Matemáticas: MATLAB, R, SPSS, PLC\n\nOfimática: Office, LaTeX"
  },
  {
    "objectID": "about.html#información-adicional",
    "href": "about.html#información-adicional",
    "title": "Sobre Mí",
    "section": "",
    "text": "Idiomas: Español (nativo), Inglés (B1)\n\nIntereses: Lectura, búsqueda, planeación"
  },
  {
    "objectID": "index.html#presentación-del-proyecto",
    "href": "index.html#presentación-del-proyecto",
    "title": "Centro de Investigación e Innovación en Tecnologías de la Información y Comunicación",
    "section": "Presentación del proyecto",
    "text": "Presentación del proyecto\nEn esta página se encuentran reflejados los cinco reportes desarrollados a lo largo de la asignatura de Análisis de Algoritmos. Cada uno de ellos ha sido debidamente documentado, estructurado y trasladado al formato Quarto con el propósito de comunicar al público el trabajo realizado y los análisis efectuados durante mi estancia en la materia. Estos reportes representan el proceso de aprendizaje y aplicación práctica de los conceptos clave abordados en cada unidad, desde la introducción al análisis algorítmico hasta los algoritmos de intersección de conjuntos, permitiendo evidenciar el desarrollo de competencias técnicas y analíticas fundamentales en el área.\nEsta documentación ha sido preparada para su publicación en un repositorio de GitHub con el objetivo de compartir de forma abierta los contenidos desarrollados, promover el acceso al conocimiento, y facilitar su consulta por parte de docentes, estudiantes y profesionales interesados en el análisis de algoritmos.\nA lo largo del curso se desarrollaron cinco tareas escritas que reflejan los temas fundamentales abordados en cada unidad:\n\nUnidad 1: Introducción al análisis de algoritmos\nSe realizó el reporte 1A. Reporte escrito. Experimentos y análisis, en el que se exploraron conceptos básicos sobre la eficiencia algorítmica y órdenes de crecimiento.\nUnidad 2: Estructuras de datos\nSe trabajó el reporte 2A. Reporte escrito. Experimentos y análisis de estructuras de datos, enfocado en el comportamiento, manipulación y análisis de distintas estructuras como listas, pilas, colas y árboles.\nUnidad 3: Algoritmos de ordenamiento por comparación\nSe elaboró el 3A. Reporte escrito. Experimentos y análisis de algoritmos de ordenamiento, donde se evaluaron métodos como burbuja, inserción, selección, quicksort y mergesort.\nUnidad 4: Algoritmos de búsqueda por comparación\nSe desarrolló el reporte 4A. Reporte escrito. Experimentos y análisis de algoritmos de búsqueda por comparación, abordando técnicas como la búsqueda lineal y binaria, con un enfoque en su eficiencia.\nUnidad 5: Algoritmos de intersección y unión de conjuntos en el modelo de comparación\nSe presentó el 5A. Reporte escrito. Experimentos y análisis de algoritmos de intersección de conjuntos, donde se analizaron distintas estrategias para operar sobre múltiples listas ordenadas."
  },
  {
    "objectID": "project2.html",
    "href": "project2.html",
    "title": "Proyecto 2",
    "section": "",
    "text": "Los experimentos y análisis de estructuras de datos son fundamentales en la ciencia de datos, ya que permiten evaluar y optimizar el rendimiento de algoritmos y sistemas que manejan grandes volúmenes de información.\nLas estructuras de datos son formas de organizar y almacenar datos en una computadora para que puedan ser accedidos y modificados de manera eficiente. Cada estructura tiene sus propias ventajas y desventajas en términos de tiempo de acceso, uso de memoria y facilidad de implementación. Los experimentos en estructuras de datos suelen centrarse en medir estos aspectos bajo diferentes condiciones para determinar cuál es la más adecuada para una aplicación específica (Goodfellow et al., 2016).\nEl diseño experimental es una etapa esencial que precede al análisis de datos. Un diseño bien planificado asegura que los datos recopilados sean relevantes y útiles para el análisis posterior. Esto incluye la definición de variables, la selección de muestras y la implementación de controles para minimizar sesgos. Estas estructuras son esenciales para el desarrollo de software eficiente, especialmente en campos como el Internet de las Cosas (IoT) y el Big Data, donde la optimización de recursos es crucial (Strang, 2016).\nEl análisis de datos implica varias técnicas estadísticas y computacionales para interpretar los resultados experimentales. Esto puede incluir análisis exploratorio de datos (EDA) para identificar patrones y relaciones, así como análisis confirmatorio para probar hipótesis específicas. Herramientas como Hadoop, Spark, Power BI, Pandas y R son comúnmente utilizadas en este proceso (Grcar, 2011).\nLa multiplicación de matrices es una operación básica que se utiliza en numerosos cálculos matemáticos y algoritmos computacionales. Su relevancia radica en su capacidad para transformar y combinar datos de manera que se puedan extraer insights significativos. La eliminación gaussiana es un método clásico para resolver sistemas de ecuaciones lineales, transformando una matriz en una forma escalonada reducida por filas. Este método es ampliamente utilizado debido a su estabilidad numérica y su capacidad para manejar sistemas grandes.\nEl objetivo es entender cómo se comportan estos algoritmos en diferentes situaciones, especialmente en cuanto al número de operaciones (como multiplicaciones y sumas) y el tiempo que tardan en ejecutarse en la práctica. Para abordar esta cuestión, se plantea un estudio comparativo utilizando matrices aleatorias de tamaño \\(n \\times n\\) para \\(n = 100\\), \\(300\\), \\(1000\\). Este enfoque permitirá evaluar la eficiencia de cada algoritmo y proporcionará una base sólida para futuras optimizaciones. Al comparar estos algoritmos, se busca responder a preguntas clave como: ¿Qué puedes concluir?, ¿Cuál es el impacto de acceder a los elementos contiguos en memoria de una matriz?, ¿Qué cambiarías si utilizas matrices dispersas?, y ¿Cuáles serían los costos?\n\n\n\n\n\nimport numpy as np\nimport time\n\n\n\ndef matrix_multiplication(A, B):\n    n = len(A)  # Tamaño de la matriz (n x n)\n    C = np.zeros((n, n))  # Inicializar la matriz resultante con ceros\n    operations = 0  # Contador de operaciones\n\n    for i in range(n):\n        for j in range(n):\n            for k in range(n):\n                C[i][j] += A[i][k] * B[k][j]  # Multiplicación y suma\n                operations += 2  # Una multiplicación y una suma\n\n    return C, operations\n\n\n\ndef gauss_jordan(A):\n    n = len(A)\n    operations = 0\n    A = np.array(A, dtype=float)\n\n    for i in range(n):\n        if A[i][i] == 0:\n            for j in range(i+1, n):\n                if A[j][i] != 0:\n                    A[[i, j]] = A[[j, i]]  # Intercambio de filas\n                    operations += 1\n                    break\n\n        div = A[i][i]\n        A[i] = A[i] / div\n        operations += n  # n divisiones\n\n        for j in range(n):\n            if i != j:\n                A[j] = A[j] - A[j][i] * A[i]\n                operations += n  # n multiplicaciones y restas\n\n    return A, operations\n\n\n\nsizes = [100, 300, 1000]\nresults = []\n\nfor n in sizes:\n    A = np.random.rand(n, n)\n    B = np.random.rand(n, n)\n\n    # Multiplicación de matrices\n    start_time = time.time()\n    C, mult_operations = matrix_multiplication(A, B)\n    mult_time = time.time() - start_time\n\n    # Eliminación Gaussiana\n    start_time = time.time()\n    _, gauss_operations = gauss_jordan(A)\n    gauss_time = time.time() - start_time\n\n    results.append({\n        'n': n,\n        'mult_operations': mult_operations,\n        'mult_time': mult_time,\n        'gauss_operations': gauss_operations,\n        'gauss_time': gauss_time\n    })\n\n\n\nfor result in results:\n    print(f\"Tamaño de la matriz (n): {result['n']}\")\n    print(\" Multiplicación de Matrices:\")\n    print(f\" Operaciones: {result['mult_operations']}\")\n    print(f\" Tiempo (s): {result['mult_time']:.6f}\")\n    print(\" Eliminación Gaussiana:\")\n    print(f\" Operaciones: {result['gauss_operations']}\")\n    print(f\" Tiempo (s): {result['gauss_time']:.6f}\")\n    print(\"-\" * 40)\n\n\n\n\n\n\nA continuación se muestra una tabla con los resultados obtenidos al comparar la multiplicación de matrices y la eliminación Gauss-Jordan sobre matrices de distintos tamaños. Se reporta el número total de operaciones y el tiempo de ejecución en segundos.\n\n\n\n\n\n\n\n\n\n\nTamaño (n)\nOperaciones Multiplicación\nTiempo Multiplicación (s)\nOperaciones Gauss\nTiempo Gauss (s)\n\n\n\n\n100\n2,000,000\n1.829321\n1,000,000\n0.066816\n\n\n300\n54,000,000\n30.990801\n27,000,000\n0.399951\n\n\n1000\n2,000,000,000\n1071.753574\n1,000,000,000\n5.316519\n\n\n\n\nNota: La eliminación Gauss-Jordan muestra una notable ventaja en tiempo de ejecución respecto a la multiplicación de matrices, aunque ambas mantienen una proporción consistente en la cantidad de operaciones requeridas.\n\nLos resultados obtenidos muestran el desempeño de dos operaciones: la multiplicación de matrices y la eliminación gaussiana/Gauss-Jordan sobre matrices de diferentes tamaños (100, 300 y 1000), cuantificando el número de operaciones y el tiempo de ejecución, como se muestra en la Tabla 1.\n\n\n\n\n\n\n\n\n\nEn los Gráficos 1 y 2, se aprecia visualmente cómo se ve afectado el número de operaciones y el tiempo de ejecución en relación con el tamaño de la matriz.\nEn cuanto a la multiplicación de matrices, el número de operaciones aumenta significativamente con el tamaño de la matriz, lo cual es de esperarse, ya que se trata de una operación computacionalmente intensiva. El tiempo de ejecución también aumenta proporcionalmente, reflejando el impacto del crecimiento en complejidad.\nPor otro lado, la eliminación gaussiana/Gauss-Jordan muestra una tendencia similar: a mayor tamaño de la matriz, mayor número de operaciones y mayor tiempo de procesamiento. Específicamente, para una matriz de tamaño \\(n = 1000\\), el tiempo fue de 1,071.753 segundos para la multiplicación de matrices y 5.316 segundos para la eliminación gaussiana, destacando una diferencia considerable a favor del segundo método en términos de eficiencia temporal.\nEl rendimiento puede verse influenciado por el hardware utilizado, como la capacidad de memoria, la arquitectura del procesador, y otros componentes del sistema. Estos factores afectan directamente el tiempo de ejecución observado. Entre los elementos que más impactan se incluyen la cantidad y velocidad de la memoria RAM, la arquitectura del procesador, el número de núcleos e hilos de ejecución, la frecuencia de reloj, la memoria caché del procesador, el tipo de almacenamiento, y la tecnología de la GPU si es utilizada (Raina et al., 2009).\n\n\n\n\n\n\nEl impacto de acceder a elementos contiguos en memoria de una matriz es un concepto fundamental en la optimización del rendimiento de programas. Este principio se basa en cómo los datos se almacenan y acceden en la memoria de una computadora.\nCuando los elementos de una matriz se almacenan de manera contigua en la memoria, el procesador puede acceder a ellos de forma más eficiente. Esto se debe a varios factores: el uso de la caché del procesador, el aumento de la localidad espacial, el prefetching, la reducción del consumo de energía y las operaciones vectorizadas.\nLa caché es una memoria de acceso rápido que almacena copias de datos que se utilizan frecuentemente, y cuando los datos están almacenados de manera contigua, es más probable que se carguen en la caché en bloques grandes, reduciendo así el tiempo de acceso.\nLa localidad espacial hace referencia a la tendencia de los programas a acceder a datos que están cerca unos de otros en la memoria. Cuando los datos están almacenados de manera contigua, se mejora la localidad espacial, lo que permite al procesador acceder a los datos de manera más rápida.\nLos procesadores utilizan técnicas de prefetching para anticipar qué datos se necesitarán en el futuro y cargarlos en la caché antes de que sean solicitados. Cuando los datos están almacenados de manera contigua, el prefetching es más efectivo, ya que el procesador puede cargar bloques grandes de datos.\nOtro impacto importante es la reducción del consumo de energía; un mejor uso de la caché y una menor latencia también pueden traducirse en un menor consumo energético.\nEl acceso contiguo a la memoria también puede influir en el rendimiento de las operaciones vectorizadas, que son comunes en aplicaciones científicas y de ingeniería. Las operaciones vectorizadas permiten al procesador realizar múltiples operaciones en paralelo, y cuando los datos están almacenados de manera contigua, es más fácil para el procesador realizarlas de manera eficiente (Bryant, 2016).\n\n\n\nSi utilizáramos matrices dispersas, estas tendrían un impacto en el rendimiento y la eficiencia de un programa, especialmente al trabajar con grandes conjuntos de datos donde la mayoría de los elementos son cero.\nLos cambios al utilizar matrices dispersas incluyen:\n\nReducción del uso de memoria: Las matrices dispersas almacenan solo los elementos no nulos y sus posiciones, lo que reduce significativamente el uso de memoria en comparación con matrices densas.\nOptimización de operaciones: Las operaciones matemáticas pueden ser más eficientes al realizarse únicamente sobre los elementos no nulos, disminuyendo así el tiempo de cómputo.\nUso de bibliotecas especializadas: Se requiere el uso de bibliotecas diseñadas específicamente para manejar matrices dispersas de forma eficiente.\n\nSin embargo, también se presentan algunos costos y desafíos:\n\nMayor complejidad en la implementación: Los algoritmos que manejan matrices dispersas pueden ser más complejos que los que utilizan matrices densas, requiriendo mayor tiempo de desarrollo y depuración.\nMantenimiento e interoperabilidad: La integración con sistemas o bibliotecas que no admiten matrices dispersas puede generar la necesidad de realizar conversiones costosas entre formatos.\nOperaciones costosas: Inserciones y eliminaciones pueden requerir mayor tiempo de procesamiento, debido a la necesidad de actualizar estructuras de índices.\nSobrecarga de gestión de datos: Aunque se reduce el uso de memoria, la administración adicional puede afectar el rendimiento, especialmente si la matriz no es extremadamente dispersa.\nCostos de conversión: Si es necesario convertir entre matrices densas y dispersas, esto puede introducir costos adicionales tanto en tiempo de ejecución como en memoria (Bryant, 2016).\n\n\n\n\n\n\nGoodfellow, I., Bengio, Y., & Courville, A. (2016). Deep Learning. MIT Press.\nStrang, G. (2016). Introduction to Linear Algebra (5ª edición). Wellesley-Cambridge Press.\nGrcar, J. F. (2011). “Mathematicians of Gaussian Elimination.” Notices of the American Mathematical Society, 58(6), 782–792.\nBryant, R. E., & O’Hallaron, D. R. (2016). Computer Systems: A Programmer’s Perspective (3rd ed.). Pearson.\nTeresa-Motiv, Saisang, brittmsantos, eross-msft, & JasonGerend. (2023, 17 de abril). Consideraciones sobre el uso de memoria para la optimización del rendimiento de AD DS.\nRaina, R., Madhavan, A., & Ng, A. Y. (2009). Large-scale Deep Unsupervised Learning using Graphics Processors. Computer Science Department, Stanford University.\n\n\n\n\nSe hicieron mejoras en el planteamiento de los experimentos y en la discusión de los resultados, como fue indicado por el maestro. Ahora cada figura tiene una explicación clara sobre lo que muestra y lo que significa. También se mejoró la redacción para que las ideas sean más comprensibles y ordenadas."
  },
  {
    "objectID": "project2.html#introducción",
    "href": "project2.html#introducción",
    "title": "Proyecto 2",
    "section": "",
    "text": "Los experimentos y análisis de estructuras de datos son fundamentales en la ciencia de datos, ya que permiten evaluar y optimizar el rendimiento de algoritmos y sistemas que manejan grandes volúmenes de información.\nLas estructuras de datos son formas de organizar y almacenar datos en una computadora para que puedan ser accedidos y modificados de manera eficiente. Cada estructura tiene sus propias ventajas y desventajas en términos de tiempo de acceso, uso de memoria y facilidad de implementación. Los experimentos en estructuras de datos suelen centrarse en medir estos aspectos bajo diferentes condiciones para determinar cuál es la más adecuada para una aplicación específica (Goodfellow et al., 2016).\nEl diseño experimental es una etapa esencial que precede al análisis de datos. Un diseño bien planificado asegura que los datos recopilados sean relevantes y útiles para el análisis posterior. Esto incluye la definición de variables, la selección de muestras y la implementación de controles para minimizar sesgos. Estas estructuras son esenciales para el desarrollo de software eficiente, especialmente en campos como el Internet de las Cosas (IoT) y el Big Data, donde la optimización de recursos es crucial (Strang, 2016).\nEl análisis de datos implica varias técnicas estadísticas y computacionales para interpretar los resultados experimentales. Esto puede incluir análisis exploratorio de datos (EDA) para identificar patrones y relaciones, así como análisis confirmatorio para probar hipótesis específicas. Herramientas como Hadoop, Spark, Power BI, Pandas y R son comúnmente utilizadas en este proceso (Grcar, 2011).\nLa multiplicación de matrices es una operación básica que se utiliza en numerosos cálculos matemáticos y algoritmos computacionales. Su relevancia radica en su capacidad para transformar y combinar datos de manera que se puedan extraer insights significativos. La eliminación gaussiana es un método clásico para resolver sistemas de ecuaciones lineales, transformando una matriz en una forma escalonada reducida por filas. Este método es ampliamente utilizado debido a su estabilidad numérica y su capacidad para manejar sistemas grandes.\nEl objetivo es entender cómo se comportan estos algoritmos en diferentes situaciones, especialmente en cuanto al número de operaciones (como multiplicaciones y sumas) y el tiempo que tardan en ejecutarse en la práctica. Para abordar esta cuestión, se plantea un estudio comparativo utilizando matrices aleatorias de tamaño \\(n \\times n\\) para \\(n = 100\\), \\(300\\), \\(1000\\). Este enfoque permitirá evaluar la eficiencia de cada algoritmo y proporcionará una base sólida para futuras optimizaciones. Al comparar estos algoritmos, se busca responder a preguntas clave como: ¿Qué puedes concluir?, ¿Cuál es el impacto de acceder a los elementos contiguos en memoria de una matriz?, ¿Qué cambiarías si utilizas matrices dispersas?, y ¿Cuáles serían los costos?"
  },
  {
    "objectID": "project2.html#desarrollo",
    "href": "project2.html#desarrollo",
    "title": "Proyecto 2",
    "section": "",
    "text": "import numpy as np\nimport time\n\n\n\ndef matrix_multiplication(A, B):\n    n = len(A)  # Tamaño de la matriz (n x n)\n    C = np.zeros((n, n))  # Inicializar la matriz resultante con ceros\n    operations = 0  # Contador de operaciones\n\n    for i in range(n):\n        for j in range(n):\n            for k in range(n):\n                C[i][j] += A[i][k] * B[k][j]  # Multiplicación y suma\n                operations += 2  # Una multiplicación y una suma\n\n    return C, operations\n\n\n\ndef gauss_jordan(A):\n    n = len(A)\n    operations = 0\n    A = np.array(A, dtype=float)\n\n    for i in range(n):\n        if A[i][i] == 0:\n            for j in range(i+1, n):\n                if A[j][i] != 0:\n                    A[[i, j]] = A[[j, i]]  # Intercambio de filas\n                    operations += 1\n                    break\n\n        div = A[i][i]\n        A[i] = A[i] / div\n        operations += n  # n divisiones\n\n        for j in range(n):\n            if i != j:\n                A[j] = A[j] - A[j][i] * A[i]\n                operations += n  # n multiplicaciones y restas\n\n    return A, operations\n\n\n\nsizes = [100, 300, 1000]\nresults = []\n\nfor n in sizes:\n    A = np.random.rand(n, n)\n    B = np.random.rand(n, n)\n\n    # Multiplicación de matrices\n    start_time = time.time()\n    C, mult_operations = matrix_multiplication(A, B)\n    mult_time = time.time() - start_time\n\n    # Eliminación Gaussiana\n    start_time = time.time()\n    _, gauss_operations = gauss_jordan(A)\n    gauss_time = time.time() - start_time\n\n    results.append({\n        'n': n,\n        'mult_operations': mult_operations,\n        'mult_time': mult_time,\n        'gauss_operations': gauss_operations,\n        'gauss_time': gauss_time\n    })\n\n\n\nfor result in results:\n    print(f\"Tamaño de la matriz (n): {result['n']}\")\n    print(\" Multiplicación de Matrices:\")\n    print(f\" Operaciones: {result['mult_operations']}\")\n    print(f\" Tiempo (s): {result['mult_time']:.6f}\")\n    print(\" Eliminación Gaussiana:\")\n    print(f\" Operaciones: {result['gauss_operations']}\")\n    print(f\" Tiempo (s): {result['gauss_time']:.6f}\")\n    print(\"-\" * 40)"
  },
  {
    "objectID": "project2.html#análisis-de-resultados",
    "href": "project2.html#análisis-de-resultados",
    "title": "Proyecto 2",
    "section": "",
    "text": "A continuación se muestra una tabla con los resultados obtenidos al comparar la multiplicación de matrices y la eliminación Gauss-Jordan sobre matrices de distintos tamaños. Se reporta el número total de operaciones y el tiempo de ejecución en segundos.\n\n\n\n\n\n\n\n\n\n\nTamaño (n)\nOperaciones Multiplicación\nTiempo Multiplicación (s)\nOperaciones Gauss\nTiempo Gauss (s)\n\n\n\n\n100\n2,000,000\n1.829321\n1,000,000\n0.066816\n\n\n300\n54,000,000\n30.990801\n27,000,000\n0.399951\n\n\n1000\n2,000,000,000\n1071.753574\n1,000,000,000\n5.316519\n\n\n\n\nNota: La eliminación Gauss-Jordan muestra una notable ventaja en tiempo de ejecución respecto a la multiplicación de matrices, aunque ambas mantienen una proporción consistente en la cantidad de operaciones requeridas.\n\nLos resultados obtenidos muestran el desempeño de dos operaciones: la multiplicación de matrices y la eliminación gaussiana/Gauss-Jordan sobre matrices de diferentes tamaños (100, 300 y 1000), cuantificando el número de operaciones y el tiempo de ejecución, como se muestra en la Tabla 1.\n\n\n\n\n\n\n\n\n\nEn los Gráficos 1 y 2, se aprecia visualmente cómo se ve afectado el número de operaciones y el tiempo de ejecución en relación con el tamaño de la matriz.\nEn cuanto a la multiplicación de matrices, el número de operaciones aumenta significativamente con el tamaño de la matriz, lo cual es de esperarse, ya que se trata de una operación computacionalmente intensiva. El tiempo de ejecución también aumenta proporcionalmente, reflejando el impacto del crecimiento en complejidad.\nPor otro lado, la eliminación gaussiana/Gauss-Jordan muestra una tendencia similar: a mayor tamaño de la matriz, mayor número de operaciones y mayor tiempo de procesamiento. Específicamente, para una matriz de tamaño \\(n = 1000\\), el tiempo fue de 1,071.753 segundos para la multiplicación de matrices y 5.316 segundos para la eliminación gaussiana, destacando una diferencia considerable a favor del segundo método en términos de eficiencia temporal.\nEl rendimiento puede verse influenciado por el hardware utilizado, como la capacidad de memoria, la arquitectura del procesador, y otros componentes del sistema. Estos factores afectan directamente el tiempo de ejecución observado. Entre los elementos que más impactan se incluyen la cantidad y velocidad de la memoria RAM, la arquitectura del procesador, el número de núcleos e hilos de ejecución, la frecuencia de reloj, la memoria caché del procesador, el tipo de almacenamiento, y la tecnología de la GPU si es utilizada (Raina et al., 2009)."
  },
  {
    "objectID": "project2.html#conclusiones",
    "href": "project2.html#conclusiones",
    "title": "Proyecto 2",
    "section": "",
    "text": "El impacto de acceder a elementos contiguos en memoria de una matriz es un concepto fundamental en la optimización del rendimiento de programas. Este principio se basa en cómo los datos se almacenan y acceden en la memoria de una computadora.\nCuando los elementos de una matriz se almacenan de manera contigua en la memoria, el procesador puede acceder a ellos de forma más eficiente. Esto se debe a varios factores: el uso de la caché del procesador, el aumento de la localidad espacial, el prefetching, la reducción del consumo de energía y las operaciones vectorizadas.\nLa caché es una memoria de acceso rápido que almacena copias de datos que se utilizan frecuentemente, y cuando los datos están almacenados de manera contigua, es más probable que se carguen en la caché en bloques grandes, reduciendo así el tiempo de acceso.\nLa localidad espacial hace referencia a la tendencia de los programas a acceder a datos que están cerca unos de otros en la memoria. Cuando los datos están almacenados de manera contigua, se mejora la localidad espacial, lo que permite al procesador acceder a los datos de manera más rápida.\nLos procesadores utilizan técnicas de prefetching para anticipar qué datos se necesitarán en el futuro y cargarlos en la caché antes de que sean solicitados. Cuando los datos están almacenados de manera contigua, el prefetching es más efectivo, ya que el procesador puede cargar bloques grandes de datos.\nOtro impacto importante es la reducción del consumo de energía; un mejor uso de la caché y una menor latencia también pueden traducirse en un menor consumo energético.\nEl acceso contiguo a la memoria también puede influir en el rendimiento de las operaciones vectorizadas, que son comunes en aplicaciones científicas y de ingeniería. Las operaciones vectorizadas permiten al procesador realizar múltiples operaciones en paralelo, y cuando los datos están almacenados de manera contigua, es más fácil para el procesador realizarlas de manera eficiente (Bryant, 2016).\n\n\n\nSi utilizáramos matrices dispersas, estas tendrían un impacto en el rendimiento y la eficiencia de un programa, especialmente al trabajar con grandes conjuntos de datos donde la mayoría de los elementos son cero.\nLos cambios al utilizar matrices dispersas incluyen:\n\nReducción del uso de memoria: Las matrices dispersas almacenan solo los elementos no nulos y sus posiciones, lo que reduce significativamente el uso de memoria en comparación con matrices densas.\nOptimización de operaciones: Las operaciones matemáticas pueden ser más eficientes al realizarse únicamente sobre los elementos no nulos, disminuyendo así el tiempo de cómputo.\nUso de bibliotecas especializadas: Se requiere el uso de bibliotecas diseñadas específicamente para manejar matrices dispersas de forma eficiente.\n\nSin embargo, también se presentan algunos costos y desafíos:\n\nMayor complejidad en la implementación: Los algoritmos que manejan matrices dispersas pueden ser más complejos que los que utilizan matrices densas, requiriendo mayor tiempo de desarrollo y depuración.\nMantenimiento e interoperabilidad: La integración con sistemas o bibliotecas que no admiten matrices dispersas puede generar la necesidad de realizar conversiones costosas entre formatos.\nOperaciones costosas: Inserciones y eliminaciones pueden requerir mayor tiempo de procesamiento, debido a la necesidad de actualizar estructuras de índices.\nSobrecarga de gestión de datos: Aunque se reduce el uso de memoria, la administración adicional puede afectar el rendimiento, especialmente si la matriz no es extremadamente dispersa.\nCostos de conversión: Si es necesario convertir entre matrices densas y dispersas, esto puede introducir costos adicionales tanto en tiempo de ejecución como en memoria (Bryant, 2016)."
  },
  {
    "objectID": "project2.html#referencias",
    "href": "project2.html#referencias",
    "title": "Proyecto 2",
    "section": "",
    "text": "Goodfellow, I., Bengio, Y., & Courville, A. (2016). Deep Learning. MIT Press.\nStrang, G. (2016). Introduction to Linear Algebra (5ª edición). Wellesley-Cambridge Press.\nGrcar, J. F. (2011). “Mathematicians of Gaussian Elimination.” Notices of the American Mathematical Society, 58(6), 782–792.\nBryant, R. E., & O’Hallaron, D. R. (2016). Computer Systems: A Programmer’s Perspective (3rd ed.). Pearson.\nTeresa-Motiv, Saisang, brittmsantos, eross-msft, & JasonGerend. (2023, 17 de abril). Consideraciones sobre el uso de memoria para la optimización del rendimiento de AD DS.\nRaina, R., Madhavan, A., & Ng, A. Y. (2009). Large-scale Deep Unsupervised Learning using Graphics Processors. Computer Science Department, Stanford University."
  },
  {
    "objectID": "project2.html#cambios-realizados",
    "href": "project2.html#cambios-realizados",
    "title": "Proyecto 2",
    "section": "",
    "text": "Se hicieron mejoras en el planteamiento de los experimentos y en la discusión de los resultados, como fue indicado por el maestro. Ahora cada figura tiene una explicación clara sobre lo que muestra y lo que significa. También se mejoró la redacción para que las ideas sean más comprensibles y ordenadas."
  },
  {
    "objectID": "project4.html#introducción",
    "href": "project4.html#introducción",
    "title": "Proyecto 4",
    "section": "1. Introducción",
    "text": "1. Introducción"
  },
  {
    "objectID": "project4.html#desarrollo",
    "href": "project4.html#desarrollo",
    "title": "Proyecto 4",
    "section": "2. Desarrollo",
    "text": "2. Desarrollo"
  },
  {
    "objectID": "project4.html#análisis-de-resultados",
    "href": "project4.html#análisis-de-resultados",
    "title": "Proyecto 4",
    "section": "3. Análisis de Resultados",
    "text": "3. Análisis de Resultados"
  },
  {
    "objectID": "project4.html#conclusiones",
    "href": "project4.html#conclusiones",
    "title": "Proyecto 4",
    "section": "4. Conclusiones",
    "text": "4. Conclusiones"
  },
  {
    "objectID": "project4.html#referencias",
    "href": "project4.html#referencias",
    "title": "Proyecto 4",
    "section": "5. Referencias",
    "text": "5. Referencias"
  },
  {
    "objectID": "project4.html#cambios-realizados",
    "href": "project4.html#cambios-realizados",
    "title": "Proyecto 4",
    "section": "6. Cambios Realizados",
    "text": "6. Cambios Realizados"
  }
]